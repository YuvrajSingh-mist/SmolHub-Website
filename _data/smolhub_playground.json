{
  "last_updated": "2025-08-09T20:14:12.243Z",
  "total_projects": 7,
  "projects": [
    {
      "name": "hf-spaces",
      "display_name": "Hf-spaces",
      "description": "SmolHub playground project: hf-spaces",
      "readme_content": "# hf-spaces\n\nThis project is part of the SmolHub Playground collection.\n\n[View on GitHub](https://github.com/YuvrajSingh-mist/SmolHub/tree/main/hf-spaces)",
      "tags": [
        "compact",
        "experimental"
      ],
      "github_url": "https://github.com/YuvrajSingh-mist/SmolHub/tree/main/hf-spaces",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/SmolHub/contents/hf-spaces?ref=main",
      "download_url": null,
      "created_date": "2025-08-09",
      "github_date": "2025-08-09"
    },
    {
      "name": "SmolLlama",
      "display_name": "Smol Llama",
      "description": "- So, I trained a Llama a 130M architecture I coded from ground up to build a small instruct model, going through the below-mentioned stages from scra...",
      "readme_content": "\n# Introducing SmolLlama - A Smaller Language Model \n\n- So, I trained a Llama a 130M architecture I coded from ground up to build a small instruct model, going through the below-mentioned stages from scratch.\n- Trained on FineWeb dataset form HuggingFace consisting of 15 M texts (10BT snapshot) for a total of full 3 epochs\n\nThe three main stages are:\n\n1) Pretraining\n2) SFT (Instruction Tuning)\n3) Reward Tuning for human-like responses (DPO)\n\n- Test it out here - [SmolLlama](https://huggingface.co/spaces/YuvrajSingh9886/SmolLlama)\n\n- Models have been uploaded on HF!\n\n ### 1) Pretraining\n\n#### Dataset\n\n - I used the [FineWeb](https://huggingface.co/datasets/HuggingFaceFW/fineweb?row=0) dataset from HuggingFace (10BT checkpoint) consisting of roughly 15M texts.\n\n  1) Train dataset - 12 M texts\n  2) Val dataset - 3M texts\n\n\n\n ### 2) SFT\n\n#### Dataset\n\n - I used the [alpaca](https://huggingface.co/datasets/yahma/alpaca-cleaned) dataset from HuggingFace consisting of roughly 52k texts.\n\n  1) Train dataset - 45k texts\n  2) Val dataset - 5k texts\n\n\n\n ### 3) Reference Alignment\n\n#### Dataset\n\n - I used the [Ultrafeedback](https://huggingface.co/datasets/trl-lib/ultrafeedback_binarized) dataset from HuggingFace consisting of roughly 62.1K texts.\n\n  1) Train dataset - 56K texts\n  2) Val dataset - 6K texts\n\n\n\n---\n\n####  ModelArgs (Hyperparameters)\n\n\n| Parameter                      | Description                                                                 | Default Value                     | Type      |\n|--------------------------------|-----------------------------------------------------------------------------|-----------------------------------|-----------|\n| `epochs`                       | Number of training epochs                                                   | `4`                               | `int`     |\n| `block_size`                   | Size of each block (context length)                                         | `512`                             | `int`     |\n| `batch_size`                   | Batch size for training                                                    | `64`                              | `int`     |\n| `inference`                    | Inference mode (not specified)                                              | `None`                            | `None`    |\n| `embeddings_dims`              | Dimensionality of embeddings                                                | `512`                             | `int`     |\n| `attn_dropout`                 | Dropout rate for attention layers                                           | `0.1`                             | `float`   |\n| `no_of_heads`                  | Number of attention heads                                                   | `8`                               | `int`     |\n| `dropout`                      | Dropout rate for the model                                                  | `0.1`                             | `float`   |\n| `val_epochs`                   | Number of validation epochs                                                 | `2`                               | `int`     |\n| `max_lr`                       | Maximum learning rate                                                       | `6e-4`                            | `float`   |\n| `no_of_decoder_layers`         | Number of decoder layers                                                    | `8`                               | `int`     |\n| `weight_decay_optim`           | Weight decay for the optimizer                                              | `0.1`                             | `float`   |\n| `beta_1`                       | Beta 1 for Adam optimizer                                                   | `0.9`                             | `float`   |\n| `beta_2`                       | Beta 2 for Adam optimizer                                                   | `0.95`                            | `float`   |\n| `clip`                         | Gradient clipping value                                                     | `1.0`                             | `float`   |\n| `device`                       | Device to run the model (`cuda` or `cpu`)                                   | `'cuda'`                          | `str`     |\n| `no_kv_heads`                  | Number of key-value heads                                                   | `2`                               | `int`     |\n| `vocab_size`                   | Size of the vocabulary                                                      | `50304`                           | `int`     |\n| `eps`                          | Epsilon value for numerical stability                                       | `1e-5`                            | `float`   |\n| `dtype`                        | Data type for tensors (`bfloat16` if supported, else `float16`)             | `'bfloat16'` or `'float16'`       | `str`     |\n| `save_checkpoint_dir`          | Directory to save model checkpoints                                         | `\"checkpoints\"`                   | `str`     |\n| `prompt`                       | Default prompt for inference                                                | `\"Once upon a time\"`              | `str`     |\n| `save_checkpoint_iter`         | Save checkpoint every N iterations                                         | `50`                              | `int`     |\n| `total_iters`                  | Total number of training iterations                                        | `10000`                           | `int`     |\n| `eval_iters`                   | Evaluate model every N iterations                                          | `50`                              | `int`     |\n| `eval_check`                   | Check evaluation metrics every N iterations                                | `100`                             | `int`     |\n| `warmup_iters`                 | Number of warmup iterations for learning rate scheduling                   | `700`                             | `int`     |\n| `min_lr`                       | Minimum learning rate (10% of `max_lr`)                                     | `0.1 * max_lr`                    | `float`   |\n| `lr_decay_iters`               | Number of iterations for learning rate decay                               | `10000`                           | `int`     |\n| `total_batch_size`             | Total batch size across all devices                                         | `524288`                          | `int`     |\n| `micro_batch_size`             | Micro batch size per device                                                | `batch_size`                      | `int`     |\n| `gradient_accumulation_steps`  | Gradient accumulation steps                                                 | `total_batch_size // (micro_batch_size * (block_size * torch.cuda.device_count()))` | `int` |\n| `no_kv_heads`                  | Number of key-value heads                                                   | `2`                               | `int`     |\n---\n### Hardware Setup\n\n - Used DPP using Pytorch torchrun consisting of 2x H100s SXM (80GB VRAM each) rented on runpod.io\n - The model is a 1.1GB in size but needs around 1.1 GB of VRAM when loaded in fp32 precision\n---\n\n#### Frameworks:\n**Pytorch**\n\n--- \n\n#### Epochs/Steps\n- Iterations (train) = 6K (gradient accumulation of 0.5M tokens)\n\n- Val iterations = every 50 steps\n---\n\n#### Losses\n\n - Result - Pretraining  \n\n   Train loss: 3.77 (stagnated)  \n   Val Loss: 3.80 (stagnated)  \n\n\n - Result - SFT  \n\n   Train Loss: 1.01  \n   Val Loss: 1.39  \n\n - Result - DPO  \n\n   Train Loss:  0.96  \n   Val Loss: 1.05  \n\n---\n\n#### Screenshots of the loss curves\n\n- Pretrain\n\n![Train Loss Curves](images/loss_curves.jpg)\n\n- SFT\n\n![Train Loss Curves (SFT)](images/loss_curves_sft.jpg)\n\n- DPO\n\n![Train Loss Curves (DPO)](images/loss_curves_dpo.jpg)\n\n\n\n--- \n#### Output\n\n- Sample Ouput\n\n![Sample Ouput 1](images/sample_1.jpg)\n\n![Sample Ouput 2](images/sample_2.jpg)\n---\n\n### Local setup\n\n\n### Requirements\n\n\n\n```python\ngit [clone the repo](https://github.com/YuvrajSingh-mist/SmolLlama.git)\ncd SmolLlama\nbash ./install.sh\n\n```\n- A wandb.ai account for plotting graphs for your loss curves\n\n- On your terminal run\n```python\nwandb login\n```\n\n- Enter the api key and follow the instructions and once you are succesfully logged in follow the given steps\n\n\n- Download the model\n\nCan use 'P or F or D' to download the model, stands for pretarined, fine tuned and preference alignment models resp.\n```python\npython donwload_model_weight.py -D\n```\n\n\n---\n\n### Running \n\n\n#### Training a model\n\n- Kindly hange 'device' to any of your available cuda gpus.\n\nTo run:\n\n```python\ncd SmolLlama\n```\n\nPrepare the dataset. You can set either \"tinystories\" or \"fw\" or \"dpo\" to True to downalod the corresponding datasets.\n\n```python\n\npython data.py --tinystories\n\n\n```\n\n```python\nbash ./install.sh\n```\n\nDownload the weights of the model\n\n```python\n\npython download_model_weight.py -sft/dpo/pretrained\n\n```\n\nTrain the model\n\nNow run the following command (Use 'P' for pretraining/SFT and 'D' for DPO)\n\n```python\ntorchrun --standalone --nproc_per_node=gpu trainer.py \\  \n    --train P \\\n    --epochs 4 \\\n    --beta 0.1 \\\n    --block_size 256 \\\n    --batch_size 128 \\\n    --embeddings_dims 512 \\\n    --attn_dropout 0.1 \\\n    --no_of_heads 8 \\\n    --dropout 0.1 \\\n    --val_epochs 2 \\\n    --max_lr 6e-4 \\\n    --no_of_decoder_layers 16 \\\n    --weight_decay_optim 0.1 \\\n    --beta_1 0.9 \\\n    --beta_2 0.95 \\\n    --clip 1.0 \\\n    --device cuda \\\n    --no_kv_heads 2 \\\n    --vocab_size 50304 \\\n    --eps 1e-5 \\\n    --dtype \"bfloat16\" \\\n    --save_checkpoint_dir \"checkpoints\" \\\n    --prompt \"Once upon a time\" \\\n    --save_checkpoint_iter 50 \\\n    --total_iters 20000 \\\n    --eval_iters 50 \\\n    --eval_check 100 \\\n    --warmup_iters 700 \\\n    --min_lr 6e-5 \\\n    --lr_decay_iters 20000 \\\n    --total_batch_size 524288 \\\n    --micro_batch_size 128 \\\n    --gradient_accumulation_steps 4096\n    --max_length 100 \\\n    --temperature 0.8\n```\n\n\n#### Inference on a model\n\n - First method:  \n \n```python\npython inference_sft.py --prompt \"Follow the given instruction carefully. What was the Civil Rights Movement?\" --max_length 256 --temperature 0.8  \n```\n\n - Second method (through Huggingface):  \n\n   <!-- python download_model_weight.py -sft/dpo/pretrained  -->  \n  ```python\n\n  cd gradio/  \n  \n  pip install gradio  \n\n  python app.py\n\n  ```\n\n",
      "tags": [
        "llama",
        "transformer",
        "pytorch",
        "compact",
        "vision",
        "interactive",
        "educational"
      ],
      "github_url": "https://github.com/YuvrajSingh-mist/SmolHub/tree/main/SmolLlama",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/SmolHub/contents/SmolLlama?ref=main",
      "download_url": null,
      "created_date": "2025-08-09",
      "github_date": "2025-08-09"
    },
    {
      "name": "SmolMixtral",
      "display_name": "Smol Mixtral",
      "description": "A PyTorch implementation of a Mixtral inspired transformer model with Mixture of Experts (MoE), designed for text generation and understanding tasks. ...",
      "readme_content": "\r\n# SmolMixtral - Mixtral Inspired Model\r\n\r\nA PyTorch implementation of a Mixtral inspired transformer model with Mixture of Experts (MoE), designed for text generation and understanding tasks. This model is built on the Mixtral architecture with enhancements like Flash Attention, SWiGLU activation, and Liger kernels for optimized performance.\r\n\r\n- So, I trained a MoE based a 124M (8x12M) architecture I coded from ground up.\r\n- Trained on TinyStories dataset from HuggingFace consisting of 1M texts for a total of 14000 steps\r\n\r\n## Examples\r\n\r\nProvided under the `generated_data/` directory, these examples showcase the model's capabilities in text generation and understanding.\r\n\r\n![SmolMixtral Model](images/loss.jpg)\r\n\r\n## 📊 Training Results & Model Weights\r\n\r\n**📈 View Training Report**: [SmolMixtral Training Results on WandB](https://wandb.ai/rentio/Mixtral-DDP-Pretrain-10-billion-tokens/reports/SmolMixtral--VmlldzoxMzYyNzc0OQ?accessToken=nybd4lxybsbq5k5fh2dqjcucdawilt3fossn583wv6jiu8tbdzcybiihe7rhsqmq)\r\n\r\n**💾 Download Pre-trained Weights**: \r\n- **Hugging Face Model**: [YuvrajSingh9886/SmolMixtral](https://huggingface.co/YuvrajSingh9886/SmolMixtral)\r\n- **WandB Checkpoints**: Check the WandB report above for additional trained model checkpoints\r\n\r\n## Features\r\n\r\n- **Flash Attention**: Efficient attention mechanism with memory optimization\r\n- **Mixture of Experts (MoE)**: 8 experts with top-2 routing and noisy top-k support\r\n- **SWiGLU Activation**: Advanced activation function in expert layers\r\n- **Rotary Positional Embeddings**: Position encoding for sequence understanding\r\n- **Liger Kernels**: Optimized kernels for faster training (optional)\r\n- **Distributed Training**: Support for multi-GPU training with DDP\r\n- **Advanced Optimizer**: AdamW optimizer with custom learning rate scheduling\r\n- **Gradio Interface**: Interactive web interface for text generation\r\n\r\n## Model Architecture\r\n\r\n### Default Configuration\r\n- **Embedding Dimensions**: 512\r\n- **Decoder Layers**: 8\r\n- **Attention Heads**: 8\r\n- **MoE Experts**: 8 (top-2 routing)\r\n- **Block Size**: 1024 tokens\r\n- **Vocabulary Size**: Based on Llama-2-7b tokenizer (~32,000 tokens)\r\n- **Batch Size**: 16\r\n\r\n### Full Parameter List\r\n\r\n#### Model Architecture Parameters\r\n- `epochs`: Number of training epochs (default: 4)\r\n- `block_size`: Maximum sequence length (default: 1024)\r\n- `batch_size`: Training batch size (default: 16)\r\n- `embeddings_dims`: Model embedding dimensions (default: 512)\r\n- `no_of_heads`: Number of attention heads (default: 8)\r\n- `no_of_decoder_layers`: Number of decoder layers (default: 8)\r\n- `attn_dropout`: Attention dropout rate (default: 0.1)\r\n- `dropout`: General dropout rate (default: 0.1)\r\n\r\n#### Mixture of Experts (MoE) Parameters\r\n- `experts`: Number of MoE experts (default: 8)\r\n- `top_experts`: Number of experts to route to (default: 2)\r\n- `noisy_topk`: Use noisy top-k routing (default: False)\r\n\r\n#### Training Hyperparameters\r\n- `max_lr`: Maximum learning rate (default: 6e-4)\r\n- `weight_decay_optim`: Weight decay for optimizer (default: 0.01)\r\n- `beta_1`: Beta1 for optimizer (default: 0.9)\r\n- `beta_2`: Beta2 for optimizer (default: 0.95)\r\n- `eps`: Epsilon for optimizer (default: 1e-8)\r\n- `clip`: Gradient clipping value (default: 1.0)\r\n\r\n#### System Configuration\r\n- `device`: Device to use (default: 'cuda:9')\r\n- `use_checkpointing`: Use gradient checkpointing (default: False)\r\n- `use_liger`: Use Liger kernels for optimization (default: True)\r\n- `use_flash_attention`: Use Flash Attention (default: True)\r\n- `use_compile`: Use torch.compile (default: True)\r\n\r\n#### Data Configuration\r\n- `vocab_size`: Vocabulary size (default: based on tokenizer + 768)\r\n- `val_epochs`: Validation frequency (default: 2)\r\n\r\n## Quick Start\r\n\r\n### Installation\r\n\r\n```bash\r\nchmod +x install.sh\r\n./install.sh\r\n```\r\n\r\n### Important: Hugging Face Token Setup\r\n\r\nSince this model uses the Llama-2 tokenizer, you'll need a Hugging Face token to access the gated model. \r\n\r\n1. **Get a Hugging Face Token:**\r\n   - Go to [Hugging Face Settings](https://huggingface.co/settings/tokens)\r\n   - Create a new token with \"Read\" permissions\r\n   - Accept the Llama-2 license at [meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)\r\n\r\n2. **Set your token in config.py:**\r\n   ```python\r\n   TOKEN = 'your_token_here'\r\n   ```\r\n\r\n### Using Pre-trained Weights\r\n\r\n1. **Download Model Weights**: \r\n   - **Option 1**: Download from [Hugging Face - YuvrajSingh9886/SmolMixtral](https://huggingface.co/YuvrajSingh9886/SmolMixtral)\r\n   - **Option 2**: Visit the [WandB Training Report](https://wandb.ai/rentio/Mixtral-DDP-Pretrain-10-billion-tokens) for additional checkpoints\r\n   - Place downloaded files in the `checkpoints/` directory\r\n\r\n2. **Load Pre-trained Model for Inference**:\r\n   ```bash\r\n   # Using the Gradio web interface\r\n   cd gradio\r\n   python app.py\r\n   \r\n   # Or use in your own code\r\n   python inference.py\r\n   ```\r\n\r\n### Training Examples\r\n\r\n#### Basic Training (Single GPU)\r\n```bash\r\npython trainer.py\r\n```\r\n\r\n#### Training with Custom Parameters\r\n```bash\r\n# Train with larger model (modify config.py)\r\npython trainer.py\r\n\r\n# Train with different dataset (modify data.py)\r\npython trainer.py\r\n```\r\n\r\n#### Multi-GPU Distributed Training\r\n```bash\r\n# 2 GPUs\r\ntorchrun --nproc_per_node=2 trainer.py\r\n\r\n# 4 GPUs\r\ntorchrun --nproc_per_node=4 trainer.py\r\n\r\n# 8 GPUs\r\ntorchrun --nproc_per_node=8 trainer.py\r\n```\r\n\r\n### Inference with Gradio\r\n\r\n**HF_TOKEN** should be set in `config.py` to use the Gradio interface. Moreover, set your token as follows:\r\n\r\n```python\r\n export HF_TOKEN=<TOKEN_HERE>\r\n```\r\n\r\n\r\n```bash\r\n# Run the Gradio app\r\ncd gradio\r\npython app.py\r\n\r\n# With custom checkpoint (edit app.py to point to your checkpoint)\r\ncd gradio\r\npython app.py\r\n```\r\n\r\n## File Structure\r\n\r\n```\r\nSmolMixtral/\r\n├── config.py          # Model configuration and hyperparameters\r\n├── model.py           # Model architecture (Mixtral, MoE, Attention, etc.)\r\n├── data.py           # Data loading and preparation\r\n├── inference.py      # Inference functions and text generation\r\n├── trainer.py        # Main training loop with DDP support\r\n├── install.sh        # Setup script\r\n├── requirements.txt  # Python dependencies\r\n├── model_summary.py  # Model architecture summary\r\n├── gradio/\r\n│   └── app.py        # Gradio web interface\r\n├── checkpoints/      # Model checkpoints\r\n├── generated_data/   # Generated text outputs\r\n├── images/           # Project images\r\n└── old/             # Original files\r\n```\r\n\r\n\r\n\r\n## Training Features\r\n\r\n- **Gradient Accumulation**: Configurable batch size scaling\r\n- **Learning Rate Scheduling**: Cosine decay with warmup\r\n- **Gradient Clipping**: Prevents gradient explosion\r\n- **Wandb Integration**: Experiment tracking and logging\r\n- **Checkpointing**: Regular model checkpoints during training\r\n- **Loss Calculation**: Optimized cross-entropy with padding token handling\r\n- **Distributed Training**: Multi-GPU support with DDP\r\n- **Memory Optimization**: Gradient checkpointing support\r\n\r\n## Generation Methods\r\n\r\n1. **Top-k Sampling**: Traditional sampling with temperature control\r\n\r\n## Advanced Usage\r\n\r\n### Configuration\r\nAll parameters can be configured by modifying `config.py`:\r\n\r\n```python\r\n@dataclass\r\nclass ModelArgs:\r\n    epochs = 4\r\n    block_size = 1024\r\n    batch_size = 16\r\n    embeddings_dims = 512\r\n    # ... other parameters\r\n```\r\n\r\n### Custom Dataset Training\r\nModify `data.py` to use different datasets:\r\n```python\r\n# TinyStories (default)\r\ntinystories = True\r\nfw = False\r\n\r\n# FineWeb\r\ntinystories = False\r\nfw = True\r\n```\r\n\r\n### Monitoring and Logging\r\nTraining automatically logs to WandB with project name \"Mixtral-DDP-Pretrain-10-billion-tokens\"\r\n\r\n## Performance Tips\r\n\r\n1. **Use Liger Kernels**: Keep `use_liger = True` for optimized operations\r\n2. **Flash Attention**: Keep `use_flash_attention = True` for memory efficiency\r\n3. **Gradient Checkpointing**: Use `use_checkpointing = True` for memory-constrained setups\r\n4. **Batch Size Tuning**: Start with smaller batch sizes and increase gradually\r\n5. **Block Size**: Larger block sizes improve quality but require more memory\r\n\r\n## Troubleshooting\r\n\r\n### Common Issues\r\n\r\n#### Authentication Error (401)\r\n```bash\r\n# Make sure you have accepted the Llama-2 license and have a valid token\r\n# Visit: https://huggingface.co/meta-llama/Llama-2-7b-hf\r\n# Then set your token in config.py\r\n```\r\n\r\n#### Out of Memory (OOM)\r\n```python\r\n# Reduce batch size and enable checkpointing in config.py\r\nbatch_size = 8\r\nuse_checkpointing = True\r\n```\r\n\r\n#### Slow Training\r\n```python\r\n# Enable optimizations in config.py\r\nuse_liger = True\r\nuse_flash_attention = True\r\nuse_compile = True\r\n```\r\n\r\n## Contributing\r\n\r\nFeel free to contribute improvements, bug fixes, or new features!\r\n\r\n## Requirements\r\n\r\n- Python 3.8+\r\n- PyTorch 2.0+\r\n- Transformers\r\n- Datasets\r\n- Gradio\r\n- Wandb\r\n- Liger-kernel (optional)\r\n\r\n## License\r\n\r\nMIT License\r\n\r\n",
      "tags": [
        "mixtral",
        "llama",
        "transformer",
        "pytorch",
        "generation",
        "compact",
        "vision",
        "distributed",
        "interactive",
        "optimized",
        "educational"
      ],
      "github_url": "https://github.com/YuvrajSingh-mist/SmolHub/tree/main/SmolMixtral",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/SmolHub/contents/SmolMixtral?ref=main",
      "download_url": null,
      "created_date": "2025-08-08",
      "github_date": "2025-08-08"
    },
    {
      "name": "SmolTransformer",
      "display_name": "Smol Transformer",
      "description": "A compact implementation of an Encoder-Decoder Transformer for sequence-to-sequence translation tasks. This project implements a translation model fro...",
      "readme_content": "# SmolTransformer\n\nA compact implementation of an Encoder-Decoder Transformer for sequence-to-sequence translation tasks. This project implements a translation model from English to Hindi using the Samanantar dataset.\n\n## Features\n\n- **Encoder-Decoder Architecture**: Full transformer implementation with separate encoder and decoder\n- **Sinusoidal Positional Embeddings**: Learnable position encoding for better sequence understanding\n- **Multi-Head Attention**: Self-attention and cross-attention mechanisms\n- **Advanced Generation**: Top-K sampling and beam search for text generation\n- **Mixed Precision Training**: Automatic mixed precision for faster training\n- **Gradient Accumulation**: Support for large effective batch sizes\n- **Comprehensive Logging**: WandB integration for experiment tracking\n\n## Architecture\n\n### Model Components\n\n1. **Encoder**:\n   - Multi-layer encoder blocks with self-attention\n   - Sinusoidal positional embeddings\n   - Layer normalization and feed-forward networks\n\n2. **Decoder**:\n   - Multi-layer decoder blocks with masked self-attention\n   - Cross-attention to encoder outputs\n   - Autoregressive generation capability\n\n3. **Attention Mechanisms**:\n   - Masked Multi-Head Attention (for decoder self-attention)\n   - Full Multi-Head Attention (for encoder self-attention)\n   - Cross Multi-Head Attention (for encoder-decoder attention)\n\n## Installation\n\n```bash\n# Clone the repository\ncd SmolTransformer\n\n# Install dependencies\nchmod +x install.sh\n./install.sh\n```\n\n## Configuration\n\nThe model configuration can be modified in `config.py`:\n\n```python\n@dataclass\nclass ModelArgs:\n    block_size: int = 512           # Maximum sequence length\n    batch_size: int = 32            # Training batch size\n    embeddings_dims: int = 512      # Model embedding dimensions\n    no_of_heads: int = 8            # Number of attention heads\n    no_of_decoder_layers: int = 6   # Number of decoder layers\n    max_lr: float = 6e-4           # Maximum learning rate\n    # ... additional parameters\n```\n\n## Usage\n\n### Training\n\n```bash\npython trainer.py\n```\n\n### Web Application\n\nLaunch the interactive Gradio web interface:\n\n```bash\npython launch_app.py\n```\n\nThe app will be available at `http://localhost:7860` and provides:\n- **Interactive Translation**: English to Hindi translation interface\n- **Multiple Generation Methods**: Top-K sampling and beam search\n- **Real-time Parameter Adjustment**: Temperature, top-k, beam width controls\n- **Model Loading**: Load trained checkpoints\n- **Example Translations**: Pre-built examples to try\n\n### Web App Features\n\n- 🌐 **User-friendly Interface**: Clean, intuitive web UI\n- 🔧 **Configurable Parameters**: Adjust generation settings in real-time\n- 📊 **Model Information**: Display architecture and training details\n- 🎯 **Multiple Methods**: Compare Top-K sampling vs Beam search\n- 💾 **Checkpoint Loading**: Load your trained models\n- 📝 **Examples**: Built-in examples to test the model\n\n### Key Training Features\n\n- **Gradient Accumulation**: Automatically calculated based on `total_batch_size`\n- **Learning Rate Scheduling**: Warmup + cosine decay\n- **Mixed Precision**: Automatic mixed precision training\n- **Checkpointing**: Regular model checkpoints saved to `checkpoints/`\n- **Generation Samples**: Periodic text generation during training\n\n### Dataset\n\nThe model is trained on the Hindi-English Samanantar dataset:\n- **Source**: English text\n- **Target**: Hindi text  \n- **Preprocessing**: Automatic tokenization with IndicBARTSS tokenizer\n\n## File Structure\n\n```\nSmolTransformer/\n├── config.py          # Model configuration and hyperparameters\n├── model.py           # Transformer model implementation\n├── data.py            # Dataset loading and preprocessing\n├── tokenizer.py       # Tokenizer setup and utilities\n├── trainer.py         # Training loop and utilities\n├── inference.py       # Text generation functions\n├── install.sh         # Installation script\n├── README.md          # This file\n├── checkpoints/       # Model checkpoints\n├── generated_data/    # Generated text samples\n├── gradio/            # Gradio interface (optional)\n└── old/              # Backup files\n```\n\n## Model Parameters\n\n- **Parameters**: ~25M (configurable)\n- **Context Length**: 512 tokens\n- **Vocabulary**: IndicBARTSS tokenizer (~30K tokens)\n- **Architecture**: 6-layer encoder-decoder\n\n## Training Features\n\n### Optimization\n- **Optimizer**: AdamW with weight decay\n- **Learning Rate**: 6e-4 with warmup and cosine decay\n- **Gradient Clipping**: 1.0 max norm\n- **Mixed Precision**: Automatic FP16 training\n\n### Monitoring\n- **WandB Integration**: Comprehensive experiment tracking\n- **Metrics**: Loss, perplexity, gradient norms\n- **Generation Samples**: Regular text generation examples\n- **Validation**: Periodic validation loss evaluation\n\n### Generation Methods\n- **Top-K Sampling**: Configurable top-k and temperature\n- **Beam Search**: Multi-beam search with configurable width\n- **Repetition Penalty**: Reduces repetitive generation\n\n## Example Usage\n\n```python\nfrom model import Transformer\nfrom tokenizer import initialize_tokenizer\nfrom inference import topk_sampling, beam_search_corrected\n\n# Initialize model and tokenizer\ntokenizer = initialize_tokenizer()\nmodel = Transformer(src_vocab_size=len(tokenizer), tgt_vocab_size=len(tokenizer))\n\n# Generate text\nprompt = \"Hello, how are you?\"\ngenerated = topk_sampling(model, prompt, tokenizer, device=\"cuda\", max_length=50)\nprint(generated)\n```\n\n## Customization\n\n### Adding New Datasets\nModify `data.py` to load your dataset:\n\n```python\ndef load_datasets(token, sample_size=None):\n    # Load your custom dataset here\n    dataset = load_dataset(\"your_dataset\")\n    return dataset\n```\n\n### Changing Model Size\nAdjust parameters in `config.py`:\n\n```python\nembeddings_dims = 768  # Larger model\nno_of_heads = 12       # More attention heads\nno_of_decoder_layers = 12  # Deeper model\n```\n\n## Requirements\n\n- Python 3.8+\n- PyTorch 2.0+\n- Transformers\n- Datasets\n- WandB\n- CUDA-capable GPU (recommended)\n\n## Model Resources\n\n- **Hugging Face Model**: [YuvrajSingh9886/SmolTransformer](https://huggingface.co/YuvrajSingh9886/SmolTransformer)\n- **Training Report**: [Weights & Biases Report](https://wandb.ai/rentio/Translation/reports/Translation--VmlldzoxMzY3OTg3MQ?accessToken=3hspzhfiyo1ekagen3o0ly0nmuqhhs5jzfpno9vb0oei2rwyum0hsgdrmfjqsycg)\n\n## License\n\nThis project is open source and available under the MIT License.\n\n## Contributing\n\nFeel free to submit issues, fork the repository, and create pull requests for any improvements.\n",
      "tags": [
        "transformer",
        "pytorch",
        "translation",
        "compact",
        "interactive",
        "optimized",
        "educational"
      ],
      "github_url": "https://github.com/YuvrajSingh-mist/SmolHub/tree/main/SmolTransformer",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/SmolHub/contents/SmolTransformer?ref=main",
      "download_url": null,
      "created_date": "2025-08-08",
      "github_date": "2025-08-08"
    },
    {
      "name": "StoryKimi",
      "display_name": "Story Kimi",
      "description": "A PyTorch implementation of a DeepSeek V3 inspired transformer model with Mixture of Experts (MoE), Latent Attention, and other advanced features.",
      "readme_content": "# StoryKimi - DeepSeek V3 Inspired Model\n\nA PyTorch implementation of a DeepSeek V3 inspired transformer model with Mixture of Experts (MoE), Latent Attention, and other advanced features.\n\n![StoryKimi Model](images/image.png)\n\n## 📊 Training Results & Model Weights\n\n**📈 View Training Report**: [StoryKimi Training Results on WandB](https://wandb.ai/rentio/DSV-Training/reports/SmolKimi-A-smaller-Kimi-K2---VmlldzoxMzYwNDQ4Mg?accessToken=lfs6n1y7gn8q0f0dwilta8yuwzxel45ztzbbcavwbqp7jsyv1p7cz9elflycv9fg)\n\n**💾 Download Pre-trained Weights**: \n- **Hugging Face Model**: [YuvrajSingh9886/StoryKimi](https://huggingface.co/YuvrajSingh9886/StoryKimi)\n- **WandB Checkpoints**: Check the WandB report above for additional trained model checkpoints\n\n## Features\n\n- **Latent Attention**: Efficient attention mechanism with compressed key-value representations\n- **Mixture of Experts (MoE)**: 8 experts with top-2 routing and shared expert support\n- **SWiGLU Activation**: Advanced activation function in expert layers\n- **Sinusoidal Positional Embeddings**: Position encoding for sequence understanding\n- **Liger Kernels**: Optimized kernels for faster training (optional)\n- **Distributed Training**: Support for multi-GPU training with DDP\n- **Advanced Optimizer**: Muon optimizer with auxiliary Adam for better convergence\n- **Gradio Interface**: Interactive web interface for text generation\n\n## Model Architecture\n\n### Default Configuration\n- **Embedding Dimensions**: 384\n- **Decoder Layers**: 6\n- **Attention Heads**: 8\n- **MoE Experts**: 8 (top-2 routing)\n- **Block Size**: 128 tokens\n- **Vocabulary Size**: Based on Llama-2-7b tokenizer (~32,000 tokens)\n- **Latent Dimension**: 64 (for compressed attention)\n\n### Full Parameter List\n\n#### Model Architecture Parameters\n- `--block_size`: Maximum sequence length (default: 128)\n- `--batch_size`: Training batch size (default: 256)\n- `--embeddings_dims`: Model embedding dimensions (default: 384)\n- `--no_of_heads`: Number of attention heads (default: 8)\n- `--no_of_decoder_layers`: Number of decoder layers (default: 6)\n- `--latent_dim`: Latent dimension for attention (default: 64)\n\n#### Mixture of Experts (MoE) Parameters\n- `--experts`: Number of MoE experts (default: 8)\n- `--top_experts`: Number of experts to route to (default: 2)\n- `--use_shared_expert`: Enable shared expert in MoE (default: True)\n- `--noisy_topk`: Use noisy top-k routing (default: False)\n- `--useauxFreeLoadBalancingLoss`: Use auxiliary-free load balancing loss (default: True)\n- `--aux_free_bias_update_rate`: Bias update rate for load balancing (default: 0.001)\n- `--loss_scale`: Loss scaling factor (default: 0.3)\n\n#### Training Hyperparameters\n- `--epochs`: Number of training epochs (default: 1)\n- `--max_lr`: Maximum learning rate (default: 6e-4)\n- `--weight_decay_optim`: Weight decay for optimizer (default: 0.1)\n- `--beta_1`: Beta1 for optimizer (default: 0.9)\n- `--beta_2`: Beta2 for optimizer (default: 0.95)\n- `--eps`: Epsilon for optimizer (default: 1e-8)\n- `--clip`: Gradient clipping value (default: 1.0)\n\n#### Regularization Parameters\n- `--dropout`: Dropout rate (default: 0.1)\n- `--attn_dropout`: Attention dropout rate (default: 0.1)\n\n#### System Configuration\n- `--device`: Device to use (default: 'cuda')\n- `--use_checkpointing`: Use gradient checkpointing (default: False)\n- `--use_liger`: Use Liger kernels for optimization (default: True)\n- `--ignore_pad_token_in_loss`: Ignore padding tokens in loss calculation (default: True)\n\n#### Data Configuration\n- `--vocab_size`: Vocabulary size (default: 32000, updated based on tokenizer)\n- `--base_freq`: Base frequency for positional encoding (default: 100000)\n- `--hf_token`: Hugging Face token for accessing gated models like Llama-2 (default: None)\n- `--dataset`: Dataset to use ('tinystories', 'fineweb', 'tinyshakespeare') (default: 'tinystories')\n\n#### Generation Parameters\n- `--generation_max_length`: Maximum length for text generation (default: 50)\n- `--generation_top_k`: Top-k value for sampling (default: 50)\n- `--generation_temperature`: Temperature for sampling (default: 1.0)\n\n#### Logging and Checkpointing\n- `--log_interval`: Steps between logging (default: 100)\n- `--save_interval`: Steps between saving checkpoints (default: 2000)\n- `--eval_interval`: Steps between evaluation (default: 400)\n- `--eval_iters`: Number of iterations for evaluation (default: 400)\n- `--warmup_iters`: Number of warmup iterations (default: 400)\n- `--total_iters`: Total training iterations (default: 10000)\n- `--lr_decay_iters`: Learning rate decay iterations (default: 10000)\n- `--wandb_project`: Wandb project name (default: 'storykimi')\n- `--wandb_run_name`: Wandb run name (default: None)\n\n#### Batch Size Configuration\n- `--total_batch_size`: Total batch size for gradient accumulation (default: 524288)\n- `--micro_batch_size`: Micro batch size (default: batch_size)\n\n#### Distributed Training\n- `--use_ddp`: Use distributed data parallel (default: False)\n\n## Quick Start\n\n### Installation\n\n```bash\nchmod +x install.sh\n./install.sh\n```\n\n### Using Pre-trained Weights\n\n1. **Download Model Weights**: \n   - **Option 1**: Download from [Hugging Face - YuvrajSingh9886/StoryKimi](https://huggingface.co/YuvrajSingh9886/StoryKimi)\n   - **Option 2**: Visit the [WandB Training Report](https://wandb.ai/rentio/DSV-Training/reports/SmolKimi-A-smaller-Kimi-K2---VmlldzoxMzYwNDQ4Mg?accessToken=lfs6n1y7gn8q0f0dwilta8yuwzxel45ztzbbcavwbqp7jsyv1p7cz9elflycv9fg) for additional checkpoints\n   - Place downloaded files in the `checkpoints/` directory\n\n2. **Load Pre-trained Model for Inference**:\n   ```bash\n   # Using the Gradio web interface\n   python gradio/app.py --hf_token \"your_token_here\"\n   \n   # Or use in your own code\n   python inference.py --checkpoint_path checkpoints/your_checkpoint.pt\n   \n   # Using Hugging Face transformers (if available)\n   from transformers import AutoModel, AutoTokenizer\n   model = AutoModel.from_pretrained(\"YuvrajSingh9886/StoryKimi\")\n   ```\n\n### Important: Hugging Face Token Setup\n\nSince this model uses the Llama-2 tokenizer, you'll need a Hugging Face token to access the gated model. \n\n1. **Get a Hugging Face Token:**\n   - Go to [Hugging Face Settings](https://huggingface.co/settings/tokens)\n   - Create a new token with \"Read\" permissions\n   - Accept the Llama-2 license at [meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)\n\n2. **Set your token in one of these ways:**\n   ```bash\n   # Option 1: Environment variable (recommended)\n   export HF_TOKEN=\"your_token_here\"\n   \n   # Option 2: Pass as command line argument\n   python trainer.py --hf_token \"your_token_here\"\n   ```\n\n### Training Examples\n\n#### Basic Training (Single GPU)\n```bash\n# With environment variable\nexport HF_TOKEN=\"your_token_here\"\npython trainer.py\n\n# With command line argument\npython trainer.py --hf_token \"your_token_here\"\n```\n\n#### Training with Custom Parameters\n```bash\n# Train with larger model\npython trainer.py --hf_token \"your_token_here\" --embeddings_dims 512 --no_of_heads 16 --no_of_decoder_layers 8\n\n# Train with different dataset\npython trainer.py --hf_token \"your_token_here\" --dataset fineweb --epochs 3\n\n# Train with custom learning rate and batch size\npython trainer.py --hf_token \"your_token_here\" --max_lr 1e-3 --batch_size 128 --block_size 256\n\n# Train with more experts\npython trainer.py --hf_token \"your_token_here\" --experts 16 --top_experts 4\n\n# Train without shared expert\npython trainer.py --hf_token \"your_token_here\" --use_shared_expert False\n\n# Train with noisy top-k routing\npython trainer.py --hf_token \"your_token_here\" --noisy_topk True\n```\n\n#### Multi-GPU Distributed Training\n```bash\n# Set token as environment variable for distributed training\nexport HF_TOKEN=\"your_token_here\"\n\n# 2 GPUs\ntorchrun --nproc_per_node=2 trainer.py\n\n# 4 GPUs with custom parameters\ntorchrun --nproc_per_node=4 trainer.py --batch_size 128 --embeddings_dims 512\n\n# 8 GPUs with large model configuration\ntorchrun --nproc_per_node=8 trainer.py \\\n    --embeddings_dims 768 \\\n    --no_of_heads 12 \\\n    --no_of_decoder_layers 12 \\\n    --experts 16 \\\n    --top_experts 4 \\\n    --batch_size 64 \\\n    --block_size 512\n```\n\n#### Advanced Training Configurations\n\n##### High-Performance Setup\n```bash\nexport HF_TOKEN=\"your_token_here\"\npython trainer.py \\\n    --embeddings_dims 768 \\\n    --no_of_heads 12 \\\n    --no_of_decoder_layers 12 \\\n    --experts 16 \\\n    --top_experts 4 \\\n    --batch_size 32 \\\n    --block_size 512 \\\n    --max_lr 3e-4 \\\n    --epochs 5 \\\n    --use_liger True \\\n    --wandb_project \"storykimi-large\"\n```\n\n##### Experimental Setup\n```bash\nexport HF_TOKEN=\"your_token_here\"\npython trainer.py \\\n    --noisy_topk True \\\n    --use_shared_expert False \\\n    --aux_free_bias_update_rate 0.01 \\\n    --loss_scale 0.5 \\\n    --dropout 0.2 \\\n    --attn_dropout 0.15 \\\n    --wandb_project \"storykimi-experimental\"\n```\n\n##### Memory-Efficient Setup\n```bash\nexport HF_TOKEN=\"your_token_here\"\npython trainer.py \\\n    --use_checkpointing True \\\n    --batch_size 64 \\\n    --micro_batch_size 16 \\\n    --total_batch_size 262144 \\\n    --block_size 128\n```\n\n### Inference with Gradio\n\n```bash\n# Set your HF token\nexport HF_TOKEN=\"your_token_here\"\n\n# Run the Gradio app\ncd gradio\npython app.py --hf_token \"your_token_here\"\n\n# Or with environment variable\ncd gradio\npython app.py\n\n# With custom port and public sharing\ncd gradio\npython app.py --hf_token \"your_token_here\" --port 8080 --share\n```\n\n### Help and Parameter Information\n\n```bash\n# View all available parameters\npython trainer.py --help\n\n# View Gradio app parameters\ncd gradio\npython app.py --help\n```\n\n### Environment Variables\n\nYou can set the following environment variables instead of passing them as arguments:\n\n```bash\n# Hugging Face token (recommended approach)\nexport HF_TOKEN=\"your_token_here\"\n\n# Wandb API key (optional, for experiment tracking)\nexport WANDB_API_KEY=\"your_wandb_key_here\"\n```\n\n## File Structure\n\n```\nStoryKimi/\n├── config.py          # Model configuration and hyperparameters with argparse\n├── model.py           # Model architecture (DeepSeekV3, MoE, Attention, etc.)\n├── tokenizer.py       # Tokenizer setup\n├── data.py           # Data loading and preparation\n├── inference.py      # Inference functions and text generation\n├── trainer.py        # Main training loop with DDP support\n├── install.sh        # Setup script\n├── requirements.txt  # Python dependencies\n├── gradio/\n│   ├── app.py        # Gradio web interface\n│   └── requirements.txt\n└── generated_data/   # Generated text outputs\n```\n\n## Training Features\n\n- **Gradient Accumulation**: Configurable batch size scaling\n- **Learning Rate Scheduling**: Cosine decay with warmup\n- **Gradient Clipping**: Prevents gradient explosion\n- **Wandb Integration**: Experiment tracking and logging\n- **Checkpointing**: Regular model checkpoints during training\n- **Loss Calculation**: Optimized cross-entropy with padding token handling\n- **Distributed Training**: Multi-GPU support with DDP\n- **Memory Optimization**: Gradient checkpointing support\n\n## Generation Methods\n\n1. **Top-k Sampling**: Traditional sampling with temperature control\n2. **Beam Search**: Deterministic search for high-quality outputs\n\n## Advanced Usage\n\n### Configuration Files\nAll parameters can be set via command line arguments. For complex configurations, consider creating shell scripts:\n\n```bash\n#!/bin/bash\n# large_model_config.sh\npython trainer.py \\\n    --embeddings_dims 1024 \\\n    --no_of_heads 16 \\\n    --no_of_decoder_layers 24 \\\n    --experts 32 \\\n    --top_experts 8 \\\n    --batch_size 16 \\\n    --block_size 1024 \\\n    --max_lr 1e-4 \\\n    --epochs 10 \\\n    --use_liger True \\\n    --use_checkpointing True \\\n    --wandb_project \"storykimi-large-scale\"\n```\n\n### Custom Dataset Training\n```bash\n# TinyStories (default)\npython trainer.py --dataset tinystories\n\n# FineWeb (large scale)\npython trainer.py --dataset fineweb --epochs 3 --batch_size 64\n\n# TinyShakespeare (character level)\npython trainer.py --dataset tinyshakespeare --block_size 256\n```\n\n### Monitoring and Logging\n```bash\n# Custom wandb configuration\npython trainer.py \\\n    --wandb_project \"my-experiment\" \\\n    --wandb_run_name \"test-run-1\" \\\n    --log_interval 50 \\\n    --eval_interval 200 \\\n    --save_interval 1000\n```\n\n### Hardware-Specific Optimizations\n\n#### For High-Memory GPUs (A100, H100)\n```bash\npython trainer.py \\\n    --batch_size 512 \\\n    --block_size 2048 \\\n    --embeddings_dims 1024 \\\n    --total_batch_size 1048576\n```\n\n#### For Low-Memory GPUs (RTX 3080, 4080)\n```bash\npython trainer.py \\\n    --batch_size 32 \\\n    --micro_batch_size 8 \\\n    --block_size 128 \\\n    --use_checkpointing True \\\n    --embeddings_dims 256\n```\n\n### Usage Examples\n\n#### Basic Training\n```python\nfrom trainer import train\ntrain()\n```\n\n#### Text Generation\n```python\nfrom inference import topk_sampling\nfrom model import DeepSeekV3\nfrom config import ModelArgs, get_args\n\n# Load with custom config\nargs = get_args()\nmodel_args = ModelArgs(args)\nmodel = DeepSeekV3(device='cuda')\ntext = topk_sampling(model, \"Once upon a time\", device='cuda')\n```\n\n#### Loading a Trained Model\n```python\nimport torch\nfrom model import DeepSeekV3\nfrom config import ModelArgs, get_args\n\n# Load saved model\nargs = get_args()\nmodel_args = ModelArgs(args)\nmodel = DeepSeekV3(device='cuda')\nmodel.load_state_dict(torch.load('path/to/checkpoint.pt'))\nmodel.eval()\n```\n\n## Performance Tips\n\n1. **Use Mixed Precision**: Enable automatic mixed precision for faster training\n2. **Gradient Checkpointing**: Use `--use_checkpointing True` for memory-constrained setups\n3. **Liger Kernels**: Keep `--use_liger True` for optimized operations\n4. **Batch Size Tuning**: Start with smaller batch sizes and increase gradually\n5. **Block Size**: Larger block sizes improve quality but require more memory\n\n## Troubleshooting\n\n### Common Issues\n\n#### Authentication Error (401)\n```bash\n# Make sure you have accepted the Llama-2 license and have a valid token\n# Visit: https://huggingface.co/meta-llama/Llama-2-7b-hf\n# Then set your token:\nexport HF_TOKEN=\"your_token_here\"\n```\n\n#### Out of Memory (OOM)\n```bash\n# Reduce batch size and enable checkpointing\npython trainer.py --hf_token \"your_token_here\" --batch_size 16 --use_checkpointing True\n\n# Use gradient accumulation\npython trainer.py --hf_token \"your_token_here\" --batch_size 32 --micro_batch_size 8\n```\n\n#### Slow Training\n```bash\n# Enable Liger kernels and increase batch size\npython trainer.py --hf_token \"your_token_here\" --use_liger True --batch_size 256\n\n# Use multiple GPUs\nexport HF_TOKEN=\"your_token_here\"\ntorchrun --nproc_per_node=4 trainer.py\n```\n\n#### NaN Loss\n```bash\n# Reduce learning rate and enable gradient clipping\npython trainer.py --hf_token \"your_token_here\" --max_lr 1e-4 --clip 0.5\n```\n\n## Contributing\n\nFeel free to contribute improvements, bug fixes, or new features!\n\n## Requirements\n\n- Python 3.8+\n- PyTorch 2.0+\n- Transformers\n- Datasets\n- Gradio\n- Wandb\n- Liger-kernel (optional)\n- Muon optimizer\n\n## License\n\nMIT License\n",
      "tags": [
        "mixtral",
        "llama",
        "kimi",
        "transformer",
        "pytorch",
        "storytelling",
        "generation",
        "compact",
        "vision",
        "distributed",
        "interactive",
        "optimized",
        "experimental",
        "educational"
      ],
      "github_url": "https://github.com/YuvrajSingh-mist/SmolHub/tree/main/StoryKimi",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/SmolHub/contents/StoryKimi?ref=main",
      "download_url": null,
      "created_date": "2025-08-08",
      "github_date": "2025-08-08"
    },
    {
      "name": "StoryLlama",
      "display_name": "Story Llama",
      "description": "- So, I trained a Llama a 88M architecture I coded from ground up to build a small instruct model, going through the below-mentioned stages from scrat...",
      "readme_content": "\n# Introducing StoryLlama - A Smaller Language Model for Bedtime Stories! \n\n- So, I trained a Llama a 88M architecture I coded from ground up to build a small instruct model, going through the below-mentioned stages from scratch.\n- Trained on TiyStories dataset form HuggingFace consisting of 4B tokens for a total of 5000 steps\n\n\n\n ###  Pretraining\n\n#### Dataset\n\n - I used the [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset from HuggingFace.\n\n  1) Train dataset - 2 M records approx\n  2) Val dataset - 26K records approx\n\n\n\n---\n\n####  ModelArgs (Hyperparameters)\n\n\nBelow is a table summarizing the configuration parameters for the model:\n\n| Parameter                      | Description                                                                 | Default Value                     | Type      |\n|--------------------------------|-----------------------------------------------------------------------------|-----------------------------------|-----------|\n| `epochs`                       | Number of training epochs                                                   | `4`                               | `int`     |\n| `block_size`                   | Size of each block (context length)                                         | `512`                             | `int`     |\n| `batch_size`                   | Batch size for training                                                    | `64`                              | `int`     |\n| `inference`                    | Inference mode (not specified)                                              | `None`                            | `None`    |\n| `embeddings_dims`              | Dimensionality of embeddings                                                | `512`                             | `int`     |\n| `attn_dropout`                 | Dropout rate for attention layers                                           | `0.1`                             | `float`   |\n| `no_of_heads`                  | Number of attention heads                                                   | `8`                               | `int`     |\n| `dropout`                      | Dropout rate for the model                                                  | `0.1`                             | `float`   |\n| `val_epochs`                   | Number of validation epochs                                                 | `2`                               | `int`     |\n| `max_lr`                       | Maximum learning rate                                                       | `6e-4`                            | `float`   |\n| `no_of_decoder_layers`         | Number of decoder layers                                                    | `8`                               | `int`     |\n| `weight_decay_optim`           | Weight decay for the optimizer                                              | `0.1`                             | `float`   |\n| `beta_1`                       | Beta 1 for Adam optimizer                                                   | `0.9`                             | `float`   |\n| `beta_2`                       | Beta 2 for Adam optimizer                                                   | `0.95`                            | `float`   |\n| `clip`                         | Gradient clipping value                                                     | `1.0`                             | `float`   |\n| `device`                       | Device to run the model (`cuda` or `cpu`)                                   | `'cuda'`                          | `str`     |\n| `no_kv_heads`                  | Number of key-value heads                                                   | `2`                               | `int`     |\n| `vocab_size`                   | Size of the vocabulary                                                      | `50304`                           | `int`     |\n| `eps`                          | Epsilon value for numerical stability                                       | `1e-5`                            | `float`   |\n| `dtype`                        | Data type for tensors (`bfloat16` if supported, else `float16`)             | `'bfloat16'` or `'float16'`       | `str`     |\n| `save_checkpoint_dir`          | Directory to save model checkpoints                                         | `\"checkpoints\"`                   | `str`     |\n| `prompt`                       | Default prompt for inference                                                | `\"Once upon a time\"`              | `str`     |\n| `save_checkpoint_iter`         | Save checkpoint every N iterations                                         | `50`                              | `int`     |\n| `total_iters`                  | Total number of training iterations                                        | `10000`                           | `int`     |\n| `eval_iters`                   | Evaluate model every N iterations                                          | `50`                              | `int`     |\n| `eval_check`                   | Check evaluation metrics every N iterations                                | `100`                             | `int`     |\n| `warmup_iters`                 | Number of warmup iterations for learning rate scheduling                   | `700`                             | `int`     |\n| `min_lr`                       | Minimum learning rate (10% of `max_lr`)                                     | `0.1 * max_lr`                    | `float`   |\n| `lr_decay_iters`               | Number of iterations for learning rate decay                               | `10000`                           | `int`     |\n| `total_batch_size`             | Total batch size across all devices                                         | `524288`                          | `int`     |\n| `micro_batch_size`             | Micro batch size per device                                                | `batch_size`                      | `int`     |\n| `gradient_accumulation_steps`  | Gradient accumulation steps                                                 | 524288 | `int` |\n---\n#### Hardware Setup\n\n - Used DPP using Pytorch torchrun consisting of 2x GeForce RTX A100 AXM (80gb VRAM each) rented on runpod.io\n - The model is a 0.768GB in size but needs around 4 GB of VRAM when loaded in fp32 precision\n---\n\n#### Frameworks:\n**Pytorch**\n\n\n--- \n\n#### Epochs/Steps\n- Iterations (train) = 5k \n\n- Val iterations = every 50 steps\n---\n\n#### Losses\n- Train loss - 1.43\n\n- Val loss - 1.45\n\n---\n\n#### Screenshots of the loss curves\n\n- Loss Curves (Train and Val)\n\n![Loss Curves (Train and Val)](imgs/loss_curves.jpg)\n\n--- \n#### Output\n\n- Prompt: Once upon a time\n\n![Prompt: Once upon a time](imgs/sample.jpg)\n\n---\n\n### Local setup\n\n\n### Requirements\n\n\n\n```python\ngit [clone the repo](https://github.com/YuvrajSingh-mist/StoryLlama.git)\ncd StoryLlama\nbash ./install.sh\n\n```\n- A wandb.ai account for plotting graphs for your loss curves\n\n- On your terminal run\n```python\nwandb login\n```\n\n- Enter the api key and follow the instructions and once you are succesfully logged in follow the given steps\n\n\n- Download the model\n\n```python\ncd gradio/\n\npython app.py\n```\n\n\n---\n\n### Running \n\n\n#### Training a model\n\n- Kindly change 'device' to any of your available cuda gpus.\n\nTo run:\n\n```python\nbash ./install.sh\n```\n\n```python\ntorchrun --standalone --nproc_per_node=gpu trainer.py \\\n    --epochs 10 \\\n    --block_size 256 \\\n    --batch_size 128 \\\n    --embeddings_dims 768 \\\n    --attn_dropout 0.2 \\\n    --no_of_heads 12 \\\n    --dropout 0.2 \\\n    --val_epochs 3 \\\n    --max_lr 5e-4 \\\n    --no_of_decoder_layers 6 \\\n    --weight_decay_optim 0.01 \\\n    --beta_1 0.85 \\\n    --beta_2 0.99 \\\n    --clip 0.5 \\\n    --device \"cuda\" \\\n    --no_kv_heads 4 \\\n    --vocab_size 50257 \\\n    --eps 1e-6 \\\n    --dtype \"float16\" \\\n    --save_checkpoint_dir \"model_checkpoints\" \\\n    --prompt \"Once upon a time\" \\\n    --save_checkpoint_iter 100 \\\n    --total_iters 5000 \\\n    --eval_iters 200 \\\n    --eval_check 500 \\\n    --warmup_iters 1000 \\\n    --min_lr 1e-5 \\\n    --lr_decay_iters 2000 \\\n    --total_batch_size 262144 \\\n    --micro_batch_size 128 \\\n    --gradient_accumulation_steps 4\n\n```\n--standalone - if all the gpu are on one server\n--npro_per_node - number of gpus available and use the keyword gpu to use all\n\n#### Inference on a model\n\n```python \npython inference.py --prompt \"Once upon a time\" --max_length 100 --temperature 0.8 --topk 50 \n```\n\n",
      "tags": [
        "llama",
        "transformer",
        "pytorch",
        "storytelling",
        "compact",
        "interactive",
        "educational"
      ],
      "github_url": "https://github.com/YuvrajSingh-mist/SmolHub/tree/main/StoryLlama",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/SmolHub/contents/StoryLlama?ref=main",
      "download_url": null,
      "created_date": "2025-08-08",
      "github_date": "2025-08-08"
    },
    {
      "name": "StoryMixtral",
      "display_name": "Story Mixtral",
      "description": "A PyTorch implementation of a Mixtral inspired transformer model with Mixture of Experts (MoE), Flash Attention, and other advanced features.",
      "readme_content": "\n# StoryMixtral - Mixtral Inspired Model\n\nA PyTorch implementation of a Mixtral inspired transformer model with Mixture of Experts (MoE), Flash Attention, and other advanced features.\n\n## Examples\n\nProvided under the `generated_data/` directory, these examples showcase the model's capabilities in text generation and understanding.\n\n![StoryMixtral Model](images/image.png)\n\n## 📊 Training Results & Model Weights\n\n**📈 View Training Report**: [StoryMixtral Training Results on WandB](https://wandb.ai/rentio/Mixtral-DDP-Pretrain-10-billion-tokens/reports/StoryMixtral--VmlldzoxMzYyNzc0OQ?accessToken=nybd4lxybsbq5k5fh2dqjcucdawilt3fossn583wv6jiu8tbdzcybiihe7rhsqmq)\n\n**💾 Download Pre-trained Weights**: \n- **Hugging Face Model**: [YuvrajSingh9886/StoryMixtral](https://huggingface.co/YuvrajSingh9886/StoryMixtral)\n- **WandB Checkpoints**: Check the WandB report above for additional trained model checkpoints\n\n## Features\n\n- **Flash Attention**: Efficient attention mechanism with memory optimization\n- **Mixture of Experts (MoE)**: 8 experts with top-2 routing and noisy top-k support\n- **SWiGLU Activation**: Advanced activation function in expert layers\n- **Rotary Positional Embeddings**: Position encoding for sequence understanding\n- **Liger Kernels**: Optimized kernels for faster training (optional)\n- **Distributed Training**: Support for multi-GPU training with DDP\n- **Advanced Optimizer**: AdamW optimizer with custom learning rate scheduling\n- **Gradio Interface**: Interactive web interface for text generation\n\n## Model Architecture\n\n### Default Configuration\n- **Embedding Dimensions**: 512\n- **Decoder Layers**: 8\n- **Attention Heads**: 8\n- **MoE Experts**: 8 (top-2 routing)\n- **Block Size**: 1024 tokens\n- **Vocabulary Size**: Based on Llama-2-7b tokenizer (~32,000 tokens)\n- **Batch Size**: 16\n\n### Full Parameter List\n\n#### Model Architecture Parameters\n- `epochs`: Number of training epochs (default: 4)\n- `block_size`: Maximum sequence length (default: 1024)\n- `batch_size`: Training batch size (default: 16)\n- `embeddings_dims`: Model embedding dimensions (default: 512)\n- `no_of_heads`: Number of attention heads (default: 8)\n- `no_of_decoder_layers`: Number of decoder layers (default: 8)\n- `attn_dropout`: Attention dropout rate (default: 0.1)\n- `dropout`: General dropout rate (default: 0.1)\n\n#### Mixture of Experts (MoE) Parameters\n- `experts`: Number of MoE experts (default: 8)\n- `top_experts`: Number of experts to route to (default: 2)\n- `noisy_topk`: Use noisy top-k routing (default: False)\n\n#### Training Hyperparameters\n- `max_lr`: Maximum learning rate (default: 6e-4)\n- `weight_decay_optim`: Weight decay for optimizer (default: 0.01)\n- `beta_1`: Beta1 for optimizer (default: 0.9)\n- `beta_2`: Beta2 for optimizer (default: 0.95)\n- `eps`: Epsilon for optimizer (default: 1e-8)\n- `clip`: Gradient clipping value (default: 1.0)\n\n#### System Configuration\n- `device`: Device to use (default: 'cuda:9')\n- `use_checkpointing`: Use gradient checkpointing (default: False)\n- `use_liger`: Use Liger kernels for optimization (default: True)\n- `use_flash_attention`: Use Flash Attention (default: True)\n- `use_compile`: Use torch.compile (default: True)\n\n#### Data Configuration\n- `vocab_size`: Vocabulary size (default: based on tokenizer + 768)\n- `val_epochs`: Validation frequency (default: 2)\n\n## Quick Start\n\n### Installation\n\n```bash\nchmod +x install.sh\n./install.sh\n```\n\n### Important: Hugging Face Token Setup\n\nSince this model uses the Llama-2 tokenizer, you'll need a Hugging Face token to access the gated model. \n\n1. **Get a Hugging Face Token:**\n   - Go to [Hugging Face Settings](https://huggingface.co/settings/tokens)\n   - Create a new token with \"Read\" permissions\n   - Accept the Llama-2 license at [meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)\n\n2. **Set your token in config.py:**\n   ```python\n   TOKEN = 'your_token_here'\n   ```\n\n### Using Pre-trained Weights\n\n1. **Download Model Weights**: \n   - **Option 1**: Download from [Hugging Face - YuvrajSingh9886/StoryMixtral](https://huggingface.co/YuvrajSingh9886/StoryMixtral)\n   - **Option 2**: Visit the [WandB Training Report](https://wandb.ai/rentio/Mixtral-DDP-Pretrain-10-billion-tokens) for additional checkpoints\n   - Place downloaded files in the `checkpoints/` directory\n\n2. **Load Pre-trained Model for Inference**:\n   ```bash\n   # Using the Gradio web interface\n   cd gradio\n   python app.py\n   \n   # Or use in your own code\n   python inference.py\n   ```\n\n### Training Examples\n\n#### Basic Training (Single GPU)\n```bash\npython trainer.py\n```\n\n#### Training with Custom Parameters\n```bash\n# Train with larger model (modify config.py)\npython trainer.py\n\n# Train with different dataset (modify data.py)\npython trainer.py\n```\n\n#### Multi-GPU Distributed Training\n```bash\n# 2 GPUs\ntorchrun --nproc_per_node=2 trainer.py\n\n# 4 GPUs\ntorchrun --nproc_per_node=4 trainer.py\n\n# 8 GPUs\ntorchrun --nproc_per_node=8 trainer.py\n```\n\n### Inference with Gradio\n\n**HF_TOKEN** should be set in `config.py` to use the Gradio interface. Moreover, set your token as follows:\n\n```python\n export HF_TOKEN=<TOKEN_HERE>\n```\n\n\n```bash\n# Run the Gradio app\ncd gradio\npython app.py\n\n# With custom checkpoint (edit app.py to point to your checkpoint)\ncd gradio\npython app.py\n```\n\n## File Structure\n\n```\nStoryMixtral/\n├── config.py          # Model configuration and hyperparameters\n├── model.py           # Model architecture (Mixtral, MoE, Attention, etc.)\n├── data.py           # Data loading and preparation\n├── inference.py      # Inference functions and text generation\n├── trainer.py        # Main training loop with DDP support\n├── install.sh        # Setup script\n├── requirements.txt  # Python dependencies\n├── model_summary.py  # Model architecture summary\n├── gradio/\n│   └── app.py        # Gradio web interface\n├── checkpoints/      # Model checkpoints\n├── generated_data/   # Generated text outputs\n├── images/           # Project images\n└── old/             # Original files\n```\n\n\n\n## Training Features\n\n- **Gradient Accumulation**: Configurable batch size scaling\n- **Learning Rate Scheduling**: Cosine decay with warmup\n- **Gradient Clipping**: Prevents gradient explosion\n- **Wandb Integration**: Experiment tracking and logging\n- **Checkpointing**: Regular model checkpoints during training\n- **Loss Calculation**: Optimized cross-entropy with padding token handling\n- **Distributed Training**: Multi-GPU support with DDP\n- **Memory Optimization**: Gradient checkpointing support\n\n## Generation Methods\n\n1. **Top-k Sampling**: Traditional sampling with temperature control\n\n## Advanced Usage\n\n### Configuration\nAll parameters can be configured by modifying `config.py`:\n\n```python\n@dataclass\nclass ModelArgs:\n    epochs = 4\n    block_size = 1024\n    batch_size = 16\n    embeddings_dims = 512\n    # ... other parameters\n```\n\n### Custom Dataset Training\nModify `data.py` to use different datasets:\n```python\n# TinyStories (default)\ntinystories = True\nfw = False\n\n# FineWeb\ntinystories = False\nfw = True\n```\n\n### Monitoring and Logging\nTraining automatically logs to WandB with project name \"Mixtral-DDP-Pretrain-10-billion-tokens\"\n\n## Performance Tips\n\n1. **Use Liger Kernels**: Keep `use_liger = True` for optimized operations\n2. **Flash Attention**: Keep `use_flash_attention = True` for memory efficiency\n3. **Gradient Checkpointing**: Use `use_checkpointing = True` for memory-constrained setups\n4. **Batch Size Tuning**: Start with smaller batch sizes and increase gradually\n5. **Block Size**: Larger block sizes improve quality but require more memory\n\n## Troubleshooting\n\n### Common Issues\n\n#### Authentication Error (401)\n```bash\n# Make sure you have accepted the Llama-2 license and have a valid token\n# Visit: https://huggingface.co/meta-llama/Llama-2-7b-hf\n# Then set your token in config.py\n```\n\n#### Out of Memory (OOM)\n```python\n# Reduce batch size and enable checkpointing in config.py\nbatch_size = 8\nuse_checkpointing = True\n```\n\n#### Slow Training\n```python\n# Enable optimizations in config.py\nuse_liger = True\nuse_flash_attention = True\nuse_compile = True\n```\n\n## Contributing\n\nFeel free to contribute improvements, bug fixes, or new features!\n\n## Requirements\n\n- Python 3.8+\n- PyTorch 2.0+\n- Transformers\n- Datasets\n- Gradio\n- Wandb\n- Liger-kernel (optional)\n\n## License\n\nMIT License\n",
      "tags": [
        "mixtral",
        "llama",
        "transformer",
        "pytorch",
        "storytelling",
        "generation",
        "compact",
        "vision",
        "distributed",
        "interactive",
        "optimized",
        "educational"
      ],
      "github_url": "https://github.com/YuvrajSingh-mist/SmolHub/tree/main/StoryMixtral",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/SmolHub/contents/StoryMixtral?ref=main",
      "download_url": null,
      "created_date": "2025-08-08",
      "github_date": "2025-08-08"
    }
  ]
}