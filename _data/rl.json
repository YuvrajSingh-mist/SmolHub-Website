{
  "rl_implementations": [
    {
      "name": "A2C",
      "path": "A2C",
      "display_name": "A2C (A2C)",
      "description": "Implementation of A2C reinforcement learning algorithm",
      "readme_content": "# Advantage Actor-Critic (A2C) Implementation\n\n## Overview\n\nThis repository contains an implementation of the Advantage Actor-Critic (A2C) algorithm, a policy gradient method that combines the benefits of both policy-based and value-based reinforcement learning. The implementation is built with PyTorch and supports training on various Gymnasium environments, with a focus on the CartPole-v1 environment.\n\n## Results\n\n### Frozen Lake Environment\n![Frozen Lake Learning Curve](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/A2C/images/frozenlakeLoss.jpg)\n\n### Lunar Lander Environment\n![Lunar Lander Learning Curve](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/A2C/images/lunarlanderLoss.jpg)\n\n## Algorithm Description\n\nA2C is a synchronous, deterministic variant of the Asynchronous Advantage Actor-Critic (A3C) algorithm. It uses two neural networks:\n\n1. **Actor Network**: Learns a policy that maps states to actions\n2. **Critic Network**: Estimates the value function to evaluate the quality of states\n\nThe key advantage of A2C over vanilla policy gradient methods (like REINFORCE) is the use of the advantage function, which reduces variance during training by subtracting a baseline (the value function) from the returns.\n\n### The Algorithm Steps\n\n1. Initialize actor and critic networks\n2. For each episode:\n   - Collect trajectory by following the current policy\n   - For each step in the trajectory:\n     - Calculate discounted returns\n     - Estimate state values using the critic network\n     - Calculate advantages (returns - values)\n     - Update the actor network using advantage-weighted policy gradients\n     - Update the critic network to better predict state values\n3. Repeat until convergence\n\n## Implementation Details\n\n### Network Architecture\n\n**Actor Network:**\n- Input layer matching state space dimensions\n- Two hidden layers (32 nodes each) with ReLU activation\n- One hidden layer (16 nodes) with ReLU activation\n- Output layer matching action space dimensions with softmax activation\n\n**Critic Network:**\n- Input layer matching state space dimensions\n- One hidden layer (32 nodes) with ReLU activation\n- One hidden layer (16 nodes) with ReLU activation\n- Output layer with a single value prediction\n\n### Key Features\n\n- **Separate Actor-Critic Architecture**: Maintains distinct networks for policy and value estimation\n- **Advantage Calculation**: Uses the difference between returns and value estimates to reduce variance\n- **Policy Updates**: Uses the advantages to weight policy gradients\n- **Value Function Learning**: Uses MSE loss to train the critic network\n- **Gradient and Parameter Monitoring**: Tracks training dynamics with WandB\n- **Evaluation**: Periodically evaluates policy performance\n- **Video Recording**: Captures agent behavior for visualization\n\n## Usage\n\n### Prerequisites\n\n- Python 3.8+\n- PyTorch\n- Gymnasium\n- Weights & Biases (for logging)\n- TensorBoard\n- tqdm, numpy, imageio, cv2\n\n### Configuration\n\nThe `Config` class contains all hyperparameters and settings:\n\n```python\nclass Config:\n    # Experiment settings\n    exp_name = \"A2C-CartPole\"\n    seed = 42\n    env_id = \"CartPole-v1\"\n    episodes = 2000\n    # Training parameters\n    learning_rate = 2e-3\n    gamma = 0.99  # Discount factor\n    # Logging & saving\n    capture_video = True\n    save_model = True\n    use_wandb = True\n    wandb_project = \"cleanRL\"\n```\n\n### Running the Training\n\n```bash\npython train.py\n```\n\n### Monitoring\n\nThe implementation integrates with Weights & Biases for comprehensive monitoring:\n\n- **Episode Returns**: Tracks performance over time\n- **Actor and Critic Losses**: Monitors learning progress\n- **Advantage Values**: Shows the effectiveness of the advantage function\n- **Gradient Statistics**: Helps identify training instability\n- **Parameter Statistics**: Tracks weight distribution changes\n- **Evaluation Videos**: Records agent behavior periodically\n\n## Results\n\nA2C typically achieves better sample efficiency and stability compared to vanilla policy gradient methods like REINFORCE. The implementation includes:\n\n- Tensorboard logging for local visualization\n- WandB integration for comprehensive tracking\n- Video recording of trained agents\n\n## Advantages of A2C over REINFORCE\n\n1. **Reduced Variance**: The advantage function reduces the variance of policy gradient estimates\n2. **Better Sample Efficiency**: Generally learns faster with fewer samples\n3. **Stability**: More stable training due to the critic network's baseline\n4. **State-Value Estimation**: Provides value function approximation as an additional output\n\n## Extending the Implementation\n\nTo adapt this implementation to other environments:\n\n1. Change the `env_id` in the Config class\n2. Adjust the actor and critic network architectures based on state/action dimensions\n3. Tune hyperparameters like learning rate and discount factor\n4. Consider adding features like entropy regularization or n-step returns\n\n## Theoretical Background\n\nThe A2C algorithm uses the policy gradient theorem with an advantage function:\n\n∇θ J(θ) = E[∇θ log π(a|s;θ) A(s,a)]\n\nWhere:\n- J(θ) is the expected return\n- π(a|s;θ) is the policy\n- A(s,a) is the advantage function, defined as:\n  A(s,a) = Q(s,a) - V(s) ≈ r + γV(s') - V(s)\n\n## References\n\n- Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., ... & Kavukcuoglu, K. (2016). Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning.\n- Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.\n\n## License\n\nThis project is open source and available under the [MIT License](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/A2C/LICENSE).\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Reinforcement-Learning/tree/master/A2C",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Reinforcement-Learning/contents/A2C",
      "download_url": null,
      "created_date": "2025-08-21",
      "github_date": "2025-08-21",
      "category": "Actor-Critic",
      "framework": "PyTorch",
      "environment": "LunarLander",
      "categories": [
        "Actor-Critic"
      ]
    },
    {
      "name": "DDPG",
      "path": "DDPG",
      "display_name": "DDPG (DDPG)",
      "description": "Implementation of DDPG reinforcement learning algorithm",
      "readme_content": "# Deep Deterministic Policy Gradient (DDPG)\n\nThis directory contains implementations of the Deep Deterministic Policy Gradient (DDPG) algorithm for various continuous control environments.\n\n## Overview\n\nDDPG is an off-policy actor-critic algorithm designed for continuous action spaces. It combines insights from both Deep Q-Networks (DQN) and policy gradient methods to learn policies in high-dimensional, continuous action spaces.\n\nKey features of this implementation:\n- Actor-Critic architecture with separate target networks\n- Experience replay buffer for stable learning\n- Soft target network updates using Polyak averaging\n- Exploration using Ornstein-Uhlenbeck noise process\n- Support for different continuous control environments\n\n## Environments\n\nThis implementation includes support for the following environments:\n- **Pendulum-v1**: A classic control problem where the goal is to balance a pendulum in an upright position.\n- **BipedalWalker-v3**: A more challenging environment where a 2D biped robot must walk forward without falling.\n- **HalfCheetah-v5**: A MuJoCo environment where a 2D cheetah-like robot must run forward as fast as possible.\n\n\n## Configuration\n\nEach implementation includes a `Config` class that specifies the hyperparameters for training. You can modify these parameters to experiment with different settings:\n\n- `exp_name`: Name of the experiment\n- `seed`: Random seed for reproducibility\n- `env_id`: ID of the Gymnasium environment\n- `total_timesteps`: Total number of training steps\n- `learning_rate`: Learning rate for the optimizer\n- `buffer_size`: Size of the replay buffer\n- `gamma`: Discount factor\n- `tau`: Soft update coefficient for target networks\n- `batch_size`: Batch size for training\n- `exploration_fraction`: Fraction of total timesteps for exploration\n- `learning_starts`: Number of timesteps before learning starts\n\n## Architecture\n\nThe DDPG implementation includes:\n\n1. **Actor Network**: Determines the best action in a given state\n2. **Critic Network**: Evaluates the Q-value of state-action pairs\n3. **Target Networks**: Slowly updated copies of both actor and critic for stability\n4. **Replay Buffer**: Stores and samples transitions for training\n5. **Noise Process**: Adds exploration noise to actions\n\n## Logging and Monitoring\n\nTraining progress is logged using:\n- **TensorBoard**: Local visualization of training metrics\n- **Weights & Biases (WandB)**: Cloud-based experiment tracking (optional)\n- **Video Capture**: Records videos of agent performance at intervals\n\n## Dependencies\n\n- PyTorch\n- Gymnasium\n- NumPy\n- Stable-Baselines3 (for the replay buffer)\n- WandB (optional, for experiment tracking)\n- TensorBoard\n- Tqdm\n\n## References\n\n- [Continuous Control with Deep Reinforcement Learning](https://arxiv.org/abs/1509.02971) - Original DDPG paper by Lillicrap et al.\n- [CleanRL](https://github.com/vwxyzjn/cleanrl) - Inspiration for code structure and implementation style\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Reinforcement-Learning/tree/master/DDPG",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Reinforcement-Learning/contents/DDPG",
      "download_url": null,
      "created_date": "2025-08-21",
      "github_date": "2025-08-21",
      "category": "Actor-Critic",
      "framework": "PyTorch",
      "environment": "MuJoCo",
      "categories": [
        "Actor-Critic",
        "Exploration"
      ]
    },
    {
      "name": "DQN-FrozenLake",
      "path": "DQN-FrozenLake",
      "display_name": "DQN Frozenlake (DQN Frozenlake)",
      "description": "Implementation of DQN-FrozenLake reinforcement learning algorithm",
      "readme_content": "# Frozen Lake Reinforcement Learning\n\nThis project implements reinforcement learning algorithms for the Frozen Lake environment from OpenAI Gymnasium. The agent learns to navigate across a frozen lake from the start to a goal without falling into holes.\n\n![Frozen Lake Environment](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/DQN-FrozenLake/images/output.gif)\n\n## Environment Description\n\n**FrozenLake-v1** is a grid-world environment where:\n- The agent navigates on a frozen lake from start (S) to goal (G)\n- Some tiles are frozen (F) and safe to walk on\n- Some tiles have holes (H) and the agent falls if it steps on them\n- The ice is slippery, so the agent's movement can be stochastic\n\nExample 4x4 map:\n```\nSFFF\nFHFH\nFFFH\nHFFG\n```\n\n- **State space:** Discrete with 16 states (for 4x4 grid) or 64 states (for 8x8 grid)\n- **Action space:** 4 discrete actions (LEFT, DOWN, RIGHT, UP)\n- **Rewards:** +1 for reaching the goal, 0 otherwise\n\n## Algorithms Implemented\n\nThis project includes implementations of:\n\n1. **Q-Learning**: A model-free, off-policy algorithm using a tabular approach\n2. **Deep Q-Network (DQN)**: Neural network implementation for Q-learning\n3. **Double DQN**: Reducing overestimation bias with two networks\n\n## Features\n\n- Multiple map sizes (4x4 and 8x8)\n- Option for deterministic or stochastic (slippery) environments\n- Exploration vs. exploitation control with epsilon-greedy strategy\n- Visualization of learned policies\n- Tracking of training metrics\n- Integration with TensorBoard and WandB\n\n\n\n```python\nclass Config:\n    # Environment settings\n    env_id = \"FrozenLake-v1\"\n    map_size = \"4x4\"  # or \"8x8\"\n    is_slippery = True\n    \n    # Algorithm parameters\n    learning_rate = 0.1  # for Q-Learning\n    gamma = 0.99  # Discount factor\n    epsilon_start = 1.0\n    epsilon_end = 0.01\n    epsilon_decay = 0.995\n    \n    # Training parameters\n    total_episodes = 10000\n    max_steps = 100\n    \n    # For DQN\n    buffer_size = 10000\n    batch_size = 64\n    target_update = 100\n    \n    # Logging\n    use_wandb = True\n    log_interval = 100\n```\n\n## Results\n\nThe algorithms learn efficient policies for navigating the Frozen Lake:\n\n- **Q-Learning**: Converges to optimal policy after ~5000 episodes for 4x4 map\n- **DQN**: Learns good policies but might be less sample-efficient for this simple environment\n- **Double DQN**: Provides more stable learning, especially for the 8x8 map\n\n## Visualization\n\nThe project includes tools to visualize:\n- Learning curves\n- Value functions\n- Optimal policies\n- Step-by-step agent behavior\n\n\n## References\n\n- [Gymnasium FrozenLake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n- [Q-Learning Paper](https://link.springer.com/article/10.1007/BF00992698)\n- [DQN Paper](https://www.nature.com/articles/nature14236)\n- [Double DQN Paper](https://arxiv.org/abs/1509.06461)\n\n## License\n\nMIT License\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Reinforcement-Learning/tree/master/DQN-FrozenLake",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Reinforcement-Learning/contents/DQN-FrozenLake",
      "download_url": null,
      "created_date": "2025-08-21",
      "github_date": "2025-08-21",
      "category": "Exploration",
      "framework": "PyTorch",
      "environment": "Frozenlake",
      "categories": [
        "Exploration"
      ]
    },
    {
      "name": "DQN-Lunar",
      "path": "DQN-Lunar",
      "display_name": "DQN Lunar (DQN Lunar)",
      "description": "Implementation of DQN-Lunar reinforcement learning algorithm",
      "readme_content": "# Deep Q-Network (DQN) for Lunar Lander\n\nThis repository contains an implementation of a Deep Q-Network (DQN) agent that learns to play the Lunar Lander environment from OpenAI Gymnasium.\n\n![Lunar Lander Demo](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/DQN-Lunar/images/output.gif)\n\n![Lunar Lander Training Visualization](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/DQN-Lunar/images/image.png)\n## Overview\n\nThis project implements a DQN agent that learns to successfully land a lunar module on the moon's surface. The agent is trained using a reinforcement learning approach where it learns to map states to actions in order to maximize cumulative rewards.\n\n### The Lunar Lander Environment\n\nIn the LunarLander-v3 environment:\n- The goal is to land the lunar module safely between two flags\n- The agent controls the thrusters (main engine and side engines) to navigate the lander\n- The state space consists of 8 continuous variables representing position, velocity, angle, and leg contact\n- The action space consists of 4 discrete actions (fire left engine, fire main engine, fire right engine, do nothing)\n- The episode ends when the lander crashes, flies off-screen, or lands successfully\n\n## Features\n\n- **Deep Q-Network (DQN)** implementation with experience replay and target network\n- **Epsilon-greedy exploration** with linear decay\n- **TensorBoard** integration for tracking training metrics\n- **Weights & Biases (WandB)** integration for experiment tracking\n- **Video recording** of agent performance during and after training\n- Evaluation mode for testing the trained agent\n\n## Architecture\n\nThe DQN uses a simple yet effective neural network architecture:\n- Input layer: State dimension (8 for Lunar Lander)\n- Hidden layer 1: 256 neurons with ReLU activation\n- Hidden layer 2: 512 neurons with ReLU activation \n- Output layer: Action dimension (4 for Lunar Lander)\n\n\n\n### Configuration\n\nThe training parameters can be modified in the `Config` class within the `train.py` file:\n\n```python\nclass Config:\n    # Experiment settings\n    exp_name = \"DQN-CartPole\"\n    seed = 42\n    env_id = \"LunarLander-v3\"\n    \n    # Training parameters\n    total_timesteps = 1000000\n    learning_rate = 2.5e-4\n    buffer_size = 20000 \n    gamma = 0.99\n    tau = 1.0\n    target_network_frequency = 50\n    batch_size = 128\n    start_e = 1.0\n    end_e = 0.05\n    exploration_fraction = 0.5\n    learning_starts = 1000\n    train_frequency = 10\n    \n    # Logging & saving\n    capture_video = True\n    save_model = True\n    upload_model = True\n    hf_entity = \"\"  # Your Hugging Face username\n    \n    # WandB settings\n    use_wandb = True\n    wandb_project = \"cleanRL\"\n    wandb_entity = \"\"  # Your WandB username/team\n```\n\n### Hyperparameters\n\nKey hyperparameters include:\n\n- **total_timesteps**: Total number of environment steps to train for\n- **learning_rate**: Learning rate for the optimizer\n- **buffer_size**: Size of the replay buffer\n- **gamma**: Discount factor for future rewards\n- **tau**: Soft update coefficient for target network\n- **target_network_frequency**: How often to update the target network\n- **batch_size**: Batch size for sampling from replay buffer\n- **start_e/end_e/exploration_fraction**: Controls the epsilon-greedy exploration schedule\n\n## Results\n\nThe DQN agent typically learns to land successfully after about 300-500 episodes of training. Performance metrics tracked during training include:\n\n- Episode returns (rewards)\n- Episode lengths\n- TD loss\n- Epsilon value\n\n## Requirements\n\n- Python 3.7+\n- PyTorch\n- Gymnasium\n- Numpy\n- TensorBoard\n- Weights & Biases (optional for tracking)\n- Stable-Baselines3 (for the replay buffer implementation)\n- OpenCV (for video processing)\n- Imageio (for creating videos)\n\n## Acknowledgments\n\nThis implementation is inspired by various DQN implementations and the CleanRL project's approach to reinforcement learning algorithm implementation.\n\n## License\n\n[MIT License](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/DQN-Lunar/LICENSE)\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Reinforcement-Learning/tree/master/DQN-Lunar",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Reinforcement-Learning/contents/DQN-Lunar",
      "download_url": null,
      "created_date": "2025-08-21",
      "github_date": "2025-08-21",
      "category": "Exploration",
      "framework": "PyTorch",
      "environment": "LunarLander",
      "categories": [
        "Exploration"
      ]
    },
    {
      "name": "DQN-Taxi",
      "path": "DQN-Taxi",
      "display_name": "DQN Taxi (DQN Taxi)",
      "description": "Implementation of DQN-Taxi reinforcement learning algorithm",
      "readme_content": "# DQN-Taxi: Deep Q-Network for OpenAI Gym Taxi-v3\n\nThis project implements a Deep Q-Network (DQN) agent to solve the classic Taxi-v3 environment from OpenAI Gym. The agent learns to efficiently pick up and drop off passengers in a grid world using reinforcement learning.\n\n[![Taxi-v3 Demo](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/DQN-Taxi/images/output.gif)]\n\n## Environment\n- **Taxi-v3** is a discrete environment with:\n  - **State space:** 16 (or 500 for the full version)\n  - **Action space:** 6 (South, North, East, West, Pickup, Dropoff)\n- The agent receives positive rewards for successful drop-offs and negative rewards for illegal moves or time steps.\n\n## Features\n- DQN with experience replay and target network\n- Epsilon-greedy exploration\n- One-hot encoding for discrete state representation\n- Logging of Q-values, advantage, and value estimates\n- Integration with TensorBoard and Weights & Biases (WandB) for experiment tracking\n\n\n## Logging & Visualization\n- Training logs and metrics are saved for visualization in TensorBoard and/or WandB.\n- Q-values, advantage, and value estimates are logged for analysis.\n\n## Customization\n- Change hyperparameters and logging options in the `Config` class in `train.py`.\n- You can switch between different exploration strategies or network architectures as needed.\n\n## Results\nThe DQN agent should learn to solve the Taxi-v3 environment, achieving high average rewards after sufficient training.\n\n## References\n- [OpenAI Gym Taxi-v3](https://www.gymlibrary.dev/environments/toy_text/taxi/)\n- [DQN Paper (Mnih et al., 2015)](https://www.nature.com/articles/nature14236)\n- [Stable Baselines3](https://stable-baselines3.readthedocs.io/)\n\n## License\nMIT License\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Reinforcement-Learning/tree/master/DQN-Taxi",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Reinforcement-Learning/contents/DQN-Taxi",
      "download_url": null,
      "created_date": "2025-08-21",
      "github_date": "2025-08-21",
      "category": "Exploration",
      "framework": "PyTorch",
      "environment": "Taxi",
      "categories": [
        "Exploration"
      ]
    },
    {
      "name": "DQN-atari",
      "path": "DQN-atari",
      "display_name": "DQN Atari (DQN Atari)",
      "description": "Implementation of DQN-atari reinforcement learning algorithm",
      "readme_content": "# Deep Q-Learning for Atari Breakout\n\nThis repository contains an implementation of Deep Q-Network (DQN) for solving the BreakoutNoFrameskip-v4 environment from Atari. The implementation includes features such as experience replay, target networks, epsilon-greedy exploration, and convolutional neural networks for processing visual input.\n\n![Atari Breakout DQN Training](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/DQN-atari/images/image.png)\n\n![Breakout Demo](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/DQN-atari/images/output.gif)\n\n## Overview\n\nThe main training script (`train.py`) implements a DQN agent to solve the BreakoutNoFrameskip-v4 environment, where the goal is to control a paddle to bounce a ball and break bricks. The agent learns to take actions (move left, right, or stay still) to maximize the score by breaking as many bricks as possible while keeping the ball in play.\n\n## Features\n\n- **Deep Q-Network (DQN)**: Uses a convolutional neural network to approximate the Q-function from raw pixel input\n- **Atari Preprocessing**: Includes frame skipping, grayscale conversion, frame stacking, and image resizing\n- **Experience Replay**: Stores transitions in a replay buffer to break correlations between consecutive samples\n- **Target Network**: Uses a separate target network to stabilize learning\n- **Epsilon-Greedy Exploration**: Balances exploration and exploitation with a decaying epsilon\n- **Evaluation**: Periodically evaluates the model and saves videos of the agent's performance\n- **Logging**: Includes logging to TensorBoard for experiment tracking\n- **Video Recording**: Records videos during training and evaluation for visualization\n\n## Requirements\n\n```\ngymnasium\ntorch\nnumpy\ntqdm\nstable-baselines3\nimageio\nopencv-python\ntensorboard\nhuggingface_hub\nale-py\n```\n\n## Configuration\n\nThe project is configured through the `Config` class, which includes the following parameters:\n\n- **Environment Settings**:\n  - `env_id`: The Atari environment ID (default: \"BreakoutNoFrameskip-v4\")\n  - `seed`: Random seed for reproducibility (default: 42)\n\n- **Training Parameters**:\n  - `total_timesteps`: Total number of timesteps to train (default: 1,000,000)\n  - `learning_rate`: Learning rate for the optimizer (default: 2.5e-4)\n  - `buffer_size`: Size of the replay buffer (default: 20,000)\n  - `gamma`: Discount factor (default: 0.99)\n  - `tau`: Soft update parameter for target network (default: 1.0)\n  - `target_network_frequency`: Frequency of target network updates (default: 50)\n  - `batch_size`: Batch size for training (default: 256)\n  - `start_e`: Initial exploration rate (default: 1.0)\n  - `end_e`: Final exploration rate (default: 0.05)\n  - `exploration_fraction`: Fraction of total timesteps over which to decay epsilon (default: 0.3)\n  - `learning_starts`: Number of timesteps before starting to learn (default: 1,000)\n  - `train_frequency`: Frequency of training steps (default: 4)\n\n- **Logging & Saving**:\n  - `capture_video`: Whether to capture videos (default: True)\n  - `save_model`: Whether to save model checkpoints (default: True)\n  - `upload_model`: Whether to upload model to Hugging Face Hub (default: True)\n  - `hf_entity`: Hugging Face username (default: \"\")\n\n## Model Architecture\n\nThe Q-network (`QNet`) is a convolutional neural network designed for processing Atari frames with the following architecture:\n- **Conv Layer 1**: 32 filters, 8x8 kernel, stride 4, ReLU activation\n- **Conv Layer 2**: 32 filters, 4x4 kernel, stride 2, ReLU activation  \n- **Conv Layer 3**: 64 filters, 3x3 kernel, stride 3, ReLU activation\n- **Fully Connected 1**: 512 units with ReLU activation\n- **Fully Connected 2**: 512 units with ReLU activation\n- **Output layer**: Action space dimension (4 for Breakout: NOOP, FIRE, RIGHT, LEFT)\n\n## Usage\n\nTo run the training script:\n\n```bash\npython train.py\n```\n\n## Atari Preprocessing\n\nThe Breakout environment uses several preprocessing steps to make the raw pixel input suitable for the DQN:\n- **Frame skipping**: Every action is repeated for 4 frames to speed up training\n- **Grayscale conversion**: RGB frames are converted to grayscale\n- **Frame resizing**: Images are resized to 84x84 pixels\n- **Frame stacking**: 4 consecutive frames are stacked to provide temporal information\n- **Pixel normalization**: Pixel values are scaled to [0, 1] range\n\n## Evaluation\n\nThe agent is evaluated periodically during training. Evaluation metrics include:\n- Average return over multiple episodes\n- Videos of the agent's performance\n\n\n### Training Progress\n\n![Atari Breakout DQN Training](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/DQN-atari/images/image.png)\n\n### Agent Performance\n\nHere's a video showing the trained agent in action:\n\n<details>\n  <summary>Click to see video (GIF format)</summary>\n  \n![Breakout Agent Performance](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/DQN-atari/images/output.gif)\n  \n</details>\n\n\n\n\n## Logging\n\nTraining metrics are logged to TensorBoard, including:\n- Episodic returns\n- Episodic lengths\n- TD loss\n- Q-values\n- Exploration rate (epsilon)\n\n## Results\n\nAfter successful training, the agent should be able to achieve high scores in Breakout by learning to effectively control the paddle to break bricks and keep the ball in play. The game terminates when all bricks are destroyed or all lives are lost.\n\n## References\n\n- [Deep Q-Network (DQN) Paper](https://www.nature.com/articles/nature14236)\n- [CleanRL](https://github.com/vwxyzjn/cleanrl) - This implementation is inspired by the CleanRL project\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Reinforcement-Learning/tree/master/DQN-atari",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Reinforcement-Learning/contents/DQN-atari",
      "download_url": null,
      "created_date": "2025-08-21",
      "github_date": "2025-08-21",
      "category": "Exploration",
      "framework": "PyTorch",
      "environment": "Atari",
      "categories": [
        "Exploration"
      ]
    },
    {
      "name": "DQN",
      "path": "DQN",
      "display_name": "DQN (DQN)",
      "description": "Implementation of DQN reinforcement learning algorithm",
      "readme_content": "# Deep Q-Learning for CartPole\n\nThis repository contains an implementation of Deep Q-Network (DQN) for solving the CartPole-v1 environment from OpenAI Gym (Gymnasium). The implementation includes features such as experience replay, target networks, and epsilon-greedy exploration.\n\n![CartPole DQN Training Visualization](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/DQN/images/image.png)\n\n## Overview\n\nThe main training script (`train.py`) implements a DQN agent to solve the CartPole-v1 environment, where the goal is to balance a pole on a moving cart. The agent learns to take actions (move left or right) to keep the pole upright for as long as possible.\n\n## Features\n\n- **Deep Q-Network (DQN)**: Uses a neural network to approximate the Q-function\n- **Experience Replay**: Stores transitions in a replay buffer to break correlations between consecutive samples\n- **Target Network**: Uses a separate target network to stabilize learning\n- **Epsilon-Greedy Exploration**: Balances exploration and exploitation with a decaying epsilon\n- **Evaluation**: Periodically evaluates the model and saves videos of the agent's performance\n- **Logging**: Includes logging to TensorBoard and Weights & Biases (wandb) for experiment tracking\n- **Video Recording**: Records videos during training and evaluation for visualization\n\n## Requirements\n\n```\ngymnasium\ntorch\nnumpy\ntqdm\nstable-baselines3\nwandb\nimageio\nopencv-python\ntensorboard\nhuggingface_hub\n```\n\n## Configuration\n\nThe project is configured through the `Config` class, which includes the following parameters:\n\n- **Environment Settings**:\n  - `env_id`: The Gym environment ID (default: \"CartPole-v1\")\n  - `seed`: Random seed for reproducibility (default: 42)\n\n- **Training Parameters**:\n  - `total_timesteps`: Total number of timesteps to train (default: 20,000)\n  - `learning_rate`: Learning rate for the optimizer (default: 2.5e-4)\n  - `buffer_size`: Size of the replay buffer (default: 10,000)\n  - `gamma`: Discount factor (default: 0.99)\n  - `tau`: Soft update parameter for target network (default: 1.0)\n  - `target_network_frequency`: Frequency of target network updates (default: 50)\n  - `batch_size`: Batch size for training (default: 128)\n  - `start_e`: Initial exploration rate (default: 1.0)\n  - `end_e`: Final exploration rate (default: 0.05)\n  - `exploration_fraction`: Fraction of total timesteps over which to decay epsilon (default: 0.5)\n  - `learning_starts`: Number of timesteps before starting to learn (default: 1,000)\n  - `train_frequency`: Frequency of training steps (default: 10)\n\n- **Logging & Saving**:\n  - `capture_video`: Whether to capture videos (default: True)\n  - `save_model`: Whether to save model checkpoints (default: True)\n  - `upload_model`: Whether to upload model to Hugging Face Hub (default: True)\n  - `hf_entity`: Hugging Face username (default: \"\")\n  - `use_wandb`: Whether to use Weights & Biases for logging (default: True)\n  - `wandb_project`: WandB project name (default: \"cleanRL\")\n  - `wandb_entity`: WandB username/team (default: \"\")\n\n## Usage\n\nTo run the training script:\n\n```bash\npython train.py\n```\n\n## Model Architecture\n\nThe Q-network (`QNet`) is a simple fully-connected neural network with the following architecture:\n- Input layer: State space dimension\n- Hidden layer 1: 256 units with ReLU activation\n- Hidden layer 2: 512 units with ReLU activation\n- Output layer: Action space dimension (2 for CartPole-v1)\n\n## Evaluation\n\nThe agent is evaluated periodically during training (every 1,000 timesteps by default). Evaluation metrics include:\n- Average return over multiple episodes\n- Videos of the agent's performance\n\n\n### Training Progress\n\n![CartPole DQN Training Visualization](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/DQN/images/image.png)\n\n### Agent Performance\n\nHere's a video showing the trained agent in action:\n\n<details>\n  <summary>Click to see video (GIF format)</summary>\n  \n  <!-- <!-- ![CartPole Agent Performance](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/DQN/images/final.mp4) -->\n  \n![CartPole Demo](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/DQN/images/cartpole_demo.gif) -->\n  \n</details>\n\n\n\n\n## Logging\n\nTraining metrics are logged to both TensorBoard and Weights & Biases (if enabled), including:\n- Episodic returns\n- Episodic lengths\n- TD loss\n- Q-values\n<!-- - Steps per second (SPS) -->\n- Exploration rate (epsilon)\n\n## Results\n\nAfter successful training, the agent should be able to balance the pole for the maximum episode length (500 timesteps in CartPole-v1).\n\n## References\n\n- [Deep Q-Network (DQN) Paper](https://www.nature.com/articles/nature14236)\n- [CleanRL](https://github.com/vwxyzjn/cleanrl) - This implementation is inspired by the CleanRL project\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Reinforcement-Learning/tree/master/DQN",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Reinforcement-Learning/contents/DQN",
      "download_url": null,
      "created_date": "2025-08-21",
      "github_date": "2025-08-21",
      "category": "Exploration",
      "framework": "PyTorch",
      "environment": "Gymnasium",
      "categories": [
        "Exploration"
      ]
    },
    {
      "name": "Duel-DQN",
      "path": "Duel-DQN",
      "display_name": "Duel DQN (Duel DQN)",
      "description": "Implementation of Duel-DQN reinforcement learning algorithm",
      "readme_content": "# Dueling Deep Q-Network (Dueling DQN)\n\nThis project implements the Dueling Deep Q-Network (Dueling DQN) algorithm for the CliffWalking environment from OpenAI Gymnasium. The agent learns to navigate through a dangerous cliff area to reach a goal without falling off.\n\n![Cliff Climbing Environment](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/Duel-DQN/images/output.gif)\n\n## Algorithm Description\n\n**Dueling DQN** is an advanced variant of the Deep Q-Network (DQN) algorithm that uses a special neural network architecture to separately estimate:\n\n1. **Value function (V)**: The value of being in a particular state\n2. **Advantage function (A)**: The advantage of taking specific actions in that state\n\nThe Q-values are then computed as: Q(s,a) = V(s) + A(s,a) - mean(A(s))\n\nThis architecture helps the agent learn which states are valuable without having to learn the effect of each action for each state, leading to more efficient learning, especially in environments with many similar-valued actions.\n\n## Environment Description\n\n**CliffWalking-v0** is a grid-world environment where:\n- The agent must navigate from a starting position to a goal position\n- There's a cliff along the bottom of the grid that the agent must avoid\n- Falling off the cliff gives a large negative reward and resets the agent to the start\n- Each step incurs a small negative reward to encourage the agent to find the shortest path\n\n- **State space:** Discrete with 48 states (represented as one-hot encoded vectors)\n- **Action space:** 4 discrete actions (LEFT, DOWN, RIGHT, UP)\n- **Rewards:** -1 for each step, -100 for falling off the cliff, 0 for reaching the goal\n\n## Implementation Details\n\n```python\nclass QNet(nn.Module):\n    def __init__(self, state_space, action_space):\n        super(QNet, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Linear(state_space, 64),\n            nn.ReLU(),\n            nn.Linear(64, 64),\n            nn.ReLU()\n        )\n        \n        self.values = nn.Sequential(\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1) \n        )\n        self.adv = nn.Sequential(\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, action_space)\n        )\n        \n    def forward(self, x):\n        feat = self.features(x)\n        values = self.values(feat)\n        adv = self.adv(feat)\n        # Q = V + A - mean(A)\n        res = values + adv - adv.mean(dim=1, keepdim=True)\n        return res, values, adv, feat\n```\n\n## Key Features\n\n- **Dueling Network Architecture**: Separate estimators for state values and action advantages\n- **Experience Replay**: Buffer stores transitions for off-policy learning\n- **Target Network**: Separate network for stable Q-value targets\n- **Epsilon-Greedy Exploration**: Linear decay of exploration rate\n- **Gradient Clipping**: Prevents unstable updates with large gradients\n- **Comprehensive Logging**: Track metrics like Q-values, advantage values, and training progress\n- **Model Evaluation**: Periodic evaluation with video recording\n\n## Configuration Options\n\n```python\nclass Config:\n    # Experiment settings\n    exp_name = \"DQN-CliffWalking\"\n    seed = 42\n    env_id = \"CliffWalking-v0\"\n    \n    # Training parameters\n    total_timesteps = 300000\n    learning_rate = 2e-4\n    buffer_size = 30000\n    gamma = 0.99\n    tau = 1.0  # Target network update rate\n    target_network_frequency = 50\n    batch_size = 128\n    start_e = 1.0  # Initial exploration rate\n    end_e = 0.05   # Final exploration rate\n    exploration_fraction = 0.4\n    learning_starts = 1000\n    train_frequency = 4\n    max_grad_norm = 4.0  # Maximum gradient norm for gradient clipping\n```\n\n## Results\n\nThe Dueling DQN algorithm demonstrates several advantages:\n\n- **Faster Learning**: Converges to better policies more quickly than standard DQN\n- **More Stable Performance**: Reduced variance in learning due to the value/advantage decomposition\n- **Better Policy Quality**: Finds more optimal paths by focusing on important state features\n\nThe agent successfully learns to navigate the cliff environment by taking the longer but safer path along the top of the grid, avoiding the risky cliff edge.\n\n## Visualization\n\nTrained agent performance is recorded as videos in the `videos/` directory, showing the learned navigation policy avoiding the cliff while reaching the goal.\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Reinforcement-Learning/tree/master/Duel-DQN",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Reinforcement-Learning/contents/Duel-DQN",
      "download_url": null,
      "created_date": "2025-08-21",
      "github_date": "2025-08-21",
      "category": "Exploration",
      "framework": "PyTorch",
      "environment": "Gymnasium",
      "categories": [
        "Exploration"
      ]
    },
    {
      "name": "FlappyBird-PPO",
      "path": "FlappyBird-PPO",
      "display_name": "Flappybird PPO (Flappybird PPO)",
      "description": "Implementation of FlappyBird-PPO reinforcement learning algorithm",
      "readme_content": "# Flappy Bird with Proximal Policy Optimization (PPO)\n\nThis directory contains an implementation of the Proximal Policy Optimization (PPO) algorithm applied to the Flappy Bird environment.\n\n## Overview\n\nThis project demonstrates how to train an agent to play Flappy Bird using the PPO algorithm, a state-of-the-art policy gradient method in reinforcement learning. The implementation leverages the `flappy_bird_gymnasium` environment, which provides a Gym-compatible interface for the classic Flappy Bird game.\n\n## Environment\n\n**Flappy Bird** is a side-scrolling game where the player controls a bird, attempting to fly between columns of green pipes without hitting them. The game mechanics are simple:\n- The bird automatically moves forward\n- The player can make the bird \"flap\" to move upward\n- Gravity pulls the bird downward\n- The goal is to navigate through as many pipes as possible\n\n**State Space**: The observation space consists of game state information, including:\n- Bird's position and velocity\n- Positions of the upcoming pipes\n- Distances between the bird and pipe openings\n\n**Action Space**: The action space is discrete with two possible actions:\n- 0: Do nothing (let the bird fall)\n- 1: Flap (make the bird move upward)\n\n## Implementation\n\nThe implementation uses a PPO agent with:\n\n- **Actor-Critic Architecture**: Separate networks for policy (actor) and value function (critic)\n- **Clipped Surrogate Objective**: Prevents excessive policy updates\n- **Entropy Bonus**: Encourages exploration\n- **Generalized Advantage Estimation (GAE)**: For variance reduction in policy gradient estimation\n\n## Configuration\n\nThe implementation uses a `Config` class with the following key parameters:\n\n- `exp_name`: \"PPO-Flappy\" - Name of the experiment\n- `env_id`: \"FlappyBird-v0\" - Environment ID\n- `episodes`: 10000 - Number of training episodes\n- `lr`: 3e-4 - Learning rate\n- `gamma`: 0.99 - Discount factor\n- `clip_value`: 0.2 - PPO clipping parameter\n- `PPO_EPOCHS`: 4 - Number of optimization epochs per batch\n- `ENTROPY_COEFF`: 0.01 - Coefficient for entropy bonus\n- `max_steps`: 512 - Maximum steps per episode\n\n## Training Process\n\nThe agent is trained through an iterative process:\n\n1. **Interaction with Environment**: The agent collects trajectories by playing the game\n2. **Advantage Calculation**: Compute advantages using Generalized Advantage Estimation\n3. **Policy Update**: Update policy and value function using the PPO objective\n4. **Repeat**: Continue training until the agent achieves satisfactory performance\n\n## Results\n\nThe agent successfully learns to play Flappy Bird, navigating through pipes with increasing proficiency as training progresses. A video of the trained agent's performance is included (`final_FlappyBird-v0.mp4`).\n\n![Flappy Bird Agent](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/FlappyBird-PPO/images/output.gif)\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Reinforcement-Learning/tree/master/FlappyBird-PPO",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Reinforcement-Learning/contents/FlappyBird-PPO",
      "download_url": null,
      "created_date": "2025-08-21",
      "github_date": "2025-08-21",
      "category": "Actor-Critic",
      "framework": "PyTorch",
      "environment": "Flappybird",
      "categories": [
        "Actor-Critic",
        "Exploration"
      ]
    },
    {
      "name": "Frozen-Lake",
      "path": "Frozen-Lake",
      "display_name": "Frozen Lake (Frozen Lake)",
      "description": "Implementation of Frozen-Lake reinforcement learning algorithm",
      "readme_content": "# Frozen Lake Reinforcement Learning\n\nThis project implements reinforcement learning algorithms for the Frozen Lake environment from OpenAI Gymnasium. The agent learns to navigate across a frozen lake from the start to a goal without falling into holes.\n\n![Frozen Lake Environment](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/Frozen-Lake/images/frozen_lake.gif)\n\n## Environment Description\n\n**FrozenLake-v1** is a grid-world environment where:\n- The agent navigates on a frozen lake from start (S) to goal (G)\n- Some tiles are frozen (F) and safe to walk on\n- Some tiles have holes (H) and the agent falls if it steps on them\n- The ice is slippery, so the agent's movement can be stochastic\n\nExample 4x4 map:\n```\nSFFF\nFHFH\nFFFH\nHFFG\n```\n\n- **State space:** Discrete with 16 states (for 4x4 grid) or 64 states (for 8x8 grid)\n- **Action space:** 4 discrete actions (LEFT, DOWN, RIGHT, UP)\n- **Rewards:** +1 for reaching the goal, 0 otherwise\n\n## Algorithms Implemented\n\nThis project includes implementations of:\n\n1. **Q-Learning**: A model-free, off-policy algorithm using a tabular approach\n2. **Deep Q-Network (DQN)**: Neural network implementation for Q-learning\n3. **Double DQN**: Reducing overestimation bias with two networks\n\n## Features\n\n- Multiple map sizes (4x4 and 8x8)\n- Option for deterministic or stochastic (slippery) environments\n- Exploration vs. exploitation control with epsilon-greedy strategy\n- Visualization of learned policies\n- Tracking of training metrics\n- Integration with TensorBoard and WandB\n\n## Getting Started\n\n### Installation\n\n```bash\npip install torch gymnasium numpy matplotlib tqdm tensorboard wandb\n```\n\n### Running the Algorithms\n\n```bash\n# For tabular Q-Learning\npython q_learning.py\n\n# For DQN\npython dqn.py\n\n# For visualization of learned policy\npython visualize_policy.py\n```\n\n## Configuration\n\nKey hyperparameters can be modified in the `Config` class:\n\n```python\nclass Config:\n    # Environment settings\n    env_id = \"FrozenLake-v1\"\n    map_size = \"4x4\"  # or \"8x8\"\n    is_slippery = True\n    \n    # Algorithm parameters\n    learning_rate = 0.1  # for Q-Learning\n    gamma = 0.99  # Discount factor\n    epsilon_start = 1.0\n    epsilon_end = 0.01\n    epsilon_decay = 0.995\n    \n    # Training parameters\n    total_episodes = 10000\n    max_steps = 100\n    \n    # For DQN\n    buffer_size = 10000\n    batch_size = 64\n    target_update = 100\n    \n    # Logging\n    use_wandb = True\n    log_interval = 100\n```\n\n## Results\n\nThe algorithms learn efficient policies for navigating the Frozen Lake:\n\n- **Q-Learning**: Converges to optimal policy after ~5000 episodes for 4x4 map\n- **DQN**: Learns good policies but might be less sample-efficient for this simple environment\n- **Double DQN**: Provides more stable learning, especially for the 8x8 map\n\n## Visualization\n\nThe project includes tools to visualize:\n- Learning curves\n- Value functions\n- Optimal policies\n- Step-by-step agent behavior\n\n## Challenges\n\n- **Sparse Rewards**: Only getting reward at the goal makes learning difficult\n- **Stochasticity**: The slippery environment introduces randomness in transitions\n- **Exploration**: Finding the goal in larger environments requires efficient exploration\n\n## References\n\n- [Gymnasium FrozenLake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n- [Q-Learning Paper](https://link.springer.com/article/10.1007/BF00992698)\n- [DQN Paper](https://www.nature.com/articles/nature14236)\n- [Double DQN Paper](https://arxiv.org/abs/1509.06461)\n\n## License\n\nMIT License\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Reinforcement-Learning/tree/master/Frozen-Lake",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Reinforcement-Learning/contents/Frozen-Lake",
      "download_url": null,
      "created_date": "2025-08-21",
      "github_date": "2025-08-21",
      "category": "Exploration",
      "framework": "PyTorch",
      "environment": "Frozenlake",
      "categories": [
        "Exploration"
      ]
    },
    {
      "name": "Imitation Learning",
      "path": "Imitation Learning",
      "display_name": "Imitation Learning (Imitation Learning)",
      "description": "Implementation of Imitation Learning reinforcement learning algorithm",
      "readme_content": "\n# Imitation Learning for GridWorld\n\nThis project implements core Imitation Learning algorithms—including Behavioral Cloning (BC) and Dataset Aggregation (DAgger)—for learning policies from expert demonstrations in a GridWorld environment. The agent learns to imitate expert behavior by training a neural network to predict actions given states, and can further improve using interactive data collection (DAgger).\n\n\n## 🎯 Overview\n\nImitation Learning is a family of techniques where agents learn to perform tasks by mimicking expert behavior. This project includes:\n\n- **Behavioral Cloning (BC):** A supervised learning approach where a policy is trained on expert state-action pairs to directly imitate the expert.\n- **DAgger (Dataset Aggregation):** An interactive algorithm that iteratively collects new data by letting the agent act and querying the expert for corrections, reducing compounding errors.\n\nThe learned policies can then be evaluated in the environment.\n\n## 📁 Project Structure\n\n```\n├── BC.py                    # Behavioral Cloning implementation\n├── DAgger.py                # DAgger (Dataset Aggregation) implementation\n├── gridworld.py             # GridWorld environment\n├── gridworld.json           # Environment configuration\n├── images/\n│   └── image.png            # GridWorld visualization\n├── imitation-learning-tutorials/\n│   ├── expert_data/\n│   │   └── ckpt0.pkl        # Expert demonstration data\n│   └── ...                  # Additional tutorial notebooks\n└── README.md                # This file\n```\n\n## 🖼️ GridWorld Visualization\n\n<!-- ![GridWorld Environment](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/Imitation Learning/images/image.png) -->\n\n*The GridWorld environment where the agent learns to navigate and imitate expert behavior.*\n\n## 🚀 Quick Start\n\n### Prerequisites\n\n```bash\npip install torch tqdm wandb\n```\n\n\n### Running the Code\n\n#### Behavioral Cloning\n```bash\npython BC.py\n```\nThis will:\n1. Load expert demonstrations from `expert_data/ckpt0.pkl`\n2. Train a policy network using behavioral cloning\n3. Evaluate the policy every 100 episodes\n4. Log training progress to Weights & Biases\n\n#### DAgger\n```bash\npython DAgger.py\n```\nThis will:\n1. Initialize with expert demonstrations\n2. Iteratively collect new data by running the agent and querying the expert\n3. Aggregate datasets and retrain the policy\n4. Evaluate and log progress\n\n## 🧠 Model Architecture\n\n### PolicyNet\n- **Input**: One-hot encoded state (2500 dimensions)\n- **Hidden Layers**: \n  - FC1: 2500 → 128 (ReLU)\n  - FC2: 128 → 64 (ReLU)\n- **Output**: 4 action logits (up, down, left, right)\n\n### Training Details\n- **Loss Function**: Cross-entropy loss\n- **Optimizer**: Adam (lr=2.5e-4)\n- **Batch Processing**: Trains on individual expert episodes\n\n## 📊 Monitoring with Weights & Biases\n\nThe code automatically logs:\n- **train_loss**: Cross-entropy loss for each training episode\n- **eval_reward**: Average reward during evaluation\n- **episode**: Training episode number\n- **hyperparameters**: Learning rate, architecture details\n\n## 🔧 Configuration\n\nModify the `Config` class in `BC.py`:\n\n```python\n@dataclass\nclass Config:\n    lr: float = 2.5e-4                    # Learning rate\n    project_name: str = \"behavioral-cloning\"  # WandB project name\n    run_name: str = \"bc-gridworld\"           # WandB run name\n```\n\n\n## 📈 Key Components\n\n### BC Class\n- `__init__()`: Initializes policy network, optimizer, and WandB logging\n- `train()`: Trains on expert state-action pairs for one episode\n- `evaluate()`: Evaluates policy performance in the environment\n\n### DAgger Class\n- `__init__()`: Initializes policy, expert, and data buffers\n- `collect_data()`: Runs the current policy, queries expert for corrections, and aggregates new data\n- `train()`: Retrains the policy on the aggregated dataset\n- `evaluate()`: Evaluates policy performance in the environment\n\n### Plots\n\n- BC and DAgger training losses and evaluation rewards are logged to Weights & Biases for visualization.\n\n![BC Loss](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/Imitation Learning/images/BC.png)\n![DAgger Loss](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/Imitation Learning/images/DAgger.png)\n\n### Helper Functions\n- `sample_action()`: Samples actions from policy logits (greedy/stochastic)\n- `one_hot_encode()`: Converts state integers to one-hot vectors\n\n### BC Training Loop Example\n```python\ncurr = 0\nfor i, length in enumerate(timestep_lens):\n    # Extract episode data\n    expert_states = all_states[curr: curr + length]\n    expert_actions = all_actions[curr: curr + length]\n    # Train on this episode\n    loss = model.train(expert_states, expert_actions)\n    # Evaluate every 100 episodes\n    if i % 100 == 0:\n        rew = model.evaluate()\n        print(f\"Episode {i}, Eval Reward: {rew}\")\n    curr += length\n```\n\n### DAgger Training Loop Example\n```python\nfor iteration in range(num_iterations):\n    # Collect data using current policy and expert\n    new_states, new_actions = model.collect_data()\n    # Aggregate with previous data\n    dataset.add(new_states, new_actions)\n    # Retrain policy\n    model.train(dataset.states, dataset.actions)\n    # Evaluate\n    if iteration % 5 == 0:\n        rew = model.evaluate()\n        print(f\"DAgger Iteration {iteration}, Eval Reward: {rew}\")\n```\n\n## 🎮 Environment Details\n\n- **GridWorld**: 50x50 grid environment\n- **States**: 2500 possible positions (50×50)\n- **Actions**: 4 discrete actions (up, down, left, right)\n- **Evaluation**: Uses batched environments (128 parallel instances)\n\n## 📊 Expert Data Format\n\nThe expert data (`ckpt0.pkl`) contains:\n- `states`: Flattened array of all expert states\n- `actions`: Flattened array of all expert actions  \n- `timestep_lens`: Length of each expert episode\n\n## 🔍 Evaluation Metrics\n\n- **Average Reward**: Mean reward per episode across evaluation runs\n- **Training Loss**: Cross-entropy loss between predicted and expert actions\n\n\n\n**Note**: Make sure to have expert demonstrations and proper environment setup before running the code.\n\n**Special Thanks**: This implementation is inspired by the Imitation Learning tutorials available at [Imitation Learning Tutorials](https://github.com/tsmatz/imitation-learning-tutorials/blob/master)",
      "github_url": "https://github.com/YuvrajSingh-mist/Reinforcement-Learning/tree/master/Imitation Learning",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Reinforcement-Learning/contents/Imitation Learning",
      "download_url": null,
      "created_date": "2025-08-21",
      "github_date": "2025-08-21",
      "category": "Imitation Learning",
      "framework": "PyTorch",
      "environment": "Custom Environment",
      "categories": [
        "Imitation Learning"
      ]
    },
    {
      "name": "MARL",
      "path": "MARL",
      "display_name": "MARL (MARL)",
      "description": "Implementation of MARL reinforcement learning algorithm",
      "readme_content": "# Multi-Agent Reinforcement Learning (MARL) Research Project\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/MARL/IPPO/images/pong.gif\" width=\"300\" alt=\"IPPO Pong Demo\"/>\n  <img src=\"https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/MARL/MAPPO/images/simple_spread.mp4\" width=\"300\" alt=\"MAPPO Simple Spread Demo\"/>\n  <br>\n  <em>IPPO agents competing in Pong (left) and MAPPO agents cooperating in Simple Spread (right)</em>\n</p>\n\n## 🚀 Project Overview\n\nThis comprehensive **Multi-Agent Reinforcement Learning (MARL)** research project implements and evaluates state-of-the-art algorithms for multi-agent systems. The project features **IPPO** (Independent Proximal Policy Optimization), **MAPPO** (Multi-Agent Proximal Policy Optimization), and **Self-Play** implementations, supporting both cooperative and competitive multi-agent scenarios.\n\n### 🎯 Key Features\n\n- **Multiple Algorithms**: IPPO, MAPPO, and Self-Play implementations\n- **Diverse Environments**: Atari, PettingZoo MPE, and Butterfly environments\n- **Action Spaces**: Support for both discrete and continuous actions\n- **Exploration**: RND (Random Network Distillation) integration\n- **Interactive Play**: Human vs AI and AI vs AI gameplay\n- **Pre-trained Models**: Ready-to-use trained agents\n- **Comprehensive Documentation**: Detailed READMEs for each algorithm\n\n---\n\n## 📚 Table of Contents\n\n1. [Algorithm Overview](#algorithm-overview)\n2. [Project Structure](#project-structure)\n3. [Supported Environments](#supported-environments)\n4. [Quick Start Guide](#quick-start-guide)\n5. [Algorithm-Specific Guides](#algorithm-specific-guides)\n6. [Training Examples](#training-examples)\n7. [Results and Performance](#results-and-performance)\n8. [Technical Details](#technical-details)\n9. [Contributing](#contributing)\n10. [References](#references)\n\n---\n\n## 🧠 Algorithm Overview\n\n### IPPO (Independent Proximal Policy Optimization)\n**Location**: [`IPPO/`](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/MARL/IPPO/README.md)\n\nIPPO extends single-agent PPO to multi-agent settings through independent learning with shared observation processing. Each agent maintains its own policy while benefiting from shared feature extraction.\n\n**Key Features:**\n- Independent learning for each agent\n- Shared observation encoder\n- Support for discrete and continuous actions\n- Self-play capabilities for competitive environments\n\n**Best For:** Cooperative tasks requiring independent decision-making, competitive scenarios, scalable multi-agent systems.\n\n### MAPPO (Multi-Agent Proximal Policy Optimization)\n**Location**: [`MAPPO/`](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/MARL/MAPPO/README.md)\n\nMAPPO implements centralized training with decentralized execution (CTDE), using a centralized critic during training while maintaining decentralized policies for execution.\n\n**Key Features:**\n- Centralized training with decentralized execution\n- Global state information during training\n- RND variants for enhanced exploration\n- Superior coordination in cooperative tasks\n\n**Best For:** Cooperative multi-agent tasks, scenarios requiring coordination, complex environments with global state information.\n\n### Self-Play\n**Location**: [`Self Play/`](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/MARL/Self%20Play/README.md)\n\nSelf-play training where agents learn by competing against themselves or other agents from the same population, creating a natural curriculum for continuous improvement.\n\n**Key Features:**\n- Population-based learning\n- Automatic curriculum generation\n- Strategy evolution through competition\n- Interactive human vs AI gameplay\n\n**Best For:** Competitive environments, strategy games, scenarios requiring emergent behavior discovery.\n\n---\n\n## 📁 Project Structure\n\n```\nMARL/\n├── README.md                 # Main project documentation (this file)\n├── train.py                  # Main training script for Pong self-play\n├── play_ippo.py             # Play script for trained models\n│\n├── IPPO/                    # Independent PPO implementations\n│   ├── README.md           # Detailed IPPO documentation\n│   ├── ippo_discrete.py    # Discrete action spaces (Simple Spread)\n│   ├── ippo_continuous.py  # Continuous action spaces\n│   ├── ippo_simple_tag.py  # Simple Tag environment\n│   ├── play_ippo.py        # Interactive play script (Pong)\n│   ├── images/             # Training visualizations\n│   │   ├── pong.gif       # Demo video\n│   │   └── image.png      # Training plots\n│   └── *.mp4              # Demo videos\n│\n├── MAPPO/                   # Multi-Agent PPO implementations\n│   ├── README.md          # Detailed MAPPO documentation\n│   ├── mappo_without_rnd.py    # Standard MAPPO\n│   ├── mappo_rnd.py           # MAPPO with RND for exploration\n│   ├── mappo_rnd_pong.py      # MAPPO with RND for cooperative Pong\n│   ├── train.py               # MAPPO training script (cooperative Pong)\n│   ├── images/                # Training visualizations\n│   │   └── simple_spread.mp4  # Demo video\n│   └── __pycache__/\n│\n└── Self Play/               # Self-play utilities\n    ├── README.md           # Detailed Self-Play documentation\n    ├── play.py             # Watch two trained agents compete (Pong)\n    ├── self_play.py        # Self-play training driver (Pong)\n    └── pt files/           # Saved checkpoints\n        └── Pong-MARL.pt    # Pre-trained Pong model (19MB)\n```\n\n---\n\n## 🌍 Supported Environments\n\n### Atari Environments\n- **Pong-v3**: Classic Atari Pong with self-play capabilities\n  - **Features**: Image-based observations, discrete actions, competitive gameplay\n  - **Use Cases**: Self-play training, competitive scenarios\n\n### PettingZoo MPE Environments\n- **Simple Spread**: Cooperative navigation task\n  - **Features**: Vector observations, discrete/continuous actions, cooperative rewards\n  - **Use Cases**: IPPO and MAPPO training, coordination studies\n- **Simple Tag**: Competitive tagging game\n  - **Features**: Vector observations, competitive rewards\n  - **Use Cases**: Competitive multi-agent scenarios\n\n### PettingZoo Butterfly Environments\n- **Cooperative Pong-v5**: Cooperative version of Pong for MAPPO\n  - **Features**: Multi-agent cooperation, image-based observations\n  - **Use Cases**: Cooperative training, coordination studies\n\n---\n\n## 🚀 Quick Start Guide\n\n### 1. Installation\n\n```bash\n# Install all dependencies\npip install torch pettingzoo[atari,mpe,butterfly] supersuit wandb tqdm imageio opencv-python gymnasium\n```\n\n### 2. Choose Your Algorithm\n\n#### For Cooperative Tasks (IPPO)\n```bash\ncd MARL/IPPO\npython ippo_discrete.py --env_id simple_spread_v3 --total_timesteps 20000000\n```\n\n#### For Cooperative Tasks with Coordination (MAPPO)\n```bash\ncd MARL/MAPPO\npython mappo_without_rnd.py --env_id simple_spread_v3 --total_timesteps 20000000\n```\n\n#### For Competitive Self-Play (Pong)\n```bash\ncd MARL\npython train.py --env_id pong_v3 --total_timesteps 15000000\n```\n\n### 3. Interactive Play\n\n#### Human vs AI (Pong)\n```bash\ncd MARL/Self Play\npython play.py \"pt files/Pong-MARL.pt\"\n```\n\n#### AI vs AI\n```bash\ncd MARL/IPPO\npython play_ippo.py \"checkpoint.pt\"\n```\n\n---\n\n## 📖 Algorithm-Specific Guides\n\n### [IPPO Documentation](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/MARL/IPPO/README.md)\n- **Theory**: Independent learning with shared observation processing\n- **Implementation**: Discrete, continuous, and Simple Tag variants\n- **Usage**: Training commands, hyperparameters, evaluation\n- **Results**: Performance metrics and emergent behaviors\n\n### [MAPPO Documentation](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/MARL/MAPPO/README.md)\n- **Theory**: Centralized training with decentralized execution\n- **Implementation**: Standard MAPPO and RND variants\n- **Usage**: Training commands, hyperparameters, evaluation\n- **Results**: Coordination performance and sample efficiency\n\n### [Self-Play Documentation](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/MARL/Self%20Play/README.md)\n- **Theory**: Population-based learning and strategy evolution\n- **Implementation**: Competitive training and interactive play\n- **Usage**: Training commands, interactive controls, evaluation\n- **Results**: Strategy emergence and competitive performance\n\n---\n\n## 🎯 Training Examples\n\n### IPPO Training Commands\n```bash\n# Discrete actions (Simple Spread)\npython IPPO/ippo_discrete.py --env_id simple_spread_v3 --total_timesteps 20000000\n\n# Continuous actions\npython IPPO/ippo_continuous.py --env_id simple_spread_v3 --total_timesteps 20000000\n\n# Simple Tag environment\npython IPPO/ippo_simple_tag.py --env_id simple_tag_v3 --total_timesteps 20000000\n```\n\n### MAPPO Training Commands\n```bash\n# Standard MAPPO (Simple Spread)\npython MAPPO/mappo_without_rnd.py --env_id simple_spread_v3 --total_timesteps 20000000\n\n# MAPPO with RND for exploration\npython MAPPO/mappo_rnd.py --env_id simple_spread_v3 --total_timesteps 20000000\n\n# MAPPO for cooperative Pong\npython MAPPO/mappo_rnd_pong.py --env_id cooperative_pong_v5 --total_timesteps 10000000\n```\n\n### Self-Play Training Commands\n```bash\n# Main self-play training (Pong)\npython train.py --env_id pong_v3 --total_timesteps 15000000\n\n# Alternative self-play driver\npython \"Self Play/self_play.py\" --env_id pong_v3 --total_timesteps 15000000\n```\n\n---\n\n## 📊 Results and Performance\n\n### Algorithm Comparison\n\n| Aspect | IPPO | MAPPO | Self-Play |\n|--------|------|-------|-----------|\n| **Training Paradigm** | Independent | Centralized | Population-based |\n| **Sample Efficiency** | High | Very High | Medium |\n| **Coordination** | Good | Excellent | N/A |\n| **Scalability** | High | Medium | High |\n| **Implementation** | Simple | Complex | Simple |\n| **Best For** | Cooperative/Competitive | Cooperative | Competitive |\n\n### Environment-Specific Performance\n\n#### Simple Spread (Cooperative)\n- **IPPO**: Achieves 85-90% landmark coverage\n- **MAPPO**: Achieves 95-98% landmark coverage\n- **Convergence**: 10-20M timesteps\n\n#### Pong (Competitive)\n- **Self-Play**: >90% win rate against random opponents\n- **Strategy Emergence**: Sophisticated defensive and offensive strategies\n- **Convergence**: 10-15M timesteps\n\n#### Simple Tag (Competitive)\n- **IPPO**: Effective competitive strategies\n- **Balance**: Maintains competitive balance between teams\n- **Adaptation**: Agents adapt to opponent strategies\n\n---\n\n## 🔧 Technical Details\n\n### Hyperparameters\n\n#### IPPO Configuration\n```python\nlr = 2.5e-4                    # Learning rate\nnum_envs = 15                  # Parallel environments\nmax_steps = 128               # Rollout length\nPPO_EPOCHS = 4                # PPO update epochs\nclip_coeff = 0.2              # PPO clipping coefficient\nENTROPY_COEFF = 0.001         # Entropy regularization\nGAE = 0.95                    # GAE lambda parameter\n```\n\n#### MAPPO Configuration\n```python\nlr = 2.5e-4                    # Learning rate\nnum_envs = 15                  # Parallel environments\nmax_steps = 256               # Rollout length (longer than IPPO)\nPPO_EPOCHS = 10               # PPO update epochs (more than IPPO)\nclip_coeff = 0.2              # PPO clipping coefficient\nENTROPY_COEFF = 0.02          # Entropy regularization (higher than IPPO)\nGAE = 0.95                    # GAE lambda parameter\n```\n\n#### Self-Play Configuration\n```python\nlr = 2.5e-4                    # Learning rate\nnum_envs = 16                  # Parallel environments\nmax_steps = 128               # Rollout length\nPPO_EPOCHS = 4                # PPO update epochs\nclip_coeff = 0.1              # PPO clipping coefficient\nENTROPY_COEFF = 0.01          # Entropy regularization\ntotal_timesteps = 15000000    # Total training steps\n```\n\n### Network Architectures\n\n#### Observation Processing\n- **Atari**: Grayscale, resize to 84×84, 4-frame stack, agent indicator channel, downsampled to 64×64\n- **MPE**: Direct vector observations with agent-specific processing\n- **Butterfly**: Image-based observations with multi-agent coordination\n\n#### Shared Components\n- **Shared Encoder**: Convolutional tower for images, MLP for vectors\n- **Agent-Specific Heads**: Separate actor and critic networks per agent\n- **Optimization**: Adam with gradient clipping (0.5) + orthogonal initialization\n\n### Pre-trained Models\n\n#### Pong-MARL.pt\n- **Location**: `Self Play/pt files/Pong-MARL.pt`\n- **Training**: 15M timesteps of self-play training\n- **Performance**: >90% win rate against random opponents\n- **Size**: ~19MB\n- **Usage**: Ready for immediate evaluation and interactive play\n\n---\n\n## 🎮 Interactive Features\n\n### Human vs AI Gameplay\n- **Controls**: Keyboard-based interaction\n- **Visualization**: Real-time rendering with OpenCV\n- **Feedback**: Immediate visual and score feedback\n\n### AI vs AI Competition\n- **Visualization**: Real-time agent competition\n- **Analysis**: Strategy observation and analysis\n- **Recording**: Video capture for analysis\n\n### Evaluation Tools\n- **Metrics**: Win rates, cooperation scores, efficiency measures\n- **Visualization**: Training curves, performance plots\n- **Comparison**: Cross-algorithm performance analysis\n\n---\n\n## 🔬 Research Contributions\n\n### Novel Implementations\n1. **IPPO Variants**: Discrete, continuous, and competitive implementations\n2. **MAPPO with RND**: Enhanced exploration for cooperative tasks\n3. **Self-Play Framework**: Comprehensive competitive training system\n\n### Technical Innovations\n1. **Shared Observation Processing**: Efficient feature extraction\n2. **RND Integration**: Intrinsic motivation for exploration\n3. **Interactive Play**: Human-AI interaction capabilities\n\n### Performance Improvements\n1. **Sample Efficiency**: Optimized training procedures\n2. **Stability**: Robust training across environments\n3. **Scalability**: Efficient multi-agent implementations\n\n---\n\n## 🚀 Future Work\n\n### Algorithm Extensions\n1. **Attention Mechanisms**: Improving observation processing\n2. **Hierarchical Policies**: Multi-level decision making\n3. **Communication Protocols**: Explicit agent communication\n4. **Meta-Learning**: Fast adaptation to new environments\n\n### Environment Support\n1. **New PettingZoo Environments**: Additional multi-agent scenarios\n2. **Custom Environments**: Domain-specific applications\n3. **Real-world Applications**: Robotics, autonomous systems\n\n### Research Directions\n1. **Multi-Objective Optimization**: Balancing multiple objectives\n2. **Transfer Learning**: Cross-environment knowledge transfer\n3. **Adversarial Training**: Improving robustness\n4. **Scalable Architectures**: Handling larger numbers of agents\n\n---\n\n## 📚 References\n\n### Key Papers\n- [The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games](https://arxiv.org/abs/2103.01955)\n- [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)\n- [Exploration by Random Network Distillation](https://arxiv.org/abs/1810.12894)\n- [Mastering the Game of Go with Deep Neural Networks and Tree Search](https://www.nature.com/articles/nature16961)\n\n### Libraries and Tools\n- [PettingZoo](https://pettingzoo.farama.org/) - Multi-agent environment library\n- [SuperSuit](https://github.com/Farama-Foundation/SuperSuit) - Environment preprocessing\n- [PyTorch](https://pytorch.org/) - Deep learning framework\n- [CleanRL](https://github.com/vwxyzjn/cleanrl) - Reference implementations\n\n### WandB Reports\n- [![WandB Report](https://img.shields.io/badge/WandB-Report-blue?logo=wandb)](https://api.wandb.ai/links/rentio/a74ndy24)\n\n---\n\n## 🤝 Contributing\n\nThis project welcomes contributions from the research community! We encourage:\n\n### Types of Contributions\n- **Bug Reports**: Help improve code quality and stability\n- **Feature Requests**: Suggest new algorithms or environments\n- **Performance Improvements**: Optimize training procedures\n- **Documentation**: Enhance tutorials and examples\n- **Research Extensions**: Implement new MARL algorithms\n\n### Getting Started\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Add tests if applicable\n5. Submit a pull request\n\n### Development Guidelines\n- Follow PEP 8 style guidelines\n- Add comprehensive documentation\n- Include performance benchmarks\n- Provide usage examples\n\n---\n\n## 📄 License\n\nThis project is open source and available under the **MIT License**. See the [LICENSE](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/MARL/LICENSE) file for details.\n\n---\n\n## 🙏 Acknowledgments\n\n- **PettingZoo Team**: For providing excellent multi-agent environments\n- **CleanRL Community**: For reference implementations and best practices\n- **PyTorch Team**: For the powerful deep learning framework\n- **Research Community**: For foundational papers and algorithms\n\n---\n\n## 📞 Contact\n\nFor questions, suggestions, or collaborations:\n- **Issues**: Use GitHub issues for bug reports and feature requests\n- **Discussions**: Join our community discussions\n- **Research**: Reach out for research collaborations\n\n---\n\n*This project represents a comprehensive exploration of multi-agent reinforcement learning, combining theoretical insights with practical implementations to advance the field of MARL research.*\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Reinforcement-Learning/tree/master/MARL",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Reinforcement-Learning/contents/MARL",
      "download_url": null,
      "created_date": "2025-08-21",
      "github_date": "2025-08-21",
      "category": "Multi-Agent",
      "framework": "PyTorch",
      "environment": "Atari",
      "categories": [
        "Actor-Critic",
        "Exploration",
        "Multi-Agent"
      ]
    },
    {
      "name": "IPPO",
      "path": "MARL/IPPO",
      "display_name": "Ippo (MARL Ippo)",
      "description": "Implementation of IPPO reinforcement learning algorithm",
      "readme_content": "# Independent Proximal Policy Optimization (IPPO)\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/MARL/IPPO/images/pong.gif\" width=\"400\" alt=\"IPPO Pong Demo\"/>\n  <br>\n  <em>IPPO agents competing in Pong environment</em>\n</p>\n\n## Overview\n\n**Independent Proximal Policy Optimization (IPPO)** is a state-of-the-art multi-agent reinforcement learning algorithm that extends the single-agent PPO algorithm to multi-agent settings. Unlike centralized training approaches, IPPO allows each agent to learn independently while sharing observation processing capabilities.\n\n## Algorithm Theory\n\n### Core Concept\nIPPO operates on the principle that each agent can learn an optimal policy independently while sharing a common observation encoder. This approach is particularly effective in cooperative multi-agent environments where agents need to coordinate but can benefit from independent learning.\n\n### Key Components\n\n#### 1. Independent Learning\n- Each agent maintains its own policy network (actor) and value network (critic)\n- Agents learn independently without direct policy sharing\n- Shared observation processing reduces computational overhead\n\n#### 2. Proximal Policy Optimization\n- Uses PPO's clipped objective function to ensure stable policy updates\n- Trust region optimization prevents large policy changes\n- Entropy regularization encourages exploration\n\n#### 3. Generalized Advantage Estimation (GAE)\n- Computes advantages using GAE with λ=0.95\n- Reduces variance in policy gradient estimates\n- Balances bias-variance trade-off in advantage estimation\n\n## Implementation Details\n\n### Network Architecture\n\n#### Shared Observation Encoder\n```python\nclass SharedEncoder(nn.Module):\n    def __init__(self, observation_dim):\n        self.network = nn.Sequential(\n            layer_init(nn.Linear(observation_dim, 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, 64)),\n            nn.Tanh(),\n        )\n```\n\n#### Agent-Specific Heads\n```python\nclass Actor(nn.Module):\n    def __init__(self, observation_dim, action_dim):\n        # Shared feature extraction\n        self.network = nn.Sequential(...)\n        # Agent-specific actor head\n        self.actor = layer_init(nn.Linear(64, action_dim), std=0.01)\n\nclass Critic(nn.Module):\n    def __init__(self, observation_dim):\n        # Shared feature extraction\n        self.network = nn.Sequential(...)\n        # Agent-specific critic head\n        self.critic = layer_init(nn.Linear(64, 1), std=1.0)\n```\n\n### Training Process\n\n1. **Environment Interaction**\n   - Multiple parallel environments (15 by default)\n   - Each agent interacts independently\n   - Observations are processed through shared encoder\n\n2. **Experience Collection**\n   - Rollout length: 128 steps per environment\n   - Store observations, actions, rewards, values, log probabilities\n   - Compute advantages using GAE\n\n3. **Policy Updates**\n   - PPO epochs: 4\n   - Minibatch size: 1920 (15 envs × 128 steps)\n   - Learning rate: 2.5e-4 with linear annealing\n\n4. **Optimization**\n   - Adam optimizer with gradient clipping (0.5)\n   - Orthogonal initialization for stable training\n   - Entropy coefficient: 0.001 for exploration\n\n## Supported Environments\n\n### 1. Simple Spread (Discrete Actions)\n- **Environment**: `simple_spread_v3`\n- **Task**: Cooperative navigation where agents must cover landmarks\n- **Actions**: Discrete (5 actions per agent)\n- **Observations**: Vector observations with agent positions and landmark locations\n- **Reward**: Cooperative reward based on landmark coverage\n\n### 2. Simple Tag (Competitive)\n- **Environment**: `simple_tag_v3`\n- **Task**: Competitive tagging game\n- **Actions**: Discrete actions for movement and tagging\n- **Observations**: Vector observations with agent positions\n- **Reward**: Competitive rewards for taggers and runners\n\n### 3. Continuous Control\n- **Environment**: `simple_spread_v3` (continuous variant)\n- **Task**: Same cooperative navigation with continuous actions\n- **Actions**: Continuous 2D movement vectors\n- **Observations**: Same vector observations\n- **Reward**: Same cooperative reward structure\n\n## Usage\n\n### Installation\n```bash\npip install torch pettingzoo[mpe] supersuit wandb tqdm imageio opencv-python gymnasium\n```\n\n### Training Commands\n\n#### Discrete Actions (Simple Spread)\n```bash\npython ippo_discrete.py --env_id simple_spread_v3 --total_timesteps 20000000\n```\n\n#### Continuous Actions\n```bash\npython ippo_continuous.py --env_id simple_spread_v3 --total_timesteps 20000000\n```\n\n#### Simple Tag Environment\n```bash\npython ippo_simple_tag.py --env_id simple_tag_v3 --total_timesteps 20000000\n```\n\n### Key Hyperparameters\n\n```python\n# Training Configuration\nlr = 2.5e-4                    # Learning rate\nnum_envs = 15                  # Parallel environments\nmax_steps = 128               # Rollout length\nPPO_EPOCHS = 4                # PPO update epochs\nclip_coeff = 0.2              # PPO clipping coefficient\nENTROPY_COEFF = 0.001         # Entropy regularization\nGAE = 0.95                    # GAE lambda parameter\ntotal_timesteps = 20000000    # Total training steps\n```\n\n### Evaluation\n```bash\n# Evaluate trained model\npython ippo_discrete.py --eval --checkpoint \"checkpoint.pt\"\n\n# Interactive play (Pong)\npython play_ippo.py \"checkpoint.pt\"\n```\n\n## Technical Implementation\n\n### File Structure\n```\nIPPO/\n├── ippo_discrete.py      # Discrete action implementation\n├── ippo_continuous.py    # Continuous action implementation\n├── ippo_simple_tag.py    # Simple Tag environment\n├── play_ippo.py         # Interactive play script\n├── images/              # Training visualizations\n│   ├── pong.gif        # Demo video\n│   └── image.png       # Training plots\n└── README.md           # This file\n```\n\n### Key Classes\n\n#### Config\nCentralized configuration class containing all hyperparameters and training settings.\n\n#### Actor/Critic Networks\nAgent-specific policy and value networks with shared observation processing.\n\n#### IPPO Trainer\nMain training loop implementing the IPPO algorithm with experience collection and policy updates.\n\n## References\n\n### Papers\n- [The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games](https://arxiv.org/abs/2103.01955)\n- [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)\n- [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438)\n\n### Code References\n- [CleanRL IPPO Implementation](https://github.com/vwxyzjn/cleanrl)\n- [PettingZoo Multi-Agent Environments](https://pettingzoo.farama.org/)\n- [SuperSuit Environment Wrappers](https://github.com/Farama-Foundation/SuperSuit)\n\n---\n\n## Contributing\n\nThis implementation is part of a larger MARL research project. Contributions are welcome in the form of:\n- Bug reports and fixes\n- Performance improvements\n- New environment support\n- Algorithm extensions\n\n## License\n\nThis implementation is open source and available under the MIT License.\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Reinforcement-Learning/tree/master/MARL/IPPO",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Reinforcement-Learning/contents/MARL/IPPO",
      "download_url": null,
      "created_date": "2025-08-21",
      "github_date": "2025-08-21",
      "category": "Multi-Agent",
      "framework": "PyTorch",
      "environment": "Atari",
      "categories": [
        "Actor-Critic",
        "Exploration",
        "Multi-Agent"
      ]
    },
    {
      "name": "MAPPO",
      "path": "MARL/MAPPO",
      "display_name": "Mappo (MARL Mappo)",
      "description": "Implementation of MAPPO reinforcement learning algorithm",
      "readme_content": "# Multi-Agent Proximal Policy Optimization (MAPPO)\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/MARL/MAPPO/images/simple_spread.mp4\" width=\"400\" alt=\"MAPPO Simple Spread Demo\"/>\n  <br>\n  <em>MAPPO agents cooperating in Simple Spread environment</em>\n</p>\n\n## Overview\n\n**Multi-Agent Proximal Policy Optimization (MAPPO)** is a centralized training with decentralized execution (CTDE) algorithm that extends PPO to multi-agent settings. MAPPO uses a centralized critic during training while maintaining decentralized policies for execution, making it highly effective for cooperative multi-agent tasks.\n\n## Algorithm Theory\n\n### Core Concept\nMAPPO operates under the **Centralized Training, Decentralized Execution (CTDE)** paradigm, where agents share information during training but act independently during execution. This approach allows agents to leverage global information for better coordination while maintaining the benefits of decentralized execution.\n\n### Key Components\n\n#### 1. Centralized Training\n- All agents share a centralized critic network\n- Global state information is available during training\n- Joint optimization of all agent policies\n\n#### 2. Decentralized Execution\n- Each agent has its own policy network\n- Agents act based on local observations only\n- No communication required during execution\n\n#### 3. Proximal Policy Optimization\n- Uses PPO's clipped objective function for stable updates\n- Trust region optimization prevents large policy changes\n- Entropy regularization encourages exploration\n\n#### 4. Random Network Distillation (RND) Variants\n- Intrinsic motivation for exploration\n- Helps agents discover novel strategies\n- Improves performance in complex environments\n\n## Implementation Details\n\n### Network Architecture\n\n#### Centralized Critic\n```python\nclass CentralizedCritic(nn.Module):\n    def __init__(self, global_state_dim, num_agents):\n        self.network = nn.Sequential(\n            layer_init(nn.Linear(global_state_dim, 128)),\n            nn.Tanh(),\n            layer_init(nn.Linear(128, 128)),\n            nn.Tanh(),\n            layer_init(nn.Linear(128, 1), std=1.0)\n        )\n```\n\n#### Decentralized Actors\n```python\nclass Actor(nn.Module):\n    def __init__(self, observation_dim, action_dim):\n        self.network = nn.Sequential(\n            layer_init(nn.Linear(observation_dim, 128)),\n            nn.Tanh(),\n            layer_init(nn.Linear(128, 128)),\n            nn.Tanh(),\n        )\n        self.actor = layer_init(nn.Linear(128, 64), std=0.01)\n```\n\n### Training Process\n\n1. **Environment Interaction**\n   - Multiple parallel environments (15 by default)\n   - Agents interact using decentralized policies\n   - Global state information is collected for critic\n\n2. **Experience Collection**\n   - Rollout length: 256 steps per environment (longer than IPPO)\n   - Store local observations, actions, rewards, global states\n   - Compute advantages using centralized critic\n\n3. **Policy Updates**\n   - PPO epochs: 10 (more than IPPO for better convergence)\n   - Minibatch size: 3840 (15 envs × 256 steps)\n   - Learning rate: 2.5e-4 with linear annealing\n\n4. **Optimization**\n   - Adam optimizer with gradient clipping (0.5)\n   - Orthogonal initialization for stable training\n   - Entropy coefficient: 0.02 for enhanced exploration\n\n## Supported Environments\n\n### 1. Simple Spread (Cooperative)\n- **Environment**: `simple_spread_v3`\n- **Task**: Cooperative navigation where agents must cover landmarks\n- **Actions**: Discrete (5 actions per agent)\n- **Observations**: Vector observations with agent positions\n- **Global State**: Full environment state including all agent positions\n\n### 2. Cooperative Pong (Butterfly)\n- **Environment**: `cooperative_pong_v5`\n- **Task**: Cooperative version of Pong where agents work together\n- **Actions**: Discrete actions for paddle movement\n- **Observations**: Image-based observations\n- **Global State**: Full game state including ball and paddle positions\n\n### 3. RND-Enhanced Environments\n- **Purpose**: Improved exploration through intrinsic motivation\n- **Implementation**: RND networks provide additional reward signals\n- **Benefits**: Better performance in complex, sparse-reward environments\n\n## Usage\n\n### Installation\n```bash\npip install torch pettingzoo[mpe,butterfly] supersuit wandb tqdm imageio opencv-python gymnasium\n```\n\n### Training Commands\n\n#### Standard MAPPO (Simple Spread)\n```bash\npython mappo_without_rnd.py --env_id simple_spread_v3 --total_timesteps 20000000\n```\n\n#### MAPPO with RND\n```bash\npython mappo_rnd.py --env_id simple_spread_v3 --total_timesteps 20000000\n```\n\n#### MAPPO for Cooperative Pong\n```bash\npython mappo_rnd_pong.py --env_id cooperative_pong_v5 --total_timesteps 10000000\n```\n\n#### MAPPO Training Script\n```bash\npython train.py --env_id cooperative_pong_v5 --total_timesteps 10000000\n```\n\n### Key Hyperparameters\n\n```python\n# Training Configuration\nlr = 2.5e-4                    # Learning rate\nnum_envs = 15                  # Parallel environments\nmax_steps = 256               # Rollout length (longer than IPPO)\nPPO_EPOCHS = 10               # PPO update epochs (more than IPPO)\nclip_coeff = 0.2              # PPO clipping coefficient\nENTROPY_COEFF = 0.02          # Entropy regularization (higher than IPPO)\nGAE = 0.95                    # GAE lambda parameter\ntotal_timesteps = 20000000    # Total training steps\n```\n\n### Evaluation\n```bash\n# Evaluate trained model\npython mappo_without_rnd.py --eval --checkpoint \"checkpoint.pt\"\n\n# Interactive play\npython play_ippo.py \"checkpoint.pt\"\n```\n\n## Technical Implementation\n\n### File Structure\n```\nMAPPO/\n├── mappo_without_rnd.py    # Standard MAPPO implementation\n├── mappo_rnd.py           # MAPPO with RND for exploration\n├── mappo_rnd_pong.py      # MAPPO with RND for cooperative Pong\n├── train.py               # MAPPO training script\n├── images/                # Training visualizations\n│   └── simple_spread.mp4  # Demo video\n└── README.md             # This file\n```\n\n### Key Classes\n\n#### Config\nCentralized configuration class containing all hyperparameters and training settings.\n\n#### CentralizedCritic\nGlobal value function that has access to the full environment state.\n\n#### Actor Networks\nDecentralized policy networks for each agent.\n\n#### MAPPO Trainer\nMain training loop implementing the MAPPO algorithm with centralized training.\n\n## RND Integration\n\n### Random Network Distillation\nRND provides intrinsic motivation by measuring how \"surprising\" or \"novel\" an observation is:\n\n```python\nclass RNDNetwork(nn.Module):\n    def __init__(self, observation_dim):\n        self.predictor = nn.Sequential(...)  # Predicts target features\n        self.target = nn.Sequential(...)     # Fixed target network\n```\n\n## References\n\n### Papers\n- [The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games](https://arxiv.org/abs/2103.01955)\n- [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)\n- [Exploration by Random Network Distillation](https://arxiv.org/abs/1810.12894)\n- [Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/abs/1706.02275)\n\n### Code References\n- [CleanRL MAPPO Implementation](https://github.com/vwxyzjn/cleanrl)\n- [PettingZoo Multi-Agent Environments](https://pettingzoo.farama.org/)\n- [SuperSuit Environment Wrappers](https://github.com/Farama-Foundation/SuperSuit)\n\n---\n\n## Contributing\n\nThis implementation is part of a larger MARL research project. Contributions are welcome in the form of:\n- Bug reports and fixes\n- Performance improvements\n- New environment support\n- Algorithm extensions\n\n## License\n\nThis implementation is open source and available under the MIT License.\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Reinforcement-Learning/tree/master/MARL/MAPPO",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Reinforcement-Learning/contents/MARL/MAPPO",
      "download_url": null,
      "created_date": "2025-08-21",
      "github_date": "2025-08-21",
      "category": "Multi-Agent",
      "framework": "PyTorch",
      "environment": "Atari",
      "categories": [
        "Actor-Critic",
        "Exploration",
        "Multi-Agent"
      ]
    },
    {
      "name": "Self Play",
      "path": "MARL/Self Play",
      "display_name": "Self Play (MARL Self Play)",
      "description": "Implementation of Self Play reinforcement learning algorithm",
      "readme_content": "# Self-Play Multi-Agent Reinforcement Learning\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/MARL/IPPO/images/pong.gif\" width=\"400\" alt=\"Self-Play Pong Demo\"/>\n  <br>\n  <em>Self-play agents competing in Pong environment</em>\n</p>\n\n## Overview\n\n**Self-Play** is a powerful training paradigm in multi-agent reinforcement learning where agents learn by competing against themselves or other agents from the same population. This approach has been instrumental in achieving superhuman performance in games like Go, Chess, and Dota 2. Our implementation focuses on competitive environments like Pong, where agents learn optimal strategies through continuous self-improvement.\n\n## Self-Play Theory\n\n### Core Concept\nSelf-play operates on the principle that an agent can improve by playing against increasingly skilled versions of itself. This creates a natural curriculum where the agent's opponent (itself) becomes progressively stronger, forcing continuous improvement.\n\n### Key Mechanisms\n\n#### 1. Population-Based Learning\n- Multiple agents form a population\n- Agents compete against each other\n- Best strategies are preserved and improved\n\n#### 2. Opponent Sampling\n- Agents play against current and historical versions\n- Prevents overfitting to a single opponent\n- Maintains diverse strategy exploration\n\n#### 3. Strategy Evolution\n- Successful strategies are reinforced\n- Novel strategies emerge through exploration\n- Continuou\n## Implementation Details\n\n### Network Architecture\n\n#### Shared Policy Network\n```python\nclass Agent(nn.Module):\n    def __init__(self, action_space):\n        # Shared CNN feature extractor\n        self.network = nn.Sequential(\n            layer_init(nn.Conv2d(6, 32, kernel_size=8, stride=4)),\n            nn.ReLU(),\n            layer_init(nn.Conv2d(32, 64, kernel_size=4, stride=2)),\n            nn.ReLU(),\n            layer_init(nn.Conv2d(64, 64, kernel_size=3, stride=1)),\n            nn.ReLU(),\n            nn.Flatten(),\n            layer_init(nn.Linear(64 * 7 * 7, 512)),\n            nn.ReLU(),\n        )\n        # Actor and Critic heads\n        self.actor = layer_init(nn.Linear(512, action_space), std=0.01)\n        self.critic = layer_init(nn.Linear(512, 1), std=1.0)\n```\n\n#### Observation Processing\n- **Input**: 6-channel observation (4-frame stack + agent indicator)\n- **Preprocessing**: Grayscale, resize to 84×84, frame stacking\n- **Output**: Action probabilities and state value\n\n### Training Process\n\n1. **Environment Setup**\n   - PettingZoo Atari Pong-v3 environment\n   - Two agents compete in each episode\n   - Shared policy network for both agents\n\n2. **Experience Collection**\n   - 16 parallel environments\n   - 128 steps per rollout\n   - Store observations, actions, rewards, values\n\n3. **Self-Play Training**\n   - Agents compete against each other\n   - Winner gets positive reward, loser gets negative\n   - Policy updates based on competitive outcomes\n\n4. **Opponent Management**\n   - Current policy serves as opponent\n   - Historical policies can be used for diversity\n   - Prevents overfitting to current strategy\n\n## Supported Environments\n\n### 1. Pong (Atari)\n- **Environment**: `pong_v3`\n- **Task**: Competitive Pong game\n- **Actions**: 6 discrete actions (NOOP, FIRE, RIGHT, LEFT, FIRE_RIGHT, FIRE_LEFT)\n- **Observations**: 6-channel image (4-frame stack + agent indicator)\n- **Reward**: +1 for winning, -1 for losing\n\n### 2. Custom Competitive Environments\n- **Extensible**: Framework supports other competitive games\n- **Modular**: Easy to adapt to new environments\n- **Scalable**: Supports multiple agents and teams\n\n## Usage\n\n### Installation\n```bash\npip install torch pettingzoo[atari] supersuit wandb tqdm imageio opencv-python gymnasium\n```\n\n### Training Commands\n\n#### Main Self-Play Training\n```bash\ncd MARL\npython train.py --env_id pong_v3 --total_timesteps 15000000\n```\n\n#### Alternative Self-Play Driver\n```bash\ncd MARL/Self Play\npython self_play.py --env_id pong_v3 --total_timesteps 15000000\n```\n\n### Key Hyperparameters\n\n```python\n# Self-Play Configuration\nlr = 2.5e-4                    # Learning rate\nnum_envs = 16                  # Parallel environments\nmax_steps = 128               # Rollout length\nPPO_EPOCHS = 4                # PPO update epochs\nclip_coeff = 0.1              # PPO clipping coefficient\nENTROPY_COEFF = 0.01          # Entropy regularization\ntotal_timesteps = 15000000    # Total training steps\n```\n\n### Interactive Play\n\n#### Human vs AI\n```bash\npython play.py \"pt files/Pong-MARL.pt\"\n```\n\n**Controls:**\n- `W` or `↑`: Move right\n- `S` or `↓`: Move left\n- `F`: Fire\n- `D`: Fire right\n- `A`: Fire left\n- `Q`: Quit\n\n#### AI vs AI\n```bash\npython play.py \"pt files/Pong-MARL.pt\" --ai_vs_ai\n```\n\n## Results and Performance\n\n### Training Metrics\n- **Convergence**: Typically converges within 10-15M timesteps\n- **Win Rate**: Agents achieve >90% win rate against random opponents\n- **Strategy Evolution**: Emergence of sophisticated playing strategies\n\n### Emergent Behaviors\n\n#### 1. Defensive Strategies\n- Agents learn to position paddles optimally\n- Effective blocking of opponent shots\n- Strategic use of paddle movement\n\n#### 2. Offensive Strategies\n- Agents develop sophisticated shot patterns\n- Use of angles and speed variations\n- Exploitation of opponent weaknesses\n\n#### 3. Adaptive Play\n- Agents adapt to opponent strategies\n- Counter-strategies emerge naturally\n- Continuous improvement through competition\n\n## Advantages of Self-Play\n\n### 1. Automatic Curriculum\n- Difficulty increases naturally with agent improvement\n- No manual curriculum design required\n- Optimal learning progression\n\n### 2. Strategy Discovery\n- Novel strategies emerge through exploration\n- Agents discover optimal play patterns\n- No human expertise required\n\n### 3. Robustness\n- Agents learn to handle diverse opponents\n- Strategies generalize well\n- Robust to different playing styles\n\n### 4. Scalability\n- Works with any number of agents\n- Easy to extend to new environments\n- Minimal human intervention required\n\n## Technical Implementation\n\n### File Structure\n```\nSelf Play/\n├── play.py              # Interactive play script\n├── self_play.py         # Self-play training driver\n├── pt files/           # Saved checkpoints\n│   └── Pong-MARL.pt   # Pre-trained model\n└── README.md          # This file\n```\n\n### Key Classes\n\n#### Agent\nShared policy network used by both competing agents.\n\n#### SelfPlayTrainer\nMain training loop implementing self-play with PPO updates.\n\n#### PlayEnvironment\nInteractive environment for human vs AI and AI vs AI gameplay.\n\n## Pre-trained Models\n\n### Pong-MARL.pt\n- **Training**: 15M timesteps of self-play training\n- **Performance**: >90% win rate against random opponents\n- **Usage**: Ready for immediate evaluation and interactive play\n- **Size**: ~19MB\n\n### Loading Pre-trained Models\n```python\nimport torch\nagent = Agent(action_space)\ncheckpoint = torch.load(\"pt files/Pong-MARL.pt\")\nagent.load_state_dict(checkpoint['model_state_dict'])\n```\n\n## Comparison with Other Approaches\n\n| Aspect | Self-Play | Supervised Learning | Imitation Learning |\n|--------|-----------|-------------------|-------------------|\n| Data Requirements | None | Human demonstrations | Human demonstrations |\n| Strategy Discovery | Automatic | Limited | Limited |\n| Scalability | High | Medium | Medium |\n| Performance | Excellent | Good | Good |\n| Implementation | Simple | Complex | Complex |\n\n## Future Work\n\n### Potential Improvements\n1. **Population-Based Training**: Multiple agent populations\n2. **Meta-Learning**: Fast adaptation to new opponents\n3. **Hierarchical Policies**: Multi-level strategy learning\n4. **Communication**: Adding explicit communication channels\n\n### Research Directions\n1. **Multi-Agent Self-Play**: Teams of agents competing\n2. **Transfer Learning**: Cross-game knowledge transfer\n3. **Adversarial Training**: Improving robustness\n4. **Scalable Architectures**: Handling larger games\n\n## References\n\n### Papers\n- [Mastering the Game of Go with Deep Neural Networks and Tree Search](https://www.nature.com/articles/nature16961)\n- [Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/abs/1712.01815)\n- [Dota 2 with Large Scale Deep Reinforcement Learning](https://arxiv.org/abs/1912.06680)\n- [The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games](https://arxiv.org/abs/2103.01955)\n\n### Code References\n- [PettingZoo Atari Environments](https://pettingzoo.farama.org/environments/atari/)\n- [SuperSuit Environment Wrappers](https://github.com/Farama-Foundation/SuperSuit)\n- [CleanRL PPO Implementation](https://github.com/vwxyzjn/cleanrl)\n\n---\n\n## Contributing\n\nThis implementation is part of a larger MARL research project. Contributions are welcome in the form of:\n- Bug reports and fixes\n- Performance improvements\n- New environment support\n- Algorithm extensions\n\n## License\n\nThis implementation is open source and available under the MIT License.\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Reinforcement-Learning/tree/master/MARL/Self Play",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Reinforcement-Learning/contents/MARL/Self Play",
      "download_url": null,
      "created_date": "2025-08-21",
      "github_date": "2025-08-21",
      "category": "Multi-Agent",
      "framework": "PyTorch",
      "environment": "Atari",
      "categories": [
        "Actor-Critic",
        "Exploration",
        "Multi-Agent"
      ]
    },
    {
      "name": "PPO",
      "path": "PPO",
      "display_name": "PPO (PPO)",
      "description": "Implementation of PPO reinforcement learning algorithm",
      "readme_content": "# Proximal Policy Optimization (PPO)\n\nThis directory contains implementations of the Proximal Policy Optimization (PPO) algorithm for various environments in PyTorch.\n\n## Overview\n\nPPO is a state-of-the-art policy gradient method that combines the stability of trust region methods with the simplicity and efficiency of first-order optimization. It addresses the issue of choosing the right step size when optimizing policies by introducing a clipped surrogate objective function.\n\nKey features of this implementation:\n- Clipped surrogate objective function for stable policy updates\n- Actor-Critic architecture with value function for advantage estimation\n- Generalized Advantage Estimation (GAE) for reduced variance\n- Configurable hyperparameters for different environments\n- Two implementations: separate actor-critic networks and unified network\n\n## Implementations\n\nThis repository includes two main PPO implementations:\n\n1. **Standard PPO (`train.py`)**: Uses separate networks for the actor (policy) and critic (value function). Applied to the LunarLander environment.\n\n\n## Environments\n\nThis implementation has been tested on:\n- **CartPole-v1**: A classic control task where a pole is attached to a cart that moves along a frictionless track.\n- **LunarLander-v3**: A more complex environment where an agent must land a lunar module on a landing pad.\n- **Pendulum-v1**: A continuous control task where the agent learns to balance a pendulum by applying torque.\n- **BipedalWalker-v3**: A continuous control environment where the agent learns to walk forward using bipedal locomotion.\n- **CarRacing-v3**: A continuous control environment where the agent learns to drive a car around a track from a top-down view.\n- **ViZDoom Basic**: A 3D first-person shooter environment where the agent learns to navigate and collect health packs.\n\nFor MuJoCo environments (HalfCheetah, Humanoid, Ant, etc.), see the dedicated [MuJoCo folder](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/PPO/MuJoCo) with specialized implementations and detailed documentation.\n\n\n\n## Configuration\n\nThe implementation uses a `Config` class that specifies hyperparameters for training:\n\n- `exp_name`: Name of the experiment\n- `seed`: Random seed for reproducibility\n- `env_id`: ID of the Gymnasium environment\n- `episodes`: Number of episodes to train\n- `lr` / `learning_rate`: Learning rate for the optimizer\n- `gamma`: Discount factor\n- `clip_value`: PPO clipping parameter (epsilon)\n- `PPO_EPOCHS`: Number of optimization epochs per batch\n- `ENTROPY_COEFF`: Coefficient for entropy bonus\n- `max_steps`: Maximum number of steps per episode (for `train.py`)\n\nAdditional important parameters:\n- `VALUE_COEFF`: Coefficient for the value function loss\n\n## Algorithm Details\n\nPPO works by:\n\n1. **Collecting Experience**: The agent interacts with the environment to collect trajectories.\n2. **Computing Advantages**: Generalized Advantage Estimation (GAE) is used to estimate the advantage function.\n3. **Policy Update**: The policy is updated using the clipped surrogate objective:\n   ```\n   L = min(r_t(θ) * A_t, clip(r_t(θ), 1-ε, 1+ε) * A_t)\n   ```\n   where r_t(θ) is the ratio of new to old policy probabilities, A_t is the advantage, and ε is the clip parameter.\n4. **Value Function Update**: The value function is updated to better predict returns.\n\nThe clipping mechanism prevents too large policy updates, improving stability without the computational overhead of trust region methods like TRPO.\n\n## Architecture\n\n### Standard PPO (train.py)\n- **Actor Network**: Maps states to action probabilities\n- **Critic Network**: Estimates the value function\n\n### Unified PPO (train_unified.py)\n- **Shared Layers**: Extract features from the state\n- **Policy Head**: Maps features to action probabilities\n- **Value Head**: Maps features to state value estimates\n\n## Logging and Monitoring\n\nTraining progress is logged using:\n- **TensorBoard**: Local visualization of training metrics\n- **Weights & Biases (WandB)**: Cloud-based experiment tracking\n- **Video Capture**: Records videos of agent performance at intervals\n\n## Results\n\n### LunarLander\n\nThe following image shows the training performance on the LunarLander environment:\n\n![LunarLander Training Results](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/PPO/images/lunar.png)\n\n### Cartpole\n\nThe following image shows the training performance on the CartPole environment:\n\n![CartPole Training Results](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/PPO/images/cart.png)\n\n### ViZDoom Basic\n\nThe following image shows the training performance on the ViZDoom Basic environment:\n\n![ViZDoom Basic Training Results](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/PPO/images/doom-basic.png)\n\nAgent gameplay demonstration:\n\n![ViZDoom Basic Gameplay](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/PPO/images/vizdoom-basic.gif)\n\n### ViZDoom Defend the Center\n\nPPO has been successfully applied to the ViZDoom Defend the Center environment, a challenging 3D first-person shooter task where the agent must defend against enemies approaching from all directions. \n\nAgent gameplay demonstration:\n\n![ViZDoom Defend the Center Gameplay](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/PPO/images/defend_the_center.gif)\n\nDetailed training results and analysis can be found in this comprehensive report:\n[**VizDoom Defend The Center PPO - WandB Report**](https://wandb.ai/rentio/cleanRL-Atari/reports/VizDoom-Defend-The-Center-PPO---VmlldzoxMzQ3MTY5NQ?accessToken=2yj2rba31fe3bd05niesrduhj0i8u7n7on0oz449nmew58bhf7h5acmc2qbdu5gf)\n\n### Car Racing\n\nThe following image shows the training performance on the CarRacing-v3 environment:\n\n![Car Racing Training Results](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/PPO/images/car-racing.png)\n\n![Car Racing Output](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/PPO/images/car-racing.gif)\n\nPPO has been successfully applied to the CarRacing-v3 environment, a challenging continuous control task where the agent must learn to drive a car around a randomly generated track. The environment features:\n- **Continuous action space**: Steering, acceleration, and braking\n- **High-dimensional visual input**: 96x96 RGB images from a top-down view\n- **Complex dynamics**: Realistic car physics and track generation\n\nDetailed training results and analysis can be found in this comprehensive report:\n[**PPO on Car Racing v3 - WandB Report**](https://wandb.ai/rentio/cleanRL/reports/PPO-on-Car-Racing-v3--VmlldzoxMzQ3MTE4MQ?accessToken=alohq5rg73rh5jzaznz68gsi1ylbuc2c3f7octr217iff29lewvau8wed6129231)\n\n### Pendulum\n\nPPO has been successfully applied to the Pendulum-v1 environment, a classic continuous control task where the agent must learn to balance a pendulum by applying torque. The environment features:\n- **Continuous action space**: Single continuous action (torque) between -2 and 2\n- **Continuous state space**: 3-dimensional observation (cos(θ), sin(θ), angular velocity)\n- **Challenging dynamics**: The agent must learn to swing up and balance the pendulum at the upright position\n\nThe following shows the training performance and agent behavior:\n\n![Pendulum Training Results](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/PPO/images/pendulum.gif)\n\nDetailed training results and analysis can be found in this comprehensive report:\n[**PPO on Pendulum-v1 - WandB Report**](https://wandb.ai/rentio/cleanRL/reports/PPO-on-Pendulum-v1--VmlldzoxMzQ3MTA5MQ)\n\n### BipedalWalker\n\nPPO has been successfully applied to the BipedalWalker-v3 environment, a challenging continuous control task where the agent must learn to walk forward using bipedal locomotion. The environment features:\n- **Continuous action space**: 4-dimensional continuous actions controlling hip and knee torques for both legs\n- **Continuous state space**: 24-dimensional observation including hull angle, angular velocity, leg positions, and velocities\n- **Challenging dynamics**: The agent must learn to coordinate multiple joints to achieve stable walking while maintaining balance\n\nThe following shows the training performance and agent behavior:\n\n![BipedalWalker Training Results](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/PPO/images/bipedal-walker.gif)\n\nDetailed training results and analysis can be found in this comprehensive report:\n[**PPO on BipedalWalker-v3 - WandB Report**](https://api.wandb.ai/links/rentio/v3cfjd2d)\n\n## Dependencies\n\n- PyTorch\n- Gymnasium\n- NumPy\n- WandB (optional, for experiment tracking)\n- TensorBoard\n- OpenCV\n- Tqdm\n\n## References\n\n\n- [CleanRL](https://github.com/vwxyzjn/cleanrl) - Inspiration for code structure and implementation style\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Reinforcement-Learning/tree/master/PPO",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Reinforcement-Learning/contents/PPO",
      "download_url": null,
      "created_date": "2025-08-21",
      "github_date": "2025-08-21",
      "category": "Actor-Critic",
      "framework": "PyTorch",
      "environment": "Atari",
      "categories": [
        "Actor-Critic"
      ]
    },
    {
      "name": "Atari",
      "path": "PPO/Atari",
      "display_name": "Atari (PPO Atari)",
      "description": "Implementation of Atari reinforcement learning algorithm",
      "readme_content": "# PPO on Atari Games\n\nThis directory contains **Proximal Policy Optimization (PPO)** implementations for training agents on classic Atari games using PyTorch and Gymnasium.\n\n## 🎮 Overview\n\nThis implementation features vectorized PPO training on Atari environments with proper preprocessing, frame stacking, and environment wrappers. The code includes both custom implementations and Stable Baselines3 benchmarks for comparison.\n\n### 🏓 Trained Agents in Action\n\n#### Pong\n![Pong Agent](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/PPO/Atari/images/pong.gif)\n\n*PPO agent playing Pong after 10M training steps - achieving consistent wins against the built-in AI opponent*\n\n#### Bowling\n![Bowling Agent](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/PPO/Atari/images/bowling.gif)\n\n*PPO agent playing Bowling after 10M training steps - demonstrating learned bowling strategies and consistent scoring*\n\n\n#### Boxing\n![Boxing Agent](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/PPO/Atari/images/boxing.gif)\n\n*PPO agent playing Boxing after 10M training steps - showing aggressive fighting strategies and effective combat techniques*\n\n## 🎯 Supported Games\n\n- **Pong** (`PongNoFrameskip-v4`) - Classic paddle game\n- **Boxing** (`BoxingNoFrameskip-v4`) - Fighting game\n- **Bowling** (`BowlingNoFrameskip-v4`) - Bowling simulation\n\n## 📁 Files\n\n- `atari-pong.py` - PPO implementation for Pong\n- `boxing-atari.py` - PPO implementation for Boxing  \n- `atari-bowling.py` - PPO implementation for Bowling\n- `sb3-atari-benchmark.py` - Stable Baselines3 benchmark comparison\n- `images/` - Contains training videos and results\n\n## 🚀 Features\n\n### Core PPO Implementation\n- **Vectorized Training**: 8 parallel environments for efficient data collection\n- **Generalized Advantage Estimation (GAE)**: λ = 0.95 for bias-variance tradeoff\n- **Clipped Surrogate Objective**: Prevents destructive policy updates\n- **Value Function Clipping**: Stabilizes critic training\n- **Gradient Clipping**: Prevents exploding gradients (max_grad_norm = 0.5)\n\n### Atari-Specific Features\n- **Frame Preprocessing**: Grayscale conversion and resizing to 64x64\n- **Frame Stacking**: 4 consecutive frames as input\n- **Standard Atari Wrappers**:\n  - `NoopResetEnv`: Random no-op actions at episode start\n  - `MaxAndSkipEnv`: Frame skipping and max pooling\n  - `EpisodicLifeEnv`: Treats life loss as episode end\n  - `FireResetEnv`: Automatically fires at episode start\n  - `ClipRewardEnv`: Clips rewards to [-1, 1]\n\n### Network Architecture\n```\nCNN Feature Extractor:\n- Conv2d(4, 32, kernel=8, stride=4) + ReLU\n- Conv2d(32, 64, kernel=4, stride=2) + ReLU  \n- Conv2d(64, 64, kernel=3, stride=1) + ReLU\n- Flatten\n- Linear(64*7*7, 512) + ReLU\n\nActor Head: Linear(512, action_space)\nCritic Head: Linear(512, 1)\n```\n\n## ⚙️ Hyperparameters\n\n| Parameter | Value | Description |\n|-----------|-------|-------------|\n| Learning Rate | 2.5e-4 | Adam optimizer learning rate |\n| Discount Factor | 0.99 | Reward discount factor |\n| Parallel Envs | 8 | Number of vectorized environments |\n| Steps per Rollout | 128 | Steps collected per environment |\n| Minibatches | 4 | Number of minibatches per update |\n| PPO Epochs | 4 | Training epochs per rollout |\n| Clip Range | 0.1 | PPO clipping parameter |\n| Entropy Coeff | 0.01 | Entropy bonus coefficient |\n| Value Coeff | 0.5 | Value loss coefficient |\n| GAE Lambda | 0.95 | Advantage estimation parameter |\n\n## 🏃‍♂️ Quick Start\n\n### Prerequisites\n```bash\npip install torch gymnasium ale-py stable-baselines3 wandb opencv-python imageio\n```\n\n### Training\n```bash\n# Train on Pong\npython atari-pong.py\n\n# Train on Boxing\npython boxing-atari.py\n\n# Train on Bowling\npython atari-bowling.py\n\n# Run SB3 benchmark\npython sb3-atari-benchmark.py\n```\n\n### Configuration\nEdit the `Config` class in each file to modify hyperparameters:\n- `total_timesteps`: Total training steps (default: 10M)\n- `env_id`: Environment name\n- `lr`: Learning rate\n- `num_envs`: Number of parallel environments\n- `use_wandb`: Enable Weights & Biases logging\n\n## 📊 Results\n\n### Training Videos\nTraining videos are saved in the `images/` directory:\n- `pong.mp4` - Trained Pong agent gameplay\n- `boxing.mp4` - Trained Boxing agent gameplay\n- `bowling.mp4` - Trained Bowling agent gameplay\n\n### Detailed Training Reports\n📈 **[PPO Atari Pong - Training Report](https://wandb.ai/rentio/cleanRL/reports/PPO-Atari-Pong--VmlldzoxMzY0NzA5NA?accessToken=0f5b8n8lprxffdwlhij5n9sfjlg077uqesbtv5g3wo28pla2gakfgre0t9j5ud4a)**\n\n🎳 **[PPO Atari Bowling - Training Report](https://wandb.ai/rentio/cleanRL/reports/PPO-Atari-Bowling--VmlldzoxMzY0NzA2MQ?accessToken=w5rxv2jqkh8rw3wmzenkfsmnggv61294ksiw44ma05hbv8i11234fuoygk1etjff)**\n\n🥊 **[PPO Atari Boxing - Training Report](https://wandb.ai/rentio/cleanRL/reports/PPO-Atari-Boxing--VmlldzoxMzY0NzA1OQ?accessToken=6yamu6w9kl2w8t799n7p28dfqupd7xvfkile82vuf4usdw8w4idx68fmbnzl5eva)**\n\nThe W&B reports include:\n- Training curves and learning progression\n- Hyperparameter sweeps and optimization\n- Performance metrics and comparisons\n- Loss functions and gradient analysis\n- Real-time training monitoring\n\n## 📈 Performance\n\nThe implementation typically achieves:\n- **Pong**: 15-20 average reward after 10M steps\n- **Boxing**: 80-95 average reward after 10M steps\n- **Bowling**: 40-60 average reward after 10M steps\n\n## 🔧 Technical Details\n\n### Environment Preprocessing\n1. **Observation**: 210x160x3 RGB frames → 64x64 grayscale\n2. **Frame Stacking**: 4 consecutive frames\n3. **Reward Clipping**: Rewards clipped to [-1, 1]\n4. **Life Management**: Episode ends on life loss\n\n### Training Loop\n1. **Rollout Collection**: Collect trajectories from vectorized environments\n2. **Advantage Estimation**: Compute GAE advantages\n3. **Policy Update**: Multiple PPO epochs with minibatch updates\n4. **Value Update**: Train critic with clipped value loss\n\n## 🎯 Key Insights\n\n- **Vectorization**: Dramatically improves sample efficiency\n- **Frame Stacking**: Provides temporal information for decision making\n- **Proper Preprocessing**: Essential for stable Atari training\n- **Clipping**: Prevents destructive policy updates\n- **GAE**: Reduces variance in advantage estimation\n\n## 🔍 Monitoring\n\n- **Tensorboard**: Real-time training metrics\n- **Weights & Biases**: Experiment tracking and visualization\n- **Video Recording**: Periodic agent gameplay videos\n- **Console Logging**: Episode rewards and training progress\n\n## 📚 References\n\n- [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347)\n- [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236)\n- [Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438)\n\n## 💡 Tips for Success\n\n1. **Sufficient Training Time**: Atari games require 10M+ steps\n2. **Proper Preprocessing**: Use standard Atari wrappers\n3. **Stable Learning Rate**: 2.5e-4 works well for most games\n4. **Monitor Training**: Watch for policy collapse or instability\n5. **Vectorization**: Use multiple environments for efficiency",
      "github_url": "https://github.com/YuvrajSingh-mist/Reinforcement-Learning/tree/master/PPO/Atari",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Reinforcement-Learning/contents/PPO/Atari",
      "download_url": null,
      "created_date": "2025-08-21",
      "github_date": "2025-08-21",
      "category": "Actor-Critic",
      "framework": "PyTorch",
      "environment": "Atari",
      "categories": [
        "Actor-Critic"
      ]
    },
    {
      "name": "images",
      "path": "PPO/Atari/images",
      "display_name": "Images (PPO Atari Images)",
      "description": "Implementation of images reinforcement learning algorithm",
      "readme_content": "# PPO on Atari Games\n\nThis directory contains **Proximal Policy Optimization (PPO)** implementations for training agents on classic Atari games using PyTorch and Gymnasium.\n\n## 🎮 Overview\n\nThis implementation features vectorized PPO training on Atari environments with proper preprocessing, frame stacking, and environment wrappers. The code includes both custom implementations and Stable Baselines3 benchmarks for comparison.\n\n### 🏓 Trained Agent in Action\n\n![Pong Agent](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/PPO/Atari/images/pong.mp4)\n\n*PPO agent playing Pong after 10M training steps - achieving consistent wins against the built-in AI opponent*\n\n## 🎯 Supported Games\n\n- **Pong** (`PongNoFrameskip-v4`) - Classic paddle game\n- **Boxing** (`BoxingNoFrameskip-v4`) - Fighting game\n- **Bowling** (`BowlingNoFrameskip-v4`) - Bowling simulation\n\n## 📁 Files\n\n- `atari-pong.py` - PPO implementation for Pong\n- `boxing-atari.py` - PPO implementation for Boxing  \n- `atari-bowling.py` - PPO implementation for Bowling\n- `sb3-atari-benchmark.py` - Stable Baselines3 benchmark comparison\n- `images/` - Contains training videos and results\n\n## 🚀 Features\n\n### Core PPO Implementation\n- **Vectorized Training**: 8 parallel environments for efficient data collection\n- **Generalized Advantage Estimation (GAE)**: λ = 0.95 for bias-variance tradeoff\n- **Clipped Surrogate Objective**: Prevents destructive policy updates\n- **Value Function Clipping**: Stabilizes critic training\n- **Gradient Clipping**: Prevents exploding gradients (max_grad_norm = 0.5)\n\n### Atari-Specific Features\n- **Frame Preprocessing**: Grayscale conversion and resizing to 64x64\n- **Frame Stacking**: 4 consecutive frames as input\n- **Standard Atari Wrappers**:\n  - `NoopResetEnv`: Random no-op actions at episode start\n  - `MaxAndSkipEnv`: Frame skipping and max pooling\n  - `EpisodicLifeEnv`: Treats life loss as episode end\n  - `FireResetEnv`: Automatically fires at episode start\n  - `ClipRewardEnv`: Clips rewards to [-1, 1]\n\n### Network Architecture\n```\nCNN Feature Extractor:\n- Conv2d(4, 32, kernel=8, stride=4) + ReLU\n- Conv2d(32, 64, kernel=4, stride=2) + ReLU  \n- Conv2d(64, 64, kernel=3, stride=1) + ReLU\n- Flatten\n- Linear(64*7*7, 512) + ReLU\n\nActor Head: Linear(512, action_space)\nCritic Head: Linear(512, 1)\n```\n\n## ⚙️ Hyperparameters\n\n| Parameter | Value | Description |\n|-----------|-------|-------------|\n| Learning Rate | 2.5e-4 | Adam optimizer learning rate |\n| Discount Factor | 0.99 | Reward discount factor |\n| Parallel Envs | 8 | Number of vectorized environments |\n| Steps per Rollout | 128 | Steps collected per environment |\n| Minibatches | 4 | Number of minibatches per update |\n| PPO Epochs | 4 | Training epochs per rollout |\n| Clip Range | 0.1 | PPO clipping parameter |\n| Entropy Coeff | 0.01 | Entropy bonus coefficient |\n| Value Coeff | 0.5 | Value loss coefficient |\n| GAE Lambda | 0.95 | Advantage estimation parameter |\n\n## 🏃‍♂️ Quick Start\n\n### Prerequisites\n```bash\npip install torch gymnasium ale-py stable-baselines3 wandb opencv-python imageio\n```\n\n### Training\n```bash\n# Train on Pong\npython atari-pong.py\n\n# Train on Boxing\npython boxing-atari.py\n\n# Train on Bowling\npython atari-bowling.py\n\n# Run SB3 benchmark\npython sb3-atari-benchmark.py\n```\n\n### Configuration\nEdit the `Config` class in each file to modify hyperparameters:\n- `total_timesteps`: Total training steps (default: 10M)\n- `env_id`: Environment name\n- `lr`: Learning rate\n- `num_envs`: Number of parallel environments\n- `use_wandb`: Enable Weights & Biases logging\n\n## 📊 Results\n\n### Training Videos\nTraining videos are saved in the `images/` directory:\n- `pong.mp4` - Trained Pong agent gameplay\n- `boxing.mp4` - Trained Boxing agent gameplay\n- `bowling.mp4` - Trained Bowling agent gameplay\n\n### Detailed Training Reports\n📈 **[View Complete Training Report on Weights & Biases](https://wandb.ai/rentio/cleanRL/reports/PPO-Atari-Pong--VmlldzoxMzY0NzA5NA?accessToken=0f5b8n8lprxffdwlhij5n9sfjlg077uqesbtv5g3wo28pla2gakfgre0t9j5ud4a)**\n\nThe W&B report includes:\n- Training curves and learning progression\n- Hyperparameter sweeps and optimization\n- Performance metrics and comparisons\n- Loss functions and gradient analysis\n- Real-time training monitoring\n\n## 📈 Performance\n\nThe implementation typically achieves:\n- **Pong**: 15-20 average reward after 10M steps\n- **Boxing**: 80-95 average reward after 10M steps\n- **Bowling**: 40-60 average reward after 10M steps\n\n## 🔧 Technical Details\n\n### Environment Preprocessing\n1. **Observation**: 210x160x3 RGB frames → 64x64 grayscale\n2. **Frame Stacking**: 4 consecutive frames\n3. **Reward Clipping**: Rewards clipped to [-1, 1]\n4. **Life Management**: Episode ends on life loss\n\n### Training Loop\n1. **Rollout Collection**: Collect trajectories from vectorized environments\n2. **Advantage Estimation**: Compute GAE advantages\n3. **Policy Update**: Multiple PPO epochs with minibatch updates\n4. **Value Update**: Train critic with clipped value loss\n\n## 🎯 Key Insights\n\n- **Vectorization**: Dramatically improves sample efficiency\n- **Frame Stacking**: Provides temporal information for decision making\n- **Proper Preprocessing**: Essential for stable Atari training\n- **Clipping**: Prevents destructive policy updates\n- **GAE**: Reduces variance in advantage estimation\n\n## 🔍 Monitoring\n\n- **Tensorboard**: Real-time training metrics\n- **Weights & Biases**: Experiment tracking and visualization\n- **Video Recording**: Periodic agent gameplay videos\n- **Console Logging**: Episode rewards and training progress\n\n## 📚 References\n\n- [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347)\n- [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236)\n- [Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438)\n\n## 💡 Tips for Success\n\n1. **Sufficient Training Time**: Atari games require 10M+ steps\n2. **Proper Preprocessing**: Use standard Atari wrappers\n3. **Stable Learning Rate**: 2.5e-4 works well for most games\n4. **Monitor Training**: Watch for policy collapse or instability\n5. **Vectorization**: Use multiple environments for efficiency",
      "github_url": "https://github.com/YuvrajSingh-mist/Reinforcement-Learning/tree/master/PPO/Atari/images",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Reinforcement-Learning/contents/PPO/Atari/images",
      "download_url": null,
      "created_date": "2025-08-21",
      "github_date": "2025-08-21",
      "category": "Actor-Critic",
      "framework": "PyTorch",
      "environment": "Atari",
      "categories": [
        "Actor-Critic"
      ]
    },
    {
      "name": "MuJoCo",
      "path": "PPO/MuJoCo",
      "display_name": "Mujoco (PPO Mujoco)",
      "description": "Implementation of MuJoCo reinforcement learning algorithm",
      "readme_content": "# PPO on MuJoCo Environments\n\nThis directory contains PPO implementations specifically for MuJoCo continuous control environments. These environments represent some of the most challenging continuous control tasks in reinforcement learning, requiring sophisticated policy learning to coordinate multiple joints and maintain dynamic stability.\n\n## Overview\n\nMuJoCo (Multi-Joint dynamics with Contact) is a physics engine designed for robotics and biomechanics simulation. The environments implemented here focus on locomotion tasks that require:\n\n- Complex multi-joint coordination\n- Dynamic stability and balance\n- Continuous action spaces with high dimensionality\n- Robust policy learning for physical simulation\n\n## Environments\n\nPPO has been successfully applied to several challenging MuJoCo continuous control environments:\n\n| Environment | Description | Action Space | Observation Space | Demo | WandB Report |\n|-------------|-------------|--------------|-------------------|------|--------------|\n| **HalfCheetah-v5** | Quadrupedal locomotion task where the agent learns to run forward using a cheetah-like body. Requires coordination of hip, knee, and ankle joints for fast and stable running. | 6D continuous (hip, knee, ankle torques) | 17D continuous (joint positions, velocities, and orientations) | ![HalfCheetah](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/PPO/MuJoCo/images/cheetah.gif) | [PPO on HalfCheetah-v5](https://wandb.ai/rentio/cleanRL/reports/PPO-C-on-Half-Cheetah--VmlldzoxMzUxNjA5Mg?accessToken=r6rxlnqyjc1lpoq1gx4w2ybt01jsrta2y8p8ycwmcnisgm60hhabbvywl3aepbpa) |\n| **Humanoid-v5** | Complex humanoid control task with full body coordination. The agent learns to maintain balance and locomotion using a humanoid robot with multiple joints and degrees of freedom. Requires sophisticated control of torso, arms, and legs. | High-dimensional continuous (full body joint torques) | High-dimensional continuous (joint positions, velocities, body orientation, and contact information) | ![Humanoid](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/PPO/MuJoCo/images/humanoid.gif) | [PPO on Humanoid-v5](https://wandb.ai/rentio/cleanRL/reports/PPO-Humanoid--VmlldzoxMzUzMzMwNA) |\n| **Hopper-v5** | Single-leg hopping locomotion task where the agent learns to hop forward while maintaining balance. Requires precise control of thigh, leg, and foot joints to achieve stable hopping motion without falling. | 3D continuous (thigh, leg, foot torques) | 11D continuous (joint positions, velocities, and body orientation) | ![Hopper](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/PPO/MuJoCo/images/hopper.gif) | [PPO on Hopper-v5](https://wandb.ai/rentio/cleanRL/reports/PPO-Hopper---VmlldzoxMzU1NDA0MQ?accessToken=oo2o366arrkrc26st173cov4n701398wca186wzx6dpxy47gadpl9zwkocbvry0r) |\n| **Walker2d-v5** | Bipedal walking locomotion task where the agent learns to walk forward using two legs. Requires coordination of thigh, leg, and foot joints for both legs to achieve stable walking motion while maintaining upright posture. | 6D continuous (left/right thigh, leg, foot torques) | 17D continuous (joint positions, velocities, and body orientation) | ![Walker2d](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/PPO/MuJoCo/images/walker2d.gif) | [PPO on Walker2d-v5](https://api.wandb.ai/links/rentio/sn4gsbac) |\n| **Pusher-v4** | Robotic arm manipulation task where the agent learns to control a 7-DOF arm to push objects to target locations. Requires precise control of multiple joints to position and manipulate objects in 3D space while avoiding obstacles. | 7D continuous (arm joint torques) | 23D continuous (joint angles, velocities, object positions, target location) | ![Pusher](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/PPO/MuJoCo/images/pusher.gif) | [PPO on Pusher-v4](https://wandb.ai/rentio/cleanRL/reports/PPO-Reacher--VmlldzoxMzU1NDA2NA?accessToken=bw14z6xzu5603mvjvha1eoqbzasq3iom5rkzg9q0kr82s57hysdx96m9fm74nz8i) |\n| **Reacher-v4** | Robotic arm reaching task where the agent learns to control a 2-joint arm to reach target positions. Requires precise control of shoulder and elbow joints to position the end effector at randomly placed targets in 2D space. | 2D continuous (shoulder, elbow torques) | 11D continuous (joint angles, velocities, target position, fingertip position) | ![Reacher](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/PPO/MuJoCo/images/reacher.gif) | [PPO on Reacher-v4](https://wandb.ai/rentio/cleanRL/reports/REACHER-V4-PPO--VmlldzoxMzU1NDA5NA?accessToken=jey5gs4ocoyq47foysww4ss9irq9y6s8pnpqamfre6xevadkzk20rvd8afmn04o0) |\n| **Ant-v4** | Quadrupedal locomotion task where the agent learns to control a four-legged ant robot to move forward. Requires coordination of 8 joints (2 per leg) to achieve stable walking while maintaining balance and avoiding falls. | 8D continuous (hip and ankle torques for 4 legs) | 27D continuous (joint positions, velocities, and body orientation) | ![Ant](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/PPO/MuJoCo/images/ant.gif) | [PPO on Ant-v4](https://wandb.ai/rentio/cleanRL/reports/PPO-Ant--VmlldzoxMzU1NDA2NA) |\n| **Swimmer-v4** | Aquatic locomotion task where the agent learns to control a swimming robot to move forward through water. Requires coordination of multiple joints to generate propulsive forces and maintain directional movement in fluid environment. | 2D continuous (joint torques for swimming motion) | 8D continuous (joint angles, velocities, and body orientation) | ![Swimmer](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/PPO/MuJoCo/images/swimmer.gif) | [PPO on Swimmer-v4](https://wandb.ai/rentio/cleanRL/reports/Swimmer-V4-PPO--VmlldzoxMzU1NDEyMA?accessToken=0k2daj964kvs5xdgugt05dx7ssgxlmhxo8y1n0c4uaa1o9w7yo3x0bq7vrnp7sh4) |\n\n## Implementation Details\n\nAll MuJoCo implementations use:\n- **Continuous PPO**: Adapted for continuous action spaces using Gaussian policies\n- **Clipped Surrogate Objective**: For stable policy updates\n- **Value Function Learning**: Separate critic network for state value estimation\n\n### Hyperparameters\nThe MuJoCo environments typically require:\n- Lower learning rates (1e-4 to 3e-4) for stable learning\n- Longer training episodes due to environment complexity\n- Careful entropy coefficient tuning for exploration vs exploitation\n- Higher GAE lambda values for better value estimation\n\n## Files Description\n\n- `half-cheetah.py`: PPO implementation for HalfCheetah-v5 environment\n- `humanoids.py`: PPO implementation for Humanoid-v5 environment\n- `ant-v4.py`: PPO implementation for Ant-v4 quadrupedal locomotion\n- `hopper.py`: PPO implementation for Hopper hopping locomotion\n- `swimmer.py`: PPO implementation for Swimmer aquatic locomotion\n- `pusher.py`: PPO implementation for Pusher manipulation task\n- `reacher.py`: PPO implementation for Reacher arm reaching task\n\n## Usage\n\n```bash\n# Train HalfCheetah\npython half-cheetah.py\n\n# Train Humanoid\npython humanoids.py\n\n# Train other MuJoCo environments\npython ant-v4.py\npython hopper.py\n# ... etc\n```\n\n## Dependencies\n\n- PyTorch\n- Gymnasium[mujoco]\n- MuJoCo (physics engine)\n- NumPy\n- WandB (for experiment tracking)\n- TensorBoard\n\n## Notes\n\n- MuJoCo environments require the MuJoCo physics engine to be installed\n- These environments are computationally intensive and benefit from GPU acceleration\n- Training times can be significant (hours to days depending on environment and hardware)\n- Hyperparameter tuning is crucial for success in these complex environments\n\n## References\n\n- [MuJoCo Official Documentation](https://mujoco.readthedocs.io/)\n- [Gymnasium MuJoCo Environments](https://gymnasium.farama.org/environments/mujoco/)\n- [PPO Paper: Proximal Policy Optimization](https://arxiv.org/abs/1707.06347)\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Reinforcement-Learning/tree/master/PPO/MuJoCo",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Reinforcement-Learning/contents/PPO/MuJoCo",
      "download_url": null,
      "created_date": "2025-08-21",
      "github_date": "2025-08-21",
      "category": "Actor-Critic",
      "framework": "PyTorch",
      "environment": "MuJoCo",
      "categories": [
        "Actor-Critic",
        "Exploration"
      ]
    },
    {
      "name": "REINFORCE",
      "path": "REINFORCE",
      "display_name": "Reinforce (Reinforce)",
      "description": "Implementation of REINFORCE reinforcement learning algorithm",
      "readme_content": "# REINFORCE Algorithm Implementation\n\n## Overview\n\nThis repository contains an implementation of the REINFORCE algorithm (also known as Monte Carlo Policy Gradient), a foundational policy gradient method in reinforcement learning. The implementation is built with PyTorch and supports training on various Gymnasium environments, with a focus on the CartPole-v1 environment.\n\n## Algorithm Description\n\nREINFORCE is a policy gradient method that directly optimizes a policy without using a value function. It belongs to the class of Monte Carlo methods as it uses complete episode returns for updating the policy. The key features of this implementation include:\n\n- **Monte Carlo Policy Gradient**: Updates policy parameters using complete episode returns\n- **Policy Network**: Neural network that maps states to action probabilities\n- **Return Calculation**: Computes discounted returns from rewards\n- **Return Normalization**: Normalizes returns to reduce variance in updates\n- **Gradient Monitoring**: Tracks parameter and gradient statistics during training\n\n### The Algorithm Steps\n\n1. Initialize a parameterized policy π(a|s; θ)\n2. For each episode:\n   - Generate a complete trajectory following the current policy\n   - For each step in the trajectory:\n     - Calculate the discounted return from that step onwards\n   - Update policy parameters using gradient ascent:\n     - θ ← θ + α ∇θ log π(at|st; θ) Gt\n3. Repeat until convergence\n\n## Implementation Details\n\n### Network Architecture\n\nThe policy network consists of:\n- Input layer matching state space dimensions\n- Two hidden layers (32 nodes each) with ReLU activation\n- One hidden layer (16 nodes) with ReLU activation\n- Output layer matching action space dimensions with softmax activation\n\n### Key Features\n\n- **Policy Network**: Maps states to action probabilities\n- **Stochastic Action Selection**: Uses categorical distribution for action sampling\n- **Return Calculation**: Computes discounted returns for each step\n- **Return Normalization**: Reduces variance in policy updates\n- **Gradient and Parameter Monitoring**: Tracks training dynamics with WandB\n- **Evaluation**: Periodically evaluates policy performance\n- **Video Recording**: Captures agent behavior for visualization\n\n## Usage\n\n### Prerequisites\n\n- Python 3.8+\n- PyTorch\n- Gymnasium\n- Stable-Baselines3 (for ReplayBuffer utility)\n- Weights & Biases (for logging)\n- TensorBoard\n- tqdm, numpy, imageio, cv2\n\n### Configuration\n\nThe `Config` class contains all hyperparameters and settings:\n\n```python\nclass Config:\n    # Experiment settings\n    exp_name = \"DQN-CartPole\"  # Can be renamed to \"REINFORCE-CartPole\"\n    seed = 42\n    env_id = \"CartPole-v1\"\n    episodes = 2000  # Number of episodes to train\n    # Training parameters\n    learning_rate = 2e-3\n    gamma = 0.99  # Discount factor\n    # Logging & saving\n    capture_video = True\n    save_model = True\n    use_wandb = True\n    wandb_project = \"cleanRL\"\n    wandb_entity = \"\"  # Your WandB username/team\n```\n\n\n## License\n\nThis project is open source and available under the [MIT License](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/REINFORCE/LICENSE).\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Reinforcement-Learning/tree/master/REINFORCE",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Reinforcement-Learning/contents/REINFORCE",
      "download_url": null,
      "created_date": "2025-08-21",
      "github_date": "2025-08-21",
      "category": "Actor-Critic",
      "framework": "PyTorch",
      "environment": "Gymnasium",
      "categories": [
        "Actor-Critic"
      ]
    },
    {
      "name": "RND",
      "path": "RND",
      "display_name": "RND (RND)",
      "description": "Implementation of RND reinforcement learning algorithm",
      "readme_content": "# Random Network Distillation (RND)\n\nThis directory contains implementations of Random Network Distillation (RND) combined with Proximal Policy Optimization (PPO) for cur### FrozenLake-v1 (8x8 Map)\n\nThe following demonstrates RND performance on the challenging FrozenLake 8x8 environment with 64 states:\n\nAgent gameplay demonstration:\n\n![FrozenLake 8x8 RND Gameplay](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/RND/images/frozen-lake-8x8.gif)\n\n**Detailed Training Metrics**: View the complete training logs and metrics on [Weights & Biases](https://api.wandb.ai/links/rentio/b6xhf0a2)\n\n*Note: FrozenLake 8x8 is a significantly more challenging environment than the standard 4x4 version, with 64 discrete states instead of 16. The stochastic slippery dynamics make exploration particularly difficult, as the agent often slips in unintended directions. RND's intrinsic motivation is crucial here, encouraging systematic exploration of the larger grid world. The prediction error helps the agent discover safe paths through the expanded maze while understanding the stochastic nature of the environment, leading to robust policies that can navigate the complex 8x8 layout.*en exploration in reinforcement learning.\n\n## Overview\n\nRandom Network Distillation (RND) is an exploration method that generates intrinsic rewards by measuring the agent's ability to predict the output of a randomly initialized neural network. The key insight is that prediction error correlates with novelty - states that are visited less frequently are harder to predict, leading to higher intrinsic rewards and encouraging exploration.\n\nKey features of this implementation:\n- **Intrinsic Motivation**: Uses prediction error as curiosity signal\n- **PPO Integration**: Combines RND with PPO for stable policy optimization\n- **Dual Advantage Estimation**: Separate advantages for extrinsic and intrinsic rewards\n- **Normalization**: Running normalization of intrinsic rewards for stable training\n- **Multi-Environment Support**: Works with discrete and continuous control tasks\n\n## How RND Works\n\nRND consists of two neural networks:\n1. **Target Network**: A randomly initialized network that remains fixed throughout training\n2. **Predictor Network**: A learnable network that tries to predict the target network's output\n\nThe prediction error serves as an intrinsic reward:\n```\nintrinsic_reward = ||predictor(state) - target(state)||²\n```\n\nThis encourages the agent to visit novel states where the prediction error is high.\n\n## Implementations\n\nThis repository includes multiple RND implementations:\n\n###  **Classic Control (`train_classic.py`)**\n- Environment: CartPole-v1\n- Simple implementation for discrete action spaces\n- Good starting point for understanding RND\n\n###  **Lunar Lander (`lunar.py`)**\n- Environment: LunarLander-v3\n- Vectorized training with multiple parallel environments\n- More complex continuous state space\n- Optimized for performance and stability\n\n###  **Mountain Car (`mountain-car.py`)**\n- Environment: MountainCar-v0\n- Sparse reward environment ideal for RND exploration\n- Demonstrates RND's effectiveness in hard exploration problems\n\n###  **Frozen Lake (`lake.py`)**\n- Environment: FrozenLake-v1 (8x8 map, 64 states)\n- Discrete stochastic environment with slippery dynamics\n- Demonstrates RND on larger discrete state spaces with one-hot encoding\n\n\n\n## Environments\n\nThis implementation has been tested on:\n- **CartPole-v1**: Classic control task for balancing a pole on a cart\n- **LunarLander-v3**: Spacecraft landing with continuous observations and discrete actions\n- **MountainCar-v0**: Sparse reward environment where the agent must reach the mountain top\n- **FrozenLake-v1**: Discrete 8x8 grid world (64 states) with stochastic slippery movement dynamics\n\n\n## Algorithm Details\n\n### RND + PPO Training Loop\n\n1. **Environment Interaction**: Collect trajectories using current policy\n2. **Intrinsic Reward Calculation**: \n   - Compute prediction error: `||predictor(state) - target(state)||²`\n   - Normalize intrinsic rewards using running statistics\n3. **Advantage Estimation**: \n   - Compute extrinsic advantages using environment rewards\n   - Compute intrinsic advantages using RND rewards\n   - Combine advantages with weighting coefficients\n4. **Policy Update**: Update policy using combined advantages via PPO\n5. **Predictor Update**: Train predictor network to minimize prediction error\n\n### Key Components\n\n#### Target Network\n```python\nclass TargetNet(nn.Module):\n    def __init__(self, state_space):\n        super(TargetNet, self).__init__()\n        # Randomly initialized, never updated\n        self.fc1 = nn.Linear(state_space, 256)\n        self.fc2 = nn.Linear(256, 256)\n        self.fc3 = nn.Linear(256, 256)\n```\n\n#### Predictor Network\n```python\nclass PredictorNet(nn.Module):\n    def __init__(self, state_space):\n        super(PredictorNet, self).__init__()\n        # Learnable network\n        self.fc1 = nn.Linear(state_space, 256)\n        self.fc2 = nn.Linear(256, 256)\n        self.fc3 = nn.Linear(256, 256)\n```\n\n## Configuration\n\n### Core Parameters\n- `EXT_COEFF`: Weight for extrinsic advantages (typically 1.0-2.0)\n- `INT_COEFF`: Weight for intrinsic advantages (typically 0.5-1.0)\n- `lr`: Learning rate for all networks\n- `gamma`: Discount factor for returns\n- `clip_value`: PPO clipping parameter\n\n### RND-Specific Parameters\n- **Intrinsic Reward Normalization**: Running mean and standard deviation\n- **Network Architecture**: Hidden layer sizes for target and predictor networks\n- **Update Frequency**: How often to update the predictor network\n\n## Results\n\n### CartPole-v1\n\nThe following image shows the training performance on the CartPole environment with RND:\n\n![CartPole RND Training Results](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/RND/images/pole.png)\n\nAgent gameplay demonstration:\n\n![CartPole RND Gameplay](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/RND/images/cart.gif)\n\n**Detailed Training Metrics**: View the complete training logs and metrics on [Weights & Biases](https://api.wandb.ai/links/rentio/flnibb95)\n\n*Note: The results demonstrate that RND enables efficient exploration while successfully learning the main task. The agent learns to balance the pole while the intrinsic rewards encourage exploration of novel states.*\n\n### LunarLander-v2\n\nThe following demonstrates RND performance on the more complex LunarLander environment:\n\nAgent gameplay demonstration:\n\n![LunarLander RND Gameplay](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/RND/images/lunar.gif)\n\n\n**Detailed Training Metrics**: View the complete training logs and metrics on [Weights & Biases](https://api.wandb.ai/links/rentio/p5aqeazk)\n\n*Note: In LunarLander, RND helps the agent explore different landing strategies and discover optimal trajectories while learning to land successfully. The intrinsic rewards encourage exploration of various parts of the landing zone.*\n\n### MountainCar-v0\n\nThe following demonstrates RND performance on the challenging MountainCar environment with sparse rewards:\n\nAgent gameplay demonstration:\n\n![MountainCar RND Gameplay](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/RND/images/mountain.gif)\n\n**Detailed Training Metrics**: View the complete training logs and metrics on [Weights & Biases](https://wandb.ai/rentio/cleanRL/reports/RND-PPO-on-MountainCar--VmlldzoxMzQ5MDA1NA)\n\n*Note: MountainCar is a particularly challenging environment for exploration due to its sparse reward structure. The agent only receives reward when reaching the goal at the mountain top. RND's intrinsic motivation is crucial here, as it encourages the agent to explore different positions and velocities, eventually discovering the momentum-building strategy needed to reach the goal. The prediction error helps the agent explore novel state combinations that lead to successful mountain climbing.*\n\n### FlappyBird\n\nThe following demonstrates RND performance on FlappyBird with curiosity-driven exploration:\n\nAgent gameplay demonstration:\n\n![FlappyBird RND Gameplay](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/RND/images/flappy.png)\n\n**Detailed Training Metrics**: View the complete training logs and metrics on [Weights & Biases](https://api.wandb.ai/links/rentio/icqoz8tn)\n\n*Note: In FlappyBird, RND encourages the agent to explore different flight patterns and obstacle navigation strategies. The intrinsic rewards help the agent discover diverse ways to navigate through pipes while learning the core task.*\n\n### FrozenLake-v1\n\nThe following demonstrates RND performance on the stochastic FrozenLake environment:\n\nAgent gameplay demonstration:\n\n![FrozenLake RND Gameplay](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/RND/images/frozenlake.gif)\n\n**Detailed Training Metrics**: View the complete training logs and metrics on [Weights & Biases](https://api.wandb.ai/links/rentio/xag9itvm)\n\n*Note: FrozenLake is a challenging discrete environment with stochastic dynamics where the agent must navigate a slippery frozen lake to reach the goal while avoiding holes. RND's intrinsic motivation is particularly valuable here as it encourages systematic exploration of the grid world. The prediction error helps the agent discover safe paths and understand the stochastic nature of the environment, leading to more robust policies that can handle the slippery dynamics.*\n\n### CarRacing-v3\n\nThe following demonstrates RND performance with CNN on the challenging CarRacing environment:\n\nAgent gameplay demonstration:\n\n![CarRacing RND Gameplay](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/RND/images/car.gif)\n\n**Detailed Training Metrics**: View the complete training logs and metrics on [Weights & Biases](https://wandb.ai/rentio/cleanRL/reports/RND-PPO-w-CNN---VmlldzoxMzU4NTYzNQ?accessToken=peteh2tfa64ls1jpppsx0rp7rk36knn8i4i0nerxges2dpfmt13x9wuzlb7nbcbo)\n\n*Note: CarRacing is a complex continuous control environment with high-dimensional visual observations. RND with CNN architecture enables effective exploration of different racing strategies and track segments. The convolutional neural networks process the visual input while the intrinsic motivation from RND encourages the agent to explore diverse driving behaviors and track areas, leading to improved racing performance through curiosity-driven exploration.*\n\n## Key Benefits of RND\n\n1. **Simple Implementation**: No need for complex count-based exploration\n2. **Scalable**: Works with high-dimensional state spaces\n3. **Stable**: Prediction error provides consistent exploration signal\n4. **Task-Agnostic**: Doesn't require domain knowledge about the environment\n\n## Common Issues and Solutions\n\n### Intrinsic Reward Explosion\n- **Problem**: Intrinsic rewards become too large, overwhelming extrinsic rewards\n- **Solution**: Use running normalization and carefully tune `INT_COEFF`\n\n### Insufficient Exploration\n- **Problem**: Agent doesn't explore enough novel states\n- **Solution**: Increase `INT_COEFF` or improve network architecture\n\n### Training Instability\n- **Problem**: Training becomes unstable due to conflicting objectives\n- **Solution**: Balance `EXT_COEFF` and `INT_COEFF`, use smaller learning rates\n\n## Dependencies\n\n- PyTorch\n- Gymnasium\n- NumPy\n- WandB (optional, for experiment tracking)\n- TensorBoard\n- OpenCV (for video recording)\n- Tqdm (for progress bars)\n- ImageIO (for video saving)\n\n## References\n\n3. **CleanRL**: [Implementation reference](https://github.com/vwxyzjn/cleanrl) - Clean and simple RL implementations\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Reinforcement-Learning/tree/master/RND",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Reinforcement-Learning/contents/RND",
      "download_url": null,
      "created_date": "2025-08-21",
      "github_date": "2025-08-21",
      "category": "Actor-Critic",
      "framework": "PyTorch",
      "environment": "LunarLander",
      "categories": [
        "Actor-Critic",
        "Exploration"
      ]
    },
    {
      "name": "SAC",
      "path": "SAC",
      "display_name": "SAC (SAC)",
      "description": "Implementation of SAC reinforcement learning algorithm",
      "readme_content": "# Soft Actor-Critic (SAC)\n\nThis directory contains implementations of the Soft Actor-Critic (SAC) algorithm for various continuous control environments.\n\n## Overview\n\nSAC is an off-policy actor-critic algorithm designed for continuous action spaces that optimizes a stochastic policy in an off-policy way. It incorporates several key features:\n\n1. **Maximum Entropy Reinforcement Learning**: Encourages exploration by maximizing the policy entropy along with the expected return.\n2. **Actor-Critic Architecture**: Uses a critic to estimate the Q-values and an actor to learn the policy.\n3. **Off-Policy Learning**: Can learn from previously collected data, making it sample-efficient.\n4. **Soft Policy Updates**: Uses soft updates of the target networks to improve stability.\n\nKey features of this implementation:\n- Entropy-regularized reinforcement learning\n- Actor-Critic architecture with automatic temperature tuning\n- Experience replay buffer for stable learning\n- Soft target network updates using Polyak averaging\n- Stochastic policy for better exploration\n- Support for different continuous control environments\n\n## Environments\n\nThis implementation includes support for the following environments:\n- **Pendulum-v1**: A classic control problem where the goal is to balance a pendulum in an upright position.\n- **BipedalWalker-v3**: A more challenging environment where a 2D biped robot must walk forward without falling.\n\n## Configuration\n\nEach implementation includes a `Config` class that specifies the hyperparameters for training. You can modify these parameters to experiment with different settings:\n\n- `exp_name`: Name of the experiment\n- `seed`: Random seed for reproducibility\n- `env_id`: ID of the Gymnasium environment\n- `total_timesteps`: Total number of training steps\n- `learning_rate`: Learning rate for the optimizer\n- `buffer_size`: Size of the replay buffer\n- `gamma`: Discount factor\n- `tau`: Soft update coefficient for target networks\n- `batch_size`: Batch size for training\n- `exploration_fraction`: Fraction of total timesteps for exploration\n- `learning_starts`: Number of timesteps before learning starts\n- `train_frequency`: Frequency of updates to the networks\n\n## Architecture\n\nThe SAC implementation includes:\n\n1. **Actor Network (Policy)**: Outputs a mean and log standard deviation for each action dimension, defining a Gaussian distribution over actions.\n2. **Twin Critic Networks**: Two separate Q-value networks to mitigate overestimation bias.\n3. **Temperature Parameter (Alpha)**: Automatically adjusted to maintain a target entropy level.\n4. **Target Networks**: Slowly updated copies of the critic networks for stability.\n5. **Replay Buffer**: Stores and samples transitions for training.\n\n## Key Advantages of SAC\n\nSAC offers several advantages over other continuous control algorithms:\n\n1. **Sample Efficiency**: Off-policy learning allows SAC to reuse past experiences.\n2. **Stability**: The entropy term and soft updates help stabilize training.\n3. **Exploration-Exploitation Balance**: The maximum entropy framework naturally balances exploration and exploitation.\n4. **Performance**: SAC has shown state-of-the-art performance across many continuous control tasks.\n5. **Robustness**: Less sensitive to hyperparameter tuning compared to other algorithms.\n\n## Logging and Monitoring\n\nTraining progress is logged using:\n- **TensorBoard**: Local visualization of training metrics\n- **Weights & Biases (WandB)**: Cloud-based experiment tracking (optional)\n- **Video Capture**: Records videos of agent performance at intervals\n\n## Results\n\n### Pendulum\n\nThe following image shows the training performance on the Pendulum environment:\n\n![Pendulum Training Results](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/SAC/images/pendulum.png)\n\n### HalfCheetah\n\nAlthough not explicitly implemented in the current codebase, we have training results for the HalfCheetah environment using SAC:\n\n![HalfCheetah Training Results](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/SAC/images/halfcheetah.png)\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Reinforcement-Learning/tree/master/SAC",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Reinforcement-Learning/contents/SAC",
      "download_url": null,
      "created_date": "2025-08-21",
      "github_date": "2025-08-21",
      "category": "Actor-Critic",
      "framework": "PyTorch",
      "environment": "Gymnasium",
      "categories": [
        "Actor-Critic",
        "Exploration"
      ]
    },
    {
      "name": "TD3",
      "path": "TD3",
      "display_name": "TD3 (TD3)",
      "description": "Implementation of TD3 reinforcement learning algorithm",
      "readme_content": "# Twin Delayed Deep Deterministic Policy Gradient (TD3)\n\nThis directory contains implementations of the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm for various continuous control environments.\n\n## Overview\n\nTD3 is an advanced off-policy actor-critic algorithm designed to address the overestimation bias in DDPG. It introduces three critical improvements:\n\n1. **Twin Critics**: Uses two Q-value networks to reduce overestimation bias through taking the minimum Q-value.\n2. **Delayed Policy Updates**: Updates the policy less frequently than the critics to reduce variance.\n3. **Target Policy Smoothing**: Adds noise to the target actions to make the algorithm more robust to errors.\n\nKey features of this implementation:\n- Actor-Critic architecture with twin critics\n- Delayed policy updates\n- Target policy smoothing regularization\n- Experience replay buffer for stable learning\n- Soft target network updates using Polyak averaging\n- Exploration using additive noise\n- Support for different continuous control environments\n\n## Environments\n\nThis implementation includes support for the following environments:\n- **Pendulum-v1**: A classic control problem where the goal is to balance a pendulum in an upright position.\n- **BipedalWalker-v3**: A more challenging environment where a 2D biped robot must walk forward without falling.\n- **HalfCheetah-v5**: A MuJoCo environment where a 2D cheetah-like robot must run forward as fast as possible.\n\n\n## Configuration\n\nEach implementation includes a `Config` class that specifies the hyperparameters for training. You can modify these parameters to experiment with different settings:\n\n- `exp_name`: Name of the experiment\n- `seed`: Random seed for reproducibility\n- `env_id`: ID of the Gymnasium environment\n- `policy_noise`: Standard deviation of noise added to target policy\n- `total_timesteps`: Total number of training steps\n- `learning_rate`: Learning rate for the optimizer\n- `buffer_size`: Size of the replay buffer\n- `gamma`: Discount factor\n- `tau`: Soft update coefficient for target networks\n- `batch_size`: Batch size for training\n- `clip`: Clipping range for target policy smoothing noise\n- `exploration_fraction`: Fraction of total timesteps for exploration\n- `learning_starts`: Number of timesteps before learning starts\n- `train_frequency`: Frequency of updates to the networks\n\n## Architecture\n\nThe TD3 implementation includes:\n\n1. **Actor Network**: Determines the best action in a given state\n2. **Twin Critic Networks**: Two separate networks that evaluate the Q-value of state-action pairs\n3. **Target Networks**: Slowly updated copies of both actor and critics for stability\n4. **Replay Buffer**: Stores and samples transitions for training\n5. **Noise Process**: Adds exploration noise to actions during training\n\n## Improvements Over DDPG\n\nTD3 addresses several shortcomings of DDPG:\n\n1. **Reducing Overestimation Bias**: By using the minimum of two critics, TD3 helps mitigate the overestimation bias that plagues many Q-learning algorithms.\n2. **Stabilized Learning**: Delayed policy updates (updating the policy less frequently than the critics) help reduce variance and stabilize learning.\n3. **Smoother Target Values**: Adding noise to target actions smooths the value function, making the learning process more robust to errors.\n\n## Results\n\nThe implementation includes a video recording (`TD3_BipedalWalker.mp4`) that demonstrates the performance of the trained TD3 agent on the BipedalWalker environment.\n\n### Training Visualizations\n\n#### BipedalWalker Agent\n\n![BipedalWalker Performance](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/TD3/images/BiPedal.png)\n\nHere's a GIF showing the trained TD3 agent navigating the BipedalWalker environment:\n\n![BipedalWalker Agent in Action](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/TD3/images/output_bipedal_walker.gif)\n\n#### HalfCheetah Training\n\nThe following graph shows the training losses for the HalfCheetah environment:\n\n![HalfCheetah Training Loss](https://raw.githubusercontent.com/YuvrajSingh-mist/Reinforcement-Learning/master/TD3/images/lossHalfCheetah.png)\n\n\n- [CleanRL](https://github.com/vwxyzjn/cleanrl) - Inspiration for code structure and implementation style\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Reinforcement-Learning/tree/master/TD3",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Reinforcement-Learning/contents/TD3",
      "download_url": null,
      "created_date": "2025-08-21",
      "github_date": "2025-08-21",
      "category": "Actor-Critic",
      "framework": "PyTorch",
      "environment": "MuJoCo",
      "categories": [
        "Actor-Critic",
        "Exploration"
      ]
    }
  ]
}