{
  "models": [
    {
      "name": "Attention Mechanisms",
      "display_name": "Attention Mechanisms",
      "description": "From scratch implementation of Attention Mechanisms",
      "readme_content": "## Implemented Bahdanau and Luong Attention from scratch in PyTorch\n\n[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n\n[Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/Attention Mechanisms",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/Attention%20Mechanisms?ref=master",
      "download_url": null,
      "created_date": "2025-03-07",
      "github_date": "2025-03-07"
    },
    {
      "name": "BERT",
      "display_name": "Bert",
      "description": "From scratch implementation of BERT",
      "readme_content": "\n# BERT architecture in Pytorch\n\nI implemented the BERT using Pytorch on Cornell Movie Dialog Corpus.\n\n[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\n\n\n### Datasets\n\n**Cornell Movie Dialog Corpus**: [Link](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)\n\n### Frameworks:\n**Pytorch**\n\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/BERT",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/BERT?ref=master",
      "download_url": null,
      "created_date": "2025-02-09",
      "github_date": "2025-02-09"
    },
    {
      "name": "CGANs",
      "display_name": "Cgans",
      "description": "From scratch implementation of CGANs",
      "readme_content": "# Conditional Generative Adversarial Networks (CGANs)\n\n![CGAN Architecture](https://github.com/YuvrajSingh-mist/Paper-Replications/blob/main/CGANs/output_images/MNIST/fake_images_steps_14000.png)\n\n## Overview\n\nThis repository contains a PyTorch implementation of Conditional Generative Adversarial Networks (CGANs) as described in the paper [\"Conditional Generative Adversarial Nets\" by Mirza & Osindero (2014)](https://arxiv.org/abs/1411.1784). CGANs extend the original GAN framework by conditioning both the generator and discriminator on auxiliary information, allowing for controlled generation of specific types of data.\n\n## Key Features\n\n- **Conditional Generation**: Generate MNIST digits conditioned on specific class labels (0-9)\n- **Deep Convolutional Architecture**: Uses ConvTranspose2d layers for the generator and Conv2d layers for the discriminator\n- **Label Embedding**: Efficient label representation using embedding layers\n- **Instance Normalization**: Stable training with InstanceNorm2d layers\n- **TensorBoard Logging**: Real-time monitoring of training progress and generated samples\n- **Progressive Image Saving**: Saves generated images at regular intervals during training\n\n## Architecture Details\n\n### Generator\n- **Input**: Random noise vector (100D) + class label\n- **Embedding**: Class labels are embedded into 100D vectors\n- **Architecture**: \n  - ConvTranspose2d layers with increasing spatial dimensions\n  - InstanceNorm2d for stable training\n  - ReLU activations (Tanh for output)\n  - Output: 64x64 grayscale images\n\n### Discriminator\n- **Input**: 64x64 image + class label\n- **Embedding**: Class labels embedded to match image dimensions\n- **Architecture**:\n  - Conv2d layers with decreasing spatial dimensions\n  - InstanceNorm2d for stable training\n  - LeakyReLU activations (Sigmoid for output)\n  - Output: Binary classification (real/fake)\n\n## Model Configuration\n\n```python\n@dataclass\nclass ModelArgs:\n    latent_vector_size = 100      # Noise vector dimension\n    batch_size = 128              # Training batch size\n    num_classes = 10              # Number of MNIST classes\n    img_size = 64                 # Output image size\n    no_of_channels = 1            # Grayscale images\n    dropout = 0.5                 # Dropout rate\n    initial_lr = 0.1              # Initial learning rate\n    final_lr = 1e-6               # Final learning rate\n    momentum_initial = 0.5        # Initial momentum\n    final_momentum_value = 0.7    # Final momentum\n```\n\n## Training Details\n\n- **Dataset**: MNIST (28x28 â†’ resized to 64x64)\n- **Optimizer**: Adam with Î²â‚=0.5, Î²â‚‚=0.999\n- **Learning Rate**: 0.0002\n- **Loss Function**: Binary Cross-Entropy Loss\n- **Epochs**: 30\n- **Batch Size**: 128\n- **Image Normalization**: [-1, 1] range using transforms.Normalize((0.5,), (0.5,))\n\n## File Structure\n\n```\nCGANs/\nâ”œâ”€â”€ cgan.ipynb              # Main implementation notebook\nâ”œâ”€â”€ output_images/          # Generated images during training\nâ”‚   â””â”€â”€ MNIST/             # MNIST-specific outputs\nâ”‚       â”œâ”€â”€ fake_images_steps_*.png    # Generated images at different steps\nâ”‚       â””â”€â”€ real_images_steps_*.png    # Real images for comparison\nâ””â”€â”€ logs/                   # TensorBoard logs\n    â”œâ”€â”€ fake/              # Fake image logs\n    â””â”€â”€ real/              # Real image logs\n```\n\n## Usage\n\n### Training the Model\n\n1. **Setup Environment**:\n   ```python\n   import torch\n   import torchvision\n   from torch import nn\n   from torchvision import transforms\n   from torch.utils.tensorboard import SummaryWriter\n   ```\n\n2. **Load Data**:\n   ```python\n   transforms = torchvision.transforms.Compose([\n       transforms.Resize(size=(64, 64)),\n       transforms.ToTensor(),\n       transforms.Normalize((0.5,), (0.5,))\n   ])\n   \n   trainset = torchvision.datasets.MNIST(root='./data', train=True, \n                                        download=True, transform=transforms)\n   trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n   ```\n\n3. **Initialize Models**:\n   ```python\n   generator = Generator().to(device)\n   discriminator = Discriminator().to(device)\n   \n   # Apply weight initialization\n   generator.apply(weights_init)\n   discriminator.apply(weights_init)\n   ```\n\n4. **Run Training**:\n   Simply execute all cells in the `cgan.ipynb` notebook.\n\n### Generating Specific Digits\n\n```python\n# Generate digit '7' examples\ntarget_label = 7\nnoise = torch.randn(batch_size, 100, 1, 1, device=device)\nlabels = torch.full((batch_size,), target_label, device=device)\n\nwith torch.no_grad():\n    fake_images = generator(noise, labels)\n```\n\n## Training Progress\n\nThe model saves generated images every 500 iterations, allowing you to monitor the quality improvement over time:\n\n- **Early Training** (steps 0-1000): Noisy, unclear digit shapes\n- **Mid Training** (steps 5000-8000): Recognizable digit structures emerge\n- **Late Training** (steps 12000+): High-quality, diverse digit generation\n\n## Key Implementation Details\n\n### Label Conditioning\n- **Generator**: Labels are embedded and concatenated with noise in feature space\n- **Discriminator**: Labels are embedded to image dimensions and concatenated with input images\n\n### Weight Initialization\n```python\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n```\n\n### Loss Functions\n- **Generator Loss**: Tries to fool discriminator â†’ `BCE(D(G(z,c)), 1)`\n- **Discriminator Loss**: Real detection + Fake detection â†’ `BCE(D(x,c), 1) + BCE(D(G(z,c)), 0)`\n\n## Monitoring Training\n\nUse TensorBoard to monitor training progress:\n```bash\ntensorboard --logdir=logs\n```\n\n## Results\n\nThe trained CGAN successfully generates high-quality MNIST digits conditioned on specific class labels. The model demonstrates:\n\n- **Class Consistency**: Generated images match the requested digit class\n- **Diversity**: Multiple variations of each digit class\n- **Quality**: Clear, recognizable handwritten digits\n- **Stability**: Consistent performance across different random seeds\n\n## Paper Reference\n\n```bibtex\n@article{mirza2014conditional,\n  title={Conditional generative adversarial nets},\n  author={Mirza, Mehdi and Osindero, Simon},\n  journal={arXiv preprint arXiv:1411.1784},\n  year={2014}\n}\n```\n\n## Requirements\n\n- PyTorch\n- torchvision\n- tensorboard\n- torchinfo\n- numpy\n- matplotlib\n\n## Future Enhancements\n\n- [ ] Multi-class conditioning beyond MNIST\n- [ ] Progressive growing for higher resolution outputs\n- [ ] Spectral normalization for training stability\n- [ ] FID/IS metrics for quantitative evaluation\n- [ ] Conditional interpolation between classes\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/CGANs",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/CGANs?ref=master",
      "download_url": null,
      "created_date": "2025-08-06",
      "github_date": "2025-08-06"
    },
    {
      "name": "CLAP",
      "display_name": "Clap",
      "description": "From scratch implementation of CLAP",
      "readme_content": "\n\n# Whisper model in Pytorch from scratch implementation\n\nImplementation of CLAP model coded from scratch in Pytorch \n\n\n\n[CLAP : LEARNING AUDIO CONCEPTS FROM NATURAL LANGUAGE SUPERVISION](https://arxiv.org/pdf/2206.04769)\n\n## ModelArgs Hyperparameters\n\n### Hyperparameters\n\n| Parameter             | Value       | Description                                                                 |\n|-----------------------|-------------|-----------------------------------------------------------------------------|\n| `epochs`              | 30          | Number of training epochs.                                                  |\n| `text_embeddings`     | 768         | Dimensionality of text embeddings.                                          |\n| `audio_embeds`        | 2048        | Dimensionality of audio embeddings.                                         |\n| `block_size`          | 100         | Size of input blocks (e.g., sequence length).                               |\n| `batch_size`          | 32          | Number of samples per batch.                                                |\n| `lr`                  | 4e-4        | Learning rate for the main model.                                           |\n| `device`              | `'cuda:0'`  | Device to run the model on (e.g., GPU).                                     |\n| `SAMPLING_RATE`       | 44100       | Sampling rate of the audio (in Hz).                                         |\n| `N_MELS`              | 64          | Number of mel-spectrogram bins.                                             |\n| `max_t`               | 500         | Maximum time steps for sequences.                                           |\n| `n_channels`          | `N_MELS`    | Number of channels in the input (same as `N_MELS`).                         |\n| `window_size`         | 1024        | Window size for STFT (Short-Time Fourier Transform).                        |\n| `hop_size`            | 320         | Hop size for STFT.                                                         |\n| `mel_bins`            | `N_MELS`    | Number of mel bins (same as `N_MELS`).                                      |\n| `fmin`                | 50          | Minimum frequency for mel-spectrogram computation.                          |\n| `fmax`                | 8000        | Maximum frequency for mel-spectrogram computation.                          |\n| `output_embeddings`   | 1024        | Dimensionality of output embeddings.                                        |\n| `head_lr`             | 1e-3        | Learning rate for the task-specific head.                                   |\n| `audio_encoder_lr`    | 1e-4        | Learning rate for the audio encoder.                                        |\n| `text_encoder_lr`     | 1e-5        | Learning rate for the text encoder.                                         |\n\n### Dataset\n\n[Gigaspeech](https://huggingface.co/datasets/speechcolab/gigaspeech)\n\nUsed the 'xs' snapshot.\n\n### Frameworks:\n**Pytorch**\n\n\n### NOTE\nThe loss was stagged at 2.079 -loge(1/8), that is, the logits tend to be too small for softmax to outputs anythign except uniform probs. Pls let me know where am I making a mistake.\n\n\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/CLAP",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/CLAP?ref=master",
      "download_url": null,
      "created_date": "2025-08-06",
      "github_date": "2025-08-06"
    },
    {
      "name": "CLiP",
      "display_name": "Clip",
      "description": "From scratch implementation of CLiP",
      "readme_content": "\n# CLIP architecture in Pytorch\n\nI implemented the CLiP using Pytorch on the flickr8000 dataset.\n\n[Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)\n\n\n### Datasets\n\n**flickr 8000**: [Link](https://www.kaggle.com/datasets/adityajn105/flickr8k)\n\n### Frameworks:\n**Pytorch**\n\n\n### Results (on T4 GPU Single)\n\n**Training epochs:** 30\n\n**Train loss:** 1.3\n**Val loss:** 2.2 \n\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/CLiP",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/CLiP?ref=master",
      "download_url": null,
      "created_date": "2025-04-25",
      "github_date": "2025-04-25"
    },
    {
      "name": "CycleGANs",
      "display_name": "Cyclegans",
      "description": "From scratch implementation of CycleGANs",
      "readme_content": "\n# CycleGAN architecture in Pytorch\n\nI implemented the CycleGAN using Pytorch on the cityscapes dataset.\n\n[CycleGAN](https://arxiv.org/abs/1703.10593)\n\n\n### Datasets\n\n**Cityscapes**: [Link](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)\n\n### Frameworks:\n**Pytorch**\n\n### Generated Images\nPlease see the **output_images_val** directory\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/CycleGANs",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/CycleGANs?ref=master",
      "download_url": null,
      "created_date": "2025-02-09",
      "github_date": "2025-02-09"
    },
    {
      "name": "DCGANs",
      "display_name": "Dcgans",
      "description": "From scratch implementation of DCGANs",
      "readme_content": "\n# DCGAN from Scratch\n\nI implemented a DCGAN Architecture from Scratch using Pytorch on CelebA and CIFAR10 dataset.\n\n[Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/2010.11929)\n\n\n### Datasets\n\n**CIFAR10**: Used torchvision to download the train part of the CIFAR10 dataset directly \\\n**CelebA**: [Link](https://drive.google.com/drive/folders/0B7EVK8r0v71pTUZsaXdaSnZBZzg?resourcekey=0-rJlzl934LzC-Xp28GeIBzQ)\n\n### Frameworks:\n**Pytorch**\n\n\n### Results\n\n**Training steps:** 7800\n\n**CelebA**\n\n![fake_images_steps_11700](https://github.com/YuvrajSingh-mist/Paper-Replications/assets/141050962/0e0c42ff-3f07-40a3-9a68-60d432461186)\n\n\n\n**Training steps:** 11700\n\n**CIFAR10**\n\n![fake_images_steps_7500](https://github.com/YuvrajSingh-mist/Paper-Replications/assets/141050962/09ce91e1-45d5-4929-ba25-50f4ef874490)\n\n\n#### Model weights Download Link: [link](https://drive.google.com/drive/folders/1BzSxP1k-6BIhgYSodi0rMsmzITP07YVS?usp=sharing)\n\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/DCGANs",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/DCGANs?ref=master",
      "download_url": null,
      "created_date": "2024-07-10",
      "github_date": "2024-07-10"
    },
    {
      "name": "DDP",
      "display_name": "Ddp",
      "description": "From scratch implementation of DDP",
      "readme_content": "\r\n# Trained Llama using DDP in Pytorch\r\n\r\nI implemented a training loop and trained a Llama made from scratch using Data Distributed Parallel and torchrun.\r\n\r\n\r\n##  ModelArgs Hyperparameters\r\n\r\n| Parameter              | Value         | Description                                                                 |\r\n|------------------------|---------------|-----------------------------------------------------------------------------|\r\n| `block_size`           | 128           | The size of each block.                                                     |\r\n| `batch_size`           | 64            | The number of samples processed before the model is updated.                |\r\n| `embeddings_dims`      | 384           | The dimensionality of the embeddings.                                       |\r\n| `attn_dropout`         | 0.1           | Dropout rate for attention layers.                                          |\r\n| `no_of_heads`          | 6             | Number of attention heads (needs thorough calculation).                     |\r\n| `dropout`              | 0.1           | Dropout rate for the model.                                                 |\r\n| `max_lr`               | 1e-4          | Maximum learning rate.                                                      |\r\n| `no_of_decoder_layers` | 6             | Number of decoder layers (needs thorough calculation).                      |\r\n| `weight_decay_optim`   | 0.1           | Weight decay for the optimizer.                                             |\r\n| `beta_1`               | 0.9           | Exponential decay rate for the first moment estimates in the optimizer.     |\r\n| `beta_2`               | 0.95          | Exponential decay rate for the second moment estimates in the optimizer.    |\r\n| `clip`                 | 1.0           | Gradient clipping value.                                                    |\r\n| `device`               | 'cuda:0'      | The device to run the model on (e.g., 'cuda:0' for GPU).                    |\r\n| `no_kv_heads`          | 2             | Number of key-value heads.                                                 \r\n\r\n\r\n### Datasets\r\n\r\n**Tineshakespeare**: in the /data folder\r\n\r\n### Frameworks:\r\n**Pytorch**\r\n\r\n\r\n### Epochs/Steps\r\nIterations (train) = 8000\r\n\r\nVal iterations = every 100\r\n\r\n\r\n### Losses\r\nTrain loss - 1.5\r\n\r\nVal loss - 1.1\r\n\r\n\r\n### Local setup\r\n\r\n\r\n### Requirements\r\n\r\n```python\r\npip install torchtune\r\npip install torchao\r\npip install torchrun\r\npip install wandb\r\n\r\n```\r\n\r\n\r\nIf you want to use your dataset, please take a look at the dataset provided in data/.\r\nIf you have one, move your dataset to the data/ folder and then change the following line to point to your dataset in the data/ (currently only .txt is supported) in the llama_multi_gpu_train.py\r\nAlso please change 'device' to any of your available cuda gpus.\r\n\r\n```python\r\n'data/input.txt' -> 'data/{YPU_FILE_NAME_HERE}' line  66\r\n\r\n```\r\nTo run:\r\n\r\n```python\r\ntorchrun --standalone --nproc_per_node=gpu llama_multi_gpu_train.py\r\n```\r\n--standalone - if all the gpu are on one server\r\n--npro_per_node - number of gpus available and use the keyword gpu to use all\r\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/DDP",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/DDP?ref=master",
      "download_url": null,
      "created_date": "2025-04-25",
      "github_date": "2025-04-25"
    },
    {
      "name": "DPO",
      "display_name": "Dpo",
      "description": "From scratch implementation of DPO",
      "readme_content": "\n# The Direct Preference Optimization in Pytorch from scratch implementation\n\nI Trained Qwen0.5B-Instruct using Direct Preference Optimization in Pytorch\n\n## ModelArgs Hyperparameters\n\n| Parameter    | Value    | Description                                                                 |\n|--------------|----------|-----------------------------------------------------------------------------|\n| `batch_size` | 2        | The number of samples processed before the model is updated.                |\n| `max_lr`     | 1e-6     | Maximum learning rate.                                                     |\n| `device`     | 'cuda:0' | The device to run the model on (e.g., 'cuda:0' for GPU).   \n\n### Datasets\n\n[UltraFeedback](https://huggingface.co/datasets/trl-lib/ultrafeedback_binarized)\n\n### Frameworks:\n**Pytorch**\n\n\n### Epochs/Steps\nIterations (train) = 3000\n\nVal iterations = every 20\n\n\n### Losses\nTrain loss - 0.67\n\nVal loss - 0.68\n\n\n\n\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/DPO",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/DPO?ref=master",
      "download_url": null,
      "created_date": "2025-04-04",
      "github_date": "2025-04-04"
    },
    {
      "name": "DeepSeekV3",
      "display_name": "Deepseekv3",
      "description": "From scratch implementation of DeepSeekV3",
      "readme_content": "\r\n# DeepSeekV3 from Scratch in PyTorch\r\nThis repository contains a PyTorch implementation of the DeepSeekV3 architecture, trained on the TinyStories dataset. The model is designed for efficient text generation and understanding tasks, leveraging a mixture of experts (MoE) architecture.\r\n\r\n- So, I trained a  DeepSeekV3 (16x4) architecture I coded from ground up.\r\n- Trained on TiyStories dataset form HuggingFace consisting of 4.2B tokens for a few steps with gradient accumulation ammounting to 300M tokens.\r\n\r\n\r\n\r\n ###  Pretraining\r\n\r\n#### Dataset\r\n\r\n - I used the [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset from HuggingFace.\r\n\r\n  1) Train dataset - 2 M records approx\r\n  2) Val dataset - 26K records approx\r\n\r\n\r\n\r\n---\r\n\r\n####  ModelArgs (Hyperparameters)\r\n# Model Configuration (`ModelArgs`)\r\n\r\nThis dataclass defines hyperparameters and configuration settings for the DeepSeekV3 model, as defined in `train.py`.\r\n\r\n## Hyperparameters Overview\r\n\r\n### Architecture\r\n| Parameter | Value | Description |\r\n|-----------|-------|-------------|\r\n| `block_size` | 256 | Context window length for sequential data |\r\n| `embeddings_dims` | 512 | Dimension size for embeddings |\r\n| `no_of_heads` | 8 | Number of attention heads in multi-head attention |\r\n| `no_of_decoder_layers` | 8 | Number of transformer decoder layers |\r\n| `vocab_size` | len(tokenizer.get_vocab()) | Vocabulary size from tokenizer |\r\n| `base_freq` | 10000 | Base frequency for positional encodings |\r\n| `latent_dim` | 64 | Latent dimension for attention |\r\n\r\n### Training\r\n| Parameter | Value | Description |\r\n|-----------|-------|-------------|\r\n| `epochs` | 1 | Total training epochs |\r\n| `batch_size` | 32 | Samples per batch |\r\n| `max_lr` | 6e-4 | Maximum learning rate |\r\n| `clip` | 1.0 | Gradient clipping threshold |\r\n\r\n### Regularization\r\n| Parameter | Value | Description |\r\n|-----------|-------|-------------|\r\n| `attn_dropout` | 0.1 | Dropout probability for attention layers |\r\n| `dropout` | 0.1 | General dropout probability |\r\n\r\n### Optimization\r\n| Parameter | Value | Description |\r\n|-----------|-------|-------------|\r\n| `weight_decay_optim` | 0.1 | L2 regularization strength |\r\n| `beta_1` | 0.9 | AdamW first momentum factor |\r\n| `beta_2` | 0.95 | AdamW second momentum factor |\r\n| `eps` | 1e-8 | Epsilon for numerical stability |\r\n| `loss_scale` | 0.3 | Loss scaling factor |\r\n\r\n### Mixture-of-Experts (MoE)\r\n| Parameter | Value | Description |\r\n|-----------|-------|-------------|\r\n| `experts` | 16 | Total number of experts in MoE layer |\r\n| `top_experts` | 4 | Number of active experts per token |\r\n| `noisy_topk` | False | Enable noisy top-k expert selection |\r\n| `use_shared_expert` | True | Enable/disable shared expert |\r\n| `useauxFreeLoadBalancingLoss` | True | Use auxiliary-free load balancing loss |\r\n| `aux_free_bias_update_rate` | 0.001 | Update rate for auxiliary-free bias |\r\n| `mtp_heads` | 1 | Multi-token prediction heads |\r\n\r\n### Hardware & Optimization\r\n| Parameter | Value | Description |\r\n|-----------|-------|-------------|\r\n| `device` | 'cuda:8' | Training accelerator (GPU/CPU) |\r\n| `use_checkpointing` | False | Enable gradient checkpointing |\r\n| `use_liger` | False | Use Liger kernels for optimized operations |\r\n| `ignore_pad_token_in_loss` | True | Whether to ignore padding tokens in loss calculation |\r\n\r\n\r\n - Used P100 on Kaggle\r\n---\r\n\r\n#### Frameworks:\r\n**Pytorch**\r\n\r\n\r\n--- \r\n\r\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/DeepSeekV3",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/DeepSeekV3?ref=master",
      "download_url": null,
      "created_date": "2025-08-06",
      "github_date": "2025-08-06"
    },
    {
      "name": "Differential Transformer",
      "display_name": "Differential Transformer",
      "description": "From scratch implementation of Differential Transformer",
      "readme_content": "\n# Differential Transformers in Pytorch\n\nI implemented the Differential Transformers using Pytorch on Tinyshakespeare dataset.\n\n[Differential Transformers](https://arxiv.org/pdf/2410.05258)\n\n\n### Datasets\n\n**Tineshakespeare**: in the /data folder\n\n### Frameworks:\n**Pytorch**\n\n\n### Results (on A100 GPU Single)\n\n**Training steps:** 2000\n**Training steps:** per 100 training steps\n\n**Train loss:**  5.95\n**Val loss:** 5.98\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/Differential Transformer",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/Differential%20Transformer?ref=master",
      "download_url": null,
      "created_date": "2025-02-09",
      "github_date": "2025-02-09"
    },
    {
      "name": "Encoder-Decoder",
      "display_name": "Encoder Decoder",
      "description": "From scratch implementation of Encoder-Decoder",
      "readme_content": "\n# Coded an Encoder-Decoder in Pytorch from scratch  \n\nTrained on the on German (de) to English (en) dataset\n\n\n[Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215)\n\n## ModelArgs Hyperparameters\n\n| Parameter    | Value    | Description                                                                 \n|--------------|----------|-----------------------------------------------------------------------------|\n| `batch_size` | 32       | The number of samples processed before the model is updated.                |\n| `max_lr`     | 1e-4     | Maximum learning rate.                                                      |\n| `dropout`    | 0.2      | Dropout.                                                                    |\n| `epochs`     | 10       | Epochs                                                                      |           \n| `block_size` | 32      | Seq Len                                                                     |\n| `num_layers` | 4      | Layers for deep lstms                                                                |\n| `No of neurons`| 128      | No of neurons in an GRU per layer                                          |    \n\n\n### Frameworks:\n**Pytorch**\n\n\n### Epochs/Steps\nEpochs (train) = 10\n\nVal iterations = every epoch\n\n\n### Losses\n\nTrain loss - 1.38\n\nVal loss - 1.39\n\n### Loss Curves\n\n![Train and Val loss curves](img/loss.jpg)\n\n\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/Encoder-Decoder",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/Encoder-Decoder?ref=master",
      "download_url": null,
      "created_date": "2025-03-07",
      "github_date": "2025-03-07"
    },
    {
      "name": "Fine Tuning using PEFT",
      "display_name": "Fine Tuning Using Peft",
      "description": "From scratch implementation of Fine Tuning using PEFT",
      "readme_content": "\r\n# Fine Tuning using PEFT (QLoRA and BitsandBytes) in Pytorch\r\n\r\nI implemented a simple fine tuning script using peft (qlora and bnb) for fine tuning llms.\r\nAlso added a file for training encoder type models.\r\n\r\n\r\n### Frameworks:\r\n**Pytorch**\r\n\r\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/Fine Tuning using PEFT",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/Fine%20Tuning%20using%20PEFT?ref=master",
      "download_url": null,
      "created_date": "2025-03-01",
      "github_date": "2025-03-01"
    },
    {
      "name": "GPT",
      "display_name": "Gpt",
      "description": "From scratch implementation of GPT",
      "readme_content": "\n# GPT in Pytorch\n\nI implemented the GPT from scratch using Pytorch on Tinyshakespeare dataset.\n\n[Improving Language Understanding\nby Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n\n\n### Datasets\n\n**Tineshakespeare**: in the /data folder\n\n### Frameworks:\n**Pytorch**\n\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/GPT",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/GPT?ref=master",
      "download_url": null,
      "created_date": "2025-02-08",
      "github_date": "2025-02-08"
    },
    {
      "name": "GRU",
      "display_name": "Gru",
      "description": "From scratch implementation of GRU",
      "readme_content": "\n# GRU in Pytorch from scratch implementation\n\nTrained a GRU model coded from scratch in Pytorch \n\n## ModelArgs Hyperparameters\n\n| Parameter    | Value    | Description                                                                 \n|--------------|----------|-----------------------------------------------------------------------------|\n| `batch_size` | 16       | The number of samples processed before the model is updated.                |\n| `max_lr`     | 1e-4     | Maximum learning rate.                                                      |\n| `dropout`    | 0.2      | Dropout.                                                                    |\n| `epochs`     | 50       | Epochs                                                                      |           \n| `block_size` | 16      | Seq Len                                     |\n| `No of neurons`| 16      | No of neurons in an GRU per layer                                          |    \n\n\n### Frameworks:\n**Pytorch**\n\n\n### Epochs/Steps\nEpochs (train) = 50\n\nVal iterations = every epoch\n\n\n### Losses\n\nTrain loss - 0.51 \n\nVal loss - 0.48\n\n<!-- ### Loss Curves\n\n![Train and Val loss curves](img/loss_curves.jpg) -->\n\n\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/GRU",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/GRU?ref=master",
      "download_url": null,
      "created_date": "2025-03-05",
      "github_date": "2025-03-05"
    },
    {
      "name": "Gemma",
      "display_name": "Gemma",
      "description": "From scratch implementation of Gemma",
      "readme_content": "\r\n# Gemma architecture in Pytorch\r\n\r\nI implemented the Gemma using Pytorch on the tineshakespeare dataset.\r\n\r\n[Gemma: Open Models Based on Gemini Research and Technology](https://arxiv.org/pdf/2403.08295)\r\n\r\n\r\n### Datasets\r\n\r\n**tinyshakespeare**: [Link](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)\r\n\r\n### Frameworks:\r\n**Pytorch**\r\n\r\n\r\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/Gemma",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/Gemma?ref=master",
      "download_url": null,
      "created_date": "2025-04-20",
      "github_date": "2025-04-20"
    },
    {
      "name": "Gemma3",
      "display_name": "Gemma3",
      "description": "From scratch implementation of Gemma3",
      "readme_content": "\n\n# Gemma 3 model in Pytorch from scratch implementation\n\nTrained a small Gemma 3 model (90M) coded and trained from scratch in Pytorch (text only) \n\n\n[Gemma 3](https://arxiv.org/abs/2503.19786)\n\n## ModelArgs Hyperparameters\n\n\n| Parameter               | Value                                  | Description                                                                 |\n|-------------------------|----------------------------------------|-----------------------------------------------------------------------------|\n| `batch_size`            | 64                                     | Number of samples processed before model update                             |\n| `max_lr`                | 2.5e-4                                 | Maximum learning rate                                                       |\n| `dropout`               | 0.1                                    | Dropout rate for regularization                                            |                                               |\n| `block_size`            | 256                                    | Sequence length (number of tokens)                                         |\n| `vocab_size`        | 32000 + 768       |  vocabulary size                                                     |\n| `embeddings_dims`       | 512                                    | Token embedding dimensionality                                             |\n| `attn_dropout`          | 0.1                                    | Dropout rate for attention layers                                          |\n| `no_of_heads`           | 8                                      | Number of attention heads in multi-head attention                          |\n| `no_of_decoder_layers`  | 6                                      | Number of decoder layers                                                   |\n| `weight_decay_optim`    | 0.1                                    | Optimizer weight decay                                                     |\n| `beta_1`                | 0.9                                    | Adam optimizer beta1 parameter                                             |\n| `beta_2`                | 0.95                                   | Adam optimizer beta2 parameter                                             |\n| `no_kv_heads`           | 2                                      | Number of key/value heads                                                  |\n| `scaling_factor`        | 0.5                                    | Scaling factor for certain operations                                      |\n| `local_block_size`      | 128                                    | Local attention block size                                                 |\n| `base_freq`             | 10000                                  | Base frequency                                                  |\n\n\n### Dataset\n\n[TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories)\n\n\n### Frameworks:\n**Pytorch**\n\n\n### Epochs/Steps\nSteps (train) = 25000\n\nVal iterations = every 500 steps\n\n\n### Loss Curves\n\n![Train and Val loss curves](img/loss.png)\n\nTrain loss: 2.08 (last step)\n\nVal loss: 1.77 \n\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/Gemma3",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/Gemma3?ref=master",
      "download_url": null,
      "created_date": "2025-04-26",
      "github_date": "2025-04-26"
    },
    {
      "name": "Kimi-K2",
      "display_name": "Kimi K2",
      "description": "From scratch implementation of Kimi-K2",
      "readme_content": "# Kimi-K2 - DeepSeek V3 Inspired Model\n\nA PyTorch reimplementation of a DeepSeek V3-inspired transformer model with Mixture of Experts (MoE), Latent Attention, and other advanced features.\n\n![StoryKimi Model](images/image.png)\n\n## ðŸ“Š Training Results & Model Weights\n\n**ðŸ“ˆ View Training Report**: [StoryKimi Training Results on WandB](https://wandb.ai/rentio/DSV-Training/reports/SmolKimi-A-smaller-Kimi-K2---VmlldzoxMzYwNDQ4Mg?accessToken=lfs6n1y7gn8q0f0dwilta8yuwzxel45ztzbbcavwbqp7jsyv1p7cz9elflycv9fg)\n\n**ðŸ’¾ Download Pre-trained Weights**: \n- **Hugging Face Model**: [YuvrajSingh9886/StoryKimi](https://huggingface.co/YuvrajSingh9886/StoryKimi)\n- **WandB Checkpoints**: Check the WandB report above for additional trained model checkpoints\n\n## Features\n\n- **Latent Attention**: Efficient attention mechanism with compressed key-value representations\n- **Mixture of Experts (MoE)**: 8 experts with top-2 routing and shared expert support\n- **SWiGLU Activation**: Advanced activation function in expert layers\n- **Sinusoidal Positional Embeddings**: Position encoding for sequence understanding\n- **Liger Kernels**: Optimized kernels for faster training (optional)\n- **Distributed Training**: Support for multi-GPU training with DDP\n- **Advanced Optimizer**: Muon optimizer with auxiliary Adam for better convergence\n- **Gradio Interface**: Interactive web interface for text generation\n\n## Model Architecture\n\n### Default Configuration\n- **Embedding Dimensions**: 384\n- **Decoder Layers**: 6\n- **Attention Heads**: 8\n- **MoE Experts**: 8 (top-2 routing)\n- **Block Size**: 128 tokens\n- **Vocabulary Size**: Based on Llama-2-7b tokenizer (~32,000 tokens)\n- **Latent Dimension**: 64 (for compressed attention)\n\n### Full Parameter List\n\n#### Model Architecture Parameters\n- `--block_size`: Maximum sequence length (default: 128)\n- `--batch_size`: Training batch size (default: 256)\n- `--embeddings_dims`: Model embedding dimensions (default: 384)\n- `--no_of_heads`: Number of attention heads (default: 8)\n- `--no_of_decoder_layers`: Number of decoder layers (default: 6)\n- `--latent_dim`: Latent dimension for attention (default: 64)\n\n#### Mixture of Experts (MoE) Parameters\n- `--experts`: Number of MoE experts (default: 8)\n- `--top_experts`: Number of experts to route to (default: 2)\n- `--use_shared_expert`: Enable shared expert in MoE (default: True)\n- `--noisy_topk`: Use noisy top-k routing (default: False)\n- `--useauxFreeLoadBalancingLoss`: Use auxiliary-free load balancing loss (default: True)\n- `--aux_free_bias_update_rate`: Bias update rate for load balancing (default: 0.001)\n- `--loss_scale`: Loss scaling factor (default: 0.3)\n\n#### Training Hyperparameters\n- `--epochs`: Number of training epochs (default: 1)\n- `--max_lr`: Maximum learning rate (default: 6e-4)\n- `--weight_decay_optim`: Weight decay for optimizer (default: 0.1)\n- `--beta_1`: Beta1 for optimizer (default: 0.9)\n- `--beta_2`: Beta2 for optimizer (default: 0.95)\n- `--eps`: Epsilon for optimizer (default: 1e-8)\n- `--clip`: Gradient clipping value (default: 1.0)\n\n#### Regularization Parameters\n- `--dropout`: Dropout rate (default: 0.1)\n- `--attn_dropout`: Attention dropout rate (default: 0.1)\n\n#### System Configuration\n- `--device`: Device to use (default: 'cuda')\n- `--use_checkpointing`: Use gradient checkpointing (default: False)\n- `--use_liger`: Use Liger kernels for optimization (default: True)\n- `--ignore_pad_token_in_loss`: Ignore padding tokens in loss calculation (default: True)\n\n#### Data Configuration\n- `--vocab_size`: Vocabulary size (default: 32000, updated based on tokenizer)\n- `--base_freq`: Base frequency for positional encoding (default: 100000)\n- `--hf_token`: Hugging Face token for accessing gated models like Llama-2 (default: None)\n- `--dataset`: Dataset to use ('tinystories', 'fineweb', 'tinyshakespeare') (default: 'tinystories')\n\n#### Generation Parameters\n- `--generation_max_length`: Maximum length for text generation (default: 50)\n- `--generation_top_k`: Top-k value for sampling (default: 50)\n- `--generation_temperature`: Temperature for sampling (default: 1.0)\n\n#### Logging and Checkpointing\n- `--log_interval`: Steps between logging (default: 100)\n- `--save_interval`: Steps between saving checkpoints (default: 2000)\n- `--eval_interval`: Steps between evaluation (default: 400)\n- `--eval_iters`: Number of iterations for evaluation (default: 400)\n- `--warmup_iters`: Number of warmup iterations (default: 400)\n- `--total_iters`: Total training iterations (default: 10000)\n- `--lr_decay_iters`: Learning rate decay iterations (default: 10000)\n- `--wandb_project`: Wandb project name (default: 'storykimi')\n- `--wandb_run_name`: Wandb run name (default: None)\n\n#### Batch Size Configuration\n- `--total_batch_size`: Total batch size for gradient accumulation (default: 524288)\n- `--micro_batch_size`: Micro batch size (default: batch_size)\n\n#### Distributed Training\n- `--use_ddp`: Use distributed data parallel (default: False)\n\n## Quick Start\n\n### Installation\n\n```bash\nchmod +x install.sh\n./install.sh\n```\n\n### Using Pre-trained Weights\n\n1. **Download Model Weights**: \n   - **Option 1**: Download from [Hugging Face - YuvrajSingh9886/StoryKimi](https://huggingface.co/YuvrajSingh9886/StoryKimi)\n   - **Option 2**: Visit the [WandB Training Report](https://wandb.ai/rentio/DSV-Training/reports/SmolKimi-A-smaller-Kimi-K2---VmlldzoxMzYwNDQ4Mg?accessToken=lfs6n1y7gn8q0f0dwilta8yuwzxel45ztzbbcavwbqp7jsyv1p7cz9elflycv9fg) for additional checkpoints\n   - Place downloaded files in the `checkpoints/` directory\n\n2. **Load Pre-trained Model for Inference**:\n   ```bash\n   # Using the Gradio web interface\n   python gradio/app.py --hf_token \"your_token_here\"\n   \n   # Or use in your own code\n   python inference.py --checkpoint_path checkpoints/your_checkpoint.pt\n   \n   # Using Hugging Face transformers (if available)\n   from transformers import AutoModel, AutoTokenizer\n   model = AutoModel.from_pretrained(\"YuvrajSingh9886/StoryKimi\")\n   ```\n\n### Important: Hugging Face Token Setup\n\nSince this model uses the Llama-2 tokenizer, you'll need a Hugging Face token to access the gated model. \n\n1. **Get a Hugging Face Token:**\n   - Go to [Hugging Face Settings](https://huggingface.co/settings/tokens)\n   - Create a new token with \"Read\" permissions\n   - Accept the Llama-2 license at [meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)\n\n2. **Set your token in one of these ways:**\n   ```bash\n   # Option 1: Environment variable (recommended)\n   export HF_TOKEN=\"your_token_here\"\n   \n   # Option 2: Pass as command line argument\n   python trainer.py --hf_token \"your_token_here\"\n   ```\n\n### Training Examples\n\n#### Basic Training (Single GPU)\n```bash\n# With environment variable\nexport HF_TOKEN=\"your_token_here\"\npython trainer.py\n\n# With command line argument\npython trainer.py --hf_token \"your_token_here\"\n```\n\n#### Training with Custom Parameters\n```bash\n# Train with larger model\npython trainer.py --hf_token \"your_token_here\" --embeddings_dims 512 --no_of_heads 16 --no_of_decoder_layers 8\n\n# Train with different dataset\npython trainer.py --hf_token \"your_token_here\" --dataset fineweb --epochs 3\n\n# Train with custom learning rate and batch size\npython trainer.py --hf_token \"your_token_here\" --max_lr 1e-3 --batch_size 128 --block_size 256\n\n# Train with more experts\npython trainer.py --hf_token \"your_token_here\" --experts 16 --top_experts 4\n\n# Train without shared expert\npython trainer.py --hf_token \"your_token_here\" --use_shared_expert False\n\n# Train with noisy top-k routing\npython trainer.py --hf_token \"your_token_here\" --noisy_topk True\n```\n\n#### Multi-GPU Distributed Training\n```bash\n# Set token as environment variable for distributed training\nexport HF_TOKEN=\"your_token_here\"\n\n# 2 GPUs\ntorchrun --nproc_per_node=2 trainer.py\n\n# 4 GPUs with custom parameters\ntorchrun --nproc_per_node=4 trainer.py --batch_size 128 --embeddings_dims 512\n\n# 8 GPUs with large model configuration\ntorchrun --nproc_per_node=8 trainer.py \\\n    --embeddings_dims 768 \\\n    --no_of_heads 12 \\\n    --no_of_decoder_layers 12 \\\n    --experts 16 \\\n    --top_experts 4 \\\n    --batch_size 64 \\\n    --block_size 512\n```\n\n#### Advanced Training Configurations\n\n##### High-Performance Setup\n```bash\nexport HF_TOKEN=\"your_token_here\"\npython trainer.py \\\n    --embeddings_dims 768 \\\n    --no_of_heads 12 \\\n    --no_of_decoder_layers 12 \\\n    --experts 16 \\\n    --top_experts 4 \\\n    --batch_size 32 \\\n    --block_size 512 \\\n    --max_lr 3e-4 \\\n    --epochs 5 \\\n    --use_liger True \\\n    --wandb_project \"storykimi-large\"\n```\n\n##### Experimental Setup\n```bash\nexport HF_TOKEN=\"your_token_here\"\npython trainer.py \\\n    --noisy_topk True \\\n    --use_shared_expert False \\\n    --aux_free_bias_update_rate 0.01 \\\n    --loss_scale 0.5 \\\n    --dropout 0.2 \\\n    --attn_dropout 0.15 \\\n    --wandb_project \"storykimi-experimental\"\n```\n\n##### Memory-Efficient Setup\n```bash\nexport HF_TOKEN=\"your_token_here\"\npython trainer.py \\\n    --use_checkpointing True \\\n    --batch_size 64 \\\n    --micro_batch_size 16 \\\n    --total_batch_size 262144 \\\n    --block_size 128\n```\n\n### Inference with Gradio\n\n```bash\n# Set your HF token\nexport HF_TOKEN=\"your_token_here\"\n\n# Run the Gradio app\ncd gradio\npython app.py --hf_token \"your_token_here\"\n\n# Or with environment variable\ncd gradio\npython app.py\n\n# With custom port and public sharing\ncd gradio\npython app.py --hf_token \"your_token_here\" --port 8080 --share\n```\n\n### Help and Parameter Information\n\n```bash\n# View all available parameters\npython trainer.py --help\n\n# View Gradio app parameters\ncd gradio\npython app.py --help\n```\n\n### Environment Variables\n\nYou can set the following environment variables instead of passing them as arguments:\n\n```bash\n# Hugging Face token (recommended approach)\nexport HF_TOKEN=\"your_token_here\"\n\n# Wandb API key (optional, for experiment tracking)\nexport WANDB_API_KEY=\"your_wandb_key_here\"\n```\n\n## File Structure\n\n```\nStoryKimi/\nâ”œâ”€â”€ config.py          # Model configuration and hyperparameters with argparse\nâ”œâ”€â”€ model.py           # Model architecture (DeepSeekV3, MoE, Attention, etc.)\nâ”œâ”€â”€ tokenizer.py       # Tokenizer setup\nâ”œâ”€â”€ data.py           # Data loading and preparation\nâ”œâ”€â”€ inference.py      # Inference functions and text generation\nâ”œâ”€â”€ trainer.py        # Main training loop with DDP support\nâ”œâ”€â”€ install.sh        # Setup script\nâ”œâ”€â”€ requirements.txt  # Python dependencies\nâ”œâ”€â”€ gradio/\nâ”‚   â”œâ”€â”€ app.py        # Gradio web interface\nâ”‚   â””â”€â”€ requirements.txt\nâ””â”€â”€ generated_data/   # Generated text outputs\n```\n\n## Training Features\n\n- **Gradient Accumulation**: Configurable batch size scaling\n- **Learning Rate Scheduling**: Cosine decay with warmup\n- **Gradient Clipping**: Prevents gradient explosion\n- **Wandb Integration**: Experiment tracking and logging\n- **Checkpointing**: Regular model checkpoints during training\n- **Loss Calculation**: Optimized cross-entropy with padding token handling\n- **Distributed Training**: Multi-GPU support with DDP\n- **Memory Optimization**: Gradient checkpointing support\n\n## Generation Methods\n\n1. **Top-k Sampling**: Traditional sampling with temperature control\n2. **Beam Search**: Deterministic search for high-quality outputs\n\n## Advanced Usage\n\n### Configuration Files\nAll parameters can be set via command line arguments. For complex configurations, consider creating shell scripts:\n\n```bash\n#!/bin/bash\n# large_model_config.sh\npython trainer.py \\\n    --embeddings_dims 1024 \\\n    --no_of_heads 16 \\\n    --no_of_decoder_layers 24 \\\n    --experts 32 \\\n    --top_experts 8 \\\n    --batch_size 16 \\\n    --block_size 1024 \\\n    --max_lr 1e-4 \\\n    --epochs 10 \\\n    --use_liger True \\\n    --use_checkpointing True \\\n    --wandb_project \"storykimi-large-scale\"\n```\n\n### Custom Dataset Training\n```bash\n# TinyStories (default)\npython trainer.py --dataset tinystories\n\n# FineWeb (large scale)\npython trainer.py --dataset fineweb --epochs 3 --batch_size 64\n\n# TinyShakespeare (character level)\npython trainer.py --dataset tinyshakespeare --block_size 256\n```\n\n### Monitoring and Logging\n```bash\n# Custom wandb configuration\npython trainer.py \\\n    --wandb_project \"my-experiment\" \\\n    --wandb_run_name \"test-run-1\" \\\n    --log_interval 50 \\\n    --eval_interval 200 \\\n    --save_interval 1000\n```\n\n### Hardware-Specific Optimizations\n\n#### For High-Memory GPUs (A100, H100)\n```bash\npython trainer.py \\\n    --batch_size 512 \\\n    --block_size 2048 \\\n    --embeddings_dims 1024 \\\n    --total_batch_size 1048576\n```\n\n#### For Low-Memory GPUs (RTX 3080, 4080)\n```bash\npython trainer.py \\\n    --batch_size 32 \\\n    --micro_batch_size 8 \\\n    --block_size 128 \\\n    --use_checkpointing True \\\n    --embeddings_dims 256\n```\n\n### Usage Examples\n\n#### Basic Training\n```python\nfrom trainer import train\ntrain()\n```\n\n#### Text Generation\n```python\nfrom inference import topk_sampling\nfrom model import DeepSeekV3\nfrom config import ModelArgs, get_args\n\n# Load with custom config\nargs = get_args()\nmodel_args = ModelArgs(args)\nmodel = DeepSeekV3(device='cuda')\ntext = topk_sampling(model, \"Once upon a time\", device='cuda')\n```\n\n#### Loading a Trained Model\n```python\nimport torch\nfrom model import DeepSeekV3\nfrom config import ModelArgs, get_args\n\n# Load saved model\nargs = get_args()\nmodel_args = ModelArgs(args)\nmodel = DeepSeekV3(device='cuda')\nmodel.load_state_dict(torch.load('path/to/checkpoint.pt'))\nmodel.eval()\n```\n\n## Performance Tips\n\n1. **Use Mixed Precision**: Enable automatic mixed precision for faster training\n2. **Gradient Checkpointing**: Use `--use_checkpointing True` for memory-constrained setups\n3. **Liger Kernels**: Keep `--use_liger True` for optimized operations\n4. **Batch Size Tuning**: Start with smaller batch sizes and increase gradually\n5. **Block Size**: Larger block sizes improve quality but require more memory\n\n## Troubleshooting\n\n### Common Issues\n\n#### Authentication Error (401)\n```bash\n# Make sure you have accepted the Llama-2 license and have a valid token\n# Visit: https://huggingface.co/meta-llama/Llama-2-7b-hf\n# Then set your token:\nexport HF_TOKEN=\"your_token_here\"\n```\n\n#### Out of Memory (OOM)\n```bash\n# Reduce batch size and enable checkpointing\npython trainer.py --hf_token \"your_token_here\" --batch_size 16 --use_checkpointing True\n\n# Use gradient accumulation\npython trainer.py --hf_token \"your_token_here\" --batch_size 32 --micro_batch_size 8\n```\n\n#### Slow Training\n```bash\n# Enable Liger kernels and increase batch size\npython trainer.py --hf_token \"your_token_here\" --use_liger True --batch_size 256\n\n# Use multiple GPUs\nexport HF_TOKEN=\"your_token_here\"\ntorchrun --nproc_per_node=4 trainer.py\n```\n\n#### NaN Loss\n```bash\n# Reduce learning rate and enable gradient clipping\npython trainer.py --hf_token \"your_token_here\" --max_lr 1e-4 --clip 0.5\n```\n\n## Contributing\n\nFeel free to contribute improvements, bug fixes, or new features!\n\n## Requirements\n\n- Python 3.8+\n- PyTorch 2.0+\n- Transformers\n- Datasets\n- Gradio\n- Wandb\n- Liger-kernel (optional)\n- Muon optimizer\n\n## License\n\nMIT License\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/Kimi-K2",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/Kimi-K2?ref=master",
      "download_url": null,
      "created_date": "2025-08-06",
      "github_date": "2025-08-06"
    },
    {
      "name": "Llama",
      "display_name": "Llama",
      "description": "From scratch implementation of Llama",
      "readme_content": "\n# Llama architecture in Pytorch\n\nI implemented Llama using Pytorch on the tineshakespeare dataset.\n\n[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971).\n\n\n### Datasets\n\n**tinyshakespeare**: [Link](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)\n\n### Frameworks:\n**Pytorch**\n\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/Llama",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/Llama?ref=master",
      "download_url": null,
      "created_date": "2025-04-20",
      "github_date": "2025-04-20"
    },
    {
      "name": "Llama4",
      "display_name": "Llama4",
      "description": "From scratch implementation of Llama4",
      "readme_content": "\n# Llama 4 Scout from-scratch in PyTorch\n- So, I trained a MoE based Llama 1.2B (32x12M) architecture I coded from ground up.\n- Trained on TiyStories dataset form HuggingFace consisting of 4.2B tokens for 1 FULL epoch.\n\n\n---\n\n### Pretraining\n\n#### Dataset\n\n - I used the [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset from HuggingFace.\n\n  1) Train dataset - 2 M records approx\n  2) Val dataset - 26K records approx\n\n\n\n---\n\n\n# Model Configuration (`ModelArgs`)\n\nThis dataclass defines hyperparameters and configuration settings for a neural network model, optimized for modern deep learning tasks.\n\n## Hyperparameters Overview\n\n### Architecture\n| Parameter | Value | Description |\n|-----------|-------|-------------|\n| `block_size` | 1024 | Context window length for sequential data |\n| `embeddings_dims` | 768 | Dimension size for embeddings |\n| `no_of_heads` | 8 | Number of attention heads in multi-head attention |\n| `no_of_decoder_layers` | 8 | Number of transformer decoder layers |\n| `vocab_size` | 32000 | Vocabulary size from tokenizer |\n| `base_freq` | 10000 | Base frequency for positional encodings |\n\n### Training\n| Parameter | Value | Description |\n|-----------|-------|-------------|\n| `epochs` | 1 | Total training epochs |\n| `batch_size` | 16 | Samples per batch |\n| `max_lr` | 6e-4 | Maximum learning rate |\n| `clip` | 1.0 | Gradient clipping threshold |\n\n### Regularization\n| Parameter | Value | Description |\n|-----------|-------|-------------|\n| `attn_dropout` | 0.1 | Dropout probability for attention layers |\n| `dropout` | 0.1 | General dropout probability |\n\n### Optimization\n| Parameter | Value | Description |\n|-----------|-------|-------------|\n| `weight_decay_optim` | 0.1 | L2 regularization strength |\n| `beta_1` | 0.9 | AdamW first momentum factor |\n| `beta_2` | 0.95 | AdamW second momentum factor |\n| `eps` | 1e-8 | Epsilon for numerical stability |\n\n### Mixture-of-Experts (MoE)\n| Parameter | Value | Description |\n|-----------|-------|-------------|\n| `experts` | 31 | Total number of experts in MoE layer |\n| `top_experts` | 1 | Number of active experts per token |\n| `noisy_topk` | False | Enable noisy top-k expert selection |\n| `use_shared_expert` | True | Enable/disable shared expert |\n| `useauxFreeLoadBalancingLoss` | True | Use auxiliary-free load balancing loss |\n| `aux_free_bias_update_rate` | 0.001 | Update rate for auxiliary-free bias |\n\n### Hardware & Optimization\n| Parameter | Value | Description |\n|-----------|-------|-------------|\n| `device` | 'cuda:4' | Training accelerator (GPU/CPU) |\n| `use_checkpointing` | False | Enable gradient checkpointing |\n| `use_liger` | True | Use Liger kernels for optimized operations |\n| `ignore_pad_token_in_loss` | True | Whether to ignore padding tokens in loss calculation |\n\n\n - Used P100 on Kaggle\n---\n\n#### Frameworks:\n**Pytorch**\n\n\n--- \n\n#### Epochs/Steps\n- Iterations (train) = 20k \n\n- Val iterations = every 400 steps\n---\n#### Losses\n- Train loss - 2.08\n\n- Val loss - 1.7\n\n---\n\n#### Screenshots of the loss curves\n\n- Loss Curves (Train and Val)\n\n![Loss Curves (Train and Val)](img/loss.png)\n\n--- \n#### Output\n\n```python\n/data/generations.txt\n```\n\n---\n\n<!-- ### Local setup\n\n\n### Requirements\n\n\n\n```python\ngit [clone the repo](https://github.com/YuvrajSingh-mist/StoryLlama.git)\ncd StoryLlama\nbash ./install.sh\n\n```\n- A wandb.ai account for plotting graphs for your loss curves\n\n- On your terminal run\n```python\nwandb login\n```\n\n- Enter the api key and follow the instructions and once you are succesfully logged in follow the given steps\n\n\n- Download the model\n\n```python\ncd gradio/\n\npython app.py\n```\n\n\n---\n\n### Running \n\n\n#### Training a model\n\n- Kindly change 'device' to any of your available cuda gpus.\n\nTo run:\n\n```python\nbash ./install.sh\n```\n\n```python\ntorchrun --standalone --nproc_per_node=gpu trainer.py \\\n    --epochs 10 \\\n    --block_size 256 \\\n    --batch_size 128 \\\n    --embeddings_dims 768 \\\n    --attn_dropout 0.2 \\\n    --no_of_heads 12 \\\n    --dropout 0.2 \\\n    --val_epochs 3 \\\n    --max_lr 5e-4 \\\n    --no_of_decoder_layers 6 \\\n    --weight_decay_optim 0.01 \\\n    --beta_1 0.85 \\\n    --beta_2 0.99 \\\n    --clip 0.5 \\\n    --device \"cuda\" \\\n    --no_kv_heads 4 \\\n    --vocab_size 50257 \\\n    --eps 1e-6 \\\n    --dtype \"float16\" \\\n    --save_checkpoint_dir \"model_checkpoints\" \\\n    --prompt \"Once upon a time\" \\\n    --save_checkpoint_iter 100 \\\n    --total_iters 5000 \\\n    --eval_iters 200 \\\n    --eval_check 500 \\\n    --warmup_iters 1000 \\\n    --min_lr 1e-5 \\\n    --lr_decay_iters 2000 \\\n    --total_batch_size 262144 \\\n    --micro_batch_size 128 \\\n    --gradient_accumulation_steps 4\n\n```\n--standalone - if all the gpu are on one server\n--npro_per_node - number of gpus available and use the keyword gpu to use all\n\n#### Inference on a model\n\n```python \npython inference.py --prompt \"Once upon a time\" --max_length 100 --temperature 0.8 --topk 50 \n```\n -->\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/Llama4",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/Llama4?ref=master",
      "download_url": null,
      "created_date": "2025-08-06",
      "github_date": "2025-08-06"
    },
    {
      "name": "Llava",
      "display_name": "Llava",
      "description": "From scratch implementation of Llava",
      "readme_content": "\n# Llava architecture in Pytorch\n\nI implemented the Llava using Pytorch on the flickr8000 dataset.\n\n[Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)\n\n\n### Datasets\n\n**flickr 8000**: [Link](https://www.kaggle.com/datasets/adityajn105/flickr8k)\n\n### Frameworks:\n**Pytorch**\n\n\n### Results (on T4 GPU Single)\n\n**Training epochs:** 5\n\n**Train loss:** 0.23\n**Val loss:** 0.22 \n\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/Llava",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/Llava?ref=master",
      "download_url": null,
      "created_date": "2025-04-25",
      "github_date": "2025-04-25"
    },
    {
      "name": "LoRA",
      "display_name": "Lora",
      "description": "From scratch implementation of LoRA",
      "readme_content": "\n# LoRA in Pytorch\n\nI implemented the LoRA framework using Pytorch on Tinyshakespeare dataset.\n\n[LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685)\n\n\n### Datasets\n\n**Tineshakespeare**: in the /data folder\n\n### Frameworks:\n**Pytorch**\n\n\n### Results (on A100 GPU Single)\n\n**Training steps:** 1000\n**Validation steps:** per 100 training steps\n\n**Train loss:**  3.51\n**Val loss:** 3.50\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/LoRA",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/LoRA?ref=master",
      "download_url": null,
      "created_date": "2025-04-05",
      "github_date": "2025-04-05"
    },
    {
      "name": "Mixtral",
      "display_name": "Mixtral",
      "description": "From scratch implementation of Mixtral",
      "readme_content": "\r\n# Mixtral in Pytorch\r\n\r\nI implemented the Mixtral architecture from scratch using Pytorch on Tinyshakespeare dataset.\r\n\r\n[Mixtral of Experts](https://arxiv.org/pdf/2401.04088)\r\n\r\n\r\n### Datasets\r\n\r\n**Tineshakespeare**: in the /data folder\r\n\r\n### Frameworks:\r\n**Pytorch**\r\n\r\n\r\n### Results (on T4 GPU Single)\r\n\r\n**Training steps:** 1000\r\n**IValidation steps:** per 50 training steps\r\n\r\n**Train loss:** 2.0422 \r\n**Val loss:** 2.0898\r\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/Mixtral",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/Mixtral?ref=master",
      "download_url": null,
      "created_date": "2025-03-20",
      "github_date": "2025-03-20"
    },
    {
      "name": "Moonshine",
      "display_name": "Moonshine",
      "description": "From scratch implementation of Moonshine",
      "readme_content": "\n\n# Moonshine- A faster Alternative to Whisper (Replication)!\n\nTrained a small transformer-based ASR model coded and trained from scratch in Pytorch.\n\n[Moonshine: Speech Recognition for Live Transcription and Voice Commands](https://arxiv.org/pdf/2410.15608)\n\n\n### Hyperparameters\n| Parameter                | Value      | Description                                                                 |\n|--------------------------|------------|-----------------------------------------------------------------------------|\n| `epochs`                 | 10         | Total training epochs.                                                      |\n| `batch_size`             | 128        | Samples per batch.                                                          |\n| `block_size`             | 40         | Context window length for attention.                                        |\n| `embeddings_dims`        | 288        | Embedding dimension (must be divisible by `no_of_heads`).                   |\n| `no_of_heads`            | 6          | Attention heads in multi-head attention.                                    |\n| `no_of_decoder_layers`   | 6          | Transformer decoder layers.                                                 |\n| `dropout`                | 0.1        | Dropout rate for regularization.                                            |\n| `max_lr`                 | 6e-4       | Peak learning rate (use with learning rate scheduler).                      |\n| `weight_decay_optim`     | 0.1        | Weight decay for AdamW (consider reducing to `0.01` if unstable).           |\n| `sr`                     | 16000      | Audio sampling rate (fix conflict with `SAMPLING_RATE=480000` if needed).   |\n\n---\n\n### Dataset\n\n[Gigaspeech](https://huggingface.co/datasets/speechcolab/gigaspeech) \n\n\n### Frameworks:\n**Pytorch**\n\n\n### Epochs/Steps\nSteps (train) = 1500\n\nVal iterations = every 50 steps\n\n\n### Loss Curves\n\n![Train and Val loss curves](images/loss_curves.jpg)\n\nLooks like 25 hours isnt enough thus started to overfit!\n\n\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/Moonshine",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/Moonshine?ref=master",
      "download_url": null,
      "created_date": "2025-03-29",
      "github_date": "2025-03-29"
    },
    {
      "name": "ORPO",
      "display_name": "Orpo",
      "description": "From scratch implementation of ORPO",
      "readme_content": "\n# ORPO in Pytorch from scratch implementation\n\nTrained OPT-330M model using ORPO in Pytorch for Instruction Following\n\n## ModelArgs Hyperparameters\n\n| Parameter    | Value    | Description                                                                 \n|--------------|----------|-----------------------------------------------------------------------------|\n| `batch_size` | 2        | The number of samples processed before the model is updated.                |\n| `max_lr`     | 8e-6     | Maximum learning rate.                                                      |\n| `device`     | 'cuda:0' | The device to run the model on (e.g., 'cuda:0' for GPU).                    |\n| `betas`      | 0.95,0.99| Beta values                                                                 |           \n| `weight_decay`| 0.1     | Weight decay values for the optimizer                                       |\n\n\n### Datasets\n\n[UltraFeedback](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned)\n\n### Frameworks:\n**Pytorch**\n\n\n### Epochs/Steps\nIterations (train) = 3k\n\nVal iterations = every 20\n\n\n### Losses\n\nTrain loss - 1.70 \n\nVal loss - 1.98\n(at 2.5k steps)\n\n### Loss Curves\n\n![Train and Val loss curves](img/curves.jpg)\n\n\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/ORPO",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/ORPO?ref=master",
      "download_url": null,
      "created_date": "2025-03-01",
      "github_date": "2025-03-01"
    },
    {
      "name": "PaliGemma",
      "display_name": "Paligemma",
      "description": "From scratch implementation of PaliGemma",
      "readme_content": "\n# Paligemma architecture in Pytorch\n\nI implemented the Paligemma using Pytorch on the flickr8000 dataset.\n\n[PaliGemma: A versatile 3B VLM for transfer](https://arxiv.org/abs/2407.07726)\n\n\n### Datasets\n\n**flickr 8000**: [Link](https://www.kaggle.com/datasets/adityajn105/flickr8k)\n\n### Frameworks:\n**Pytorch**\n\n\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/PaliGemma",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/PaliGemma?ref=master",
      "download_url": null,
      "created_date": "2025-02-09",
      "github_date": "2025-02-09"
    },
    {
      "name": "Pix2Pix",
      "display_name": "Pix2Pix",
      "description": "From scratch implementation of Pix2Pix",
      "readme_content": "\n# Pix2Pix architecture in Pytorch\n\nI implemented the Pix2Pix using Pytorch on the cityscapes dataset.\n\n[Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004)\n\n\n### Datasets\n\n**Aerial2Map**: [Link](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)\n\n### Frameworks:\n**Pytorch**\n\n\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/Pix2Pix",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/Pix2Pix?ref=master",
      "download_url": null,
      "created_date": "2025-02-09",
      "github_date": "2025-02-09"
    },
    {
      "name": "RNNs",
      "display_name": "Rnns",
      "description": "From scratch implementation of RNNs",
      "readme_content": "\n# RNNs in Pytorch from scratch implementation\n\nTrained a RNN model coded from scratch in Pytorch \n\n## ModelArgs Hyperparameters\n\n| Parameter    | Value    | Description                                                                 \n|--------------|----------|-----------------------------------------------------------------------------|\n| `batch_size` | 16       | The number of samples processed before the model is updated.                |\n| `max_lr`     | 1e-4     | Maximum learning rate.                                                      |\n| `dropout`    | 0.2      | Dropout.                                                                    |\n| `epochs`     | 50       | Epochs                                                                      |           \n| `block_size` | 16      | Sequence Length                                       |\n| `No of neurons`| 16      | No of neurons in an RNN per layer                                          |    \n\n\n### Frameworks:\n**Pytorch**\n\n\n### Epochs/Steps\nEpochs (train) = 50\n\nVal iterations = every epoch\n\n\n### Losses\n\nTrain loss - 0.51 \n\nVal loss - 0.50\n\n### Loss Curves\n\n![Train and Val loss curves](img/loss_curves.jpg) \n\n\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/RNNs",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/RNNs?ref=master",
      "download_url": null,
      "created_date": "2025-03-07",
      "github_date": "2025-03-07"
    },
    {
      "name": "Seq2Seq",
      "display_name": "Seq2Seq",
      "description": "From scratch implementation of Seq2Seq",
      "readme_content": "\n\n# Seq2Seq with Bahdanau and Luong Attention in Pytorch from scratch implementation\n\nTrained a Seq2Seq model with the said attention mechanism  coded from scratch in Pytorch \n\n[Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)\n\n[Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.0473)\n\n## ModelArgs Hyperparameters\n\n| Parameter    | Value    | Description                                                                 \n|--------------|----------|-----------------------------------------------------------------------------|\n| `batch_size` | 32       | The number of samples processed before the model is updated.                |\n| `max_lr`     | 1e-4     | Maximum learning rate.                                                      |\n| `dropout`    | 0.1      | Dropout.                                                                    |\n| `epochs`     | 50       | Epochs                                                                      |           \n| `block_size` | 32      | Seq Len                                                                      |\n| `No of neurons`| 128      | No of neurons in an GRU per layer                                         |    \n| `hidden_dim`| 4*embedding_dims      | No of neurons in FFN                                            |  \n| `No of neurons`| 128      | No of neurons in an GRU per layer                                         |  \n\n### Frameworks:\n**Pytorch**\n\n\n### Epochs/Steps\nEpochs (train) = 50\n\nVal iterations = every epoch\n\n\n### Loss Curves\n\n![Train and Val loss curves](img/loss_curves.jpg)\n\n\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/Seq2Seq",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/Seq2Seq?ref=master",
      "download_url": null,
      "created_date": "2025-04-25",
      "github_date": "2025-04-25"
    },
    {
      "name": "SigLip",
      "display_name": "Siglip",
      "description": "From scratch implementation of SigLip",
      "readme_content": "\n# SigLIP architecture in Pytorch\n\nI implemented the SigLIP using Pytorch on the flickr8000 dataset.\n\n[Sigmoid Loss for Language Image Pre-Training](https://arxiv.org/abs/2303.15343)\n\n\n### Datasets\n\n**flickr 8000**: [Link](https://www.kaggle.com/datasets/adityajn105/flickr8k)\n\n### Frameworks:\n**Pytorch**\n\n\n\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/SigLip",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/SigLip?ref=master",
      "download_url": null,
      "created_date": "2025-02-09",
      "github_date": "2025-02-09"
    },
    {
      "name": "SimplePO",
      "display_name": "Simplepo",
      "description": "From scratch implementation of SimplePO",
      "readme_content": "\n# SimplePO in Pytorch from scratch implementation\n\nTrained OPT-330M model using SimplePO in Pytorch for Instruction Following\n\n[SimplePO: Simple Preference Optimization with a Reference-Free Reward](https://arxiv.org/abs/2405.14734)\n\n## ModelArgs Hyperparameters\n\n| Parameter    | Value    | Description                                                                 \n|--------------|----------|-----------------------------------------------------------------------------|\n| `batch_size` | 128        | The number of samples processed before the model is updated.                |\n| `max_lr`     | 2e-5     | Maximum learning rate.                                                      |\n| `device`     | 'cuda:0' | The device to run the model on (e.g., 'cuda:0' for GPU).                    |\n| `beta`      | 2 | Beta values                                                                 |           \n| `gamma`| 1.6     | Gamma values for the optimizer                                       |\n\n\n### Datasets\n\n[UltraFeedback](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned)\n\n### Frameworks:\n**Pytorch**\n\n\n\n\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/SimplePO",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/SimplePO?ref=master",
      "download_url": null,
      "created_date": "2025-04-04",
      "github_date": "2025-04-04"
    },
    {
      "name": "TTS",
      "display_name": "Tts",
      "description": "From scratch implementation of TTS",
      "readme_content": "\n\n# Transformer based TTS model in Pytorch from scratch implementation\n\nTrained a small transformer based TTS model coded and trained from scratch in Pytorch \n\n(will be uploading the implementation of Wavenet soon)\n\n[Neural Speech Synthesis with Transformer Network](https://arxiv.org/pdf/1809.08895)\n\n## Model Hyperparameters\n\n### Core Architecture\n| Parameter                      | Value            | Description                                  |\n|--------------------------------|------------------|----------------------------------------------|\n| `batch_size`                   | 32               | Number of samples per batch                 |\n| `max_lr`                       | 6e-4             | Maximum learning rate                       |\n| `dropout`                      | 0.1              | General dropout rate                        |\n| `epochs`                       | 10               | Total training epochs                       |\n| `block_size`                   | 80               | Sequence length in tokens                   |\n| `src_vocab_size`               | dynamic          | Source vocabulary size                      |\n| `phenome_embeddings_dims`      | 512              | Phoneme embedding dimension                 |\n| `embeddings_dims`              | 512              | Main embedding dimension                    |\n| `prenet_encoder_embeddings_dims` | 512            | Encoder prenet dimension                    |\n| `embeddings_dims_decoder`      | 256              | Decoder-specific embedding dimension        |\n| `attn_dropout`                 | 0.1              | Attention dropout rate                      |\n| `no_of_heads`                  | 4                | Attention heads per layer                   |\n| `no_of_decoder_layers`         | 8                | Number of decoder layers                    |\n| `weight_decay_optim`           | 0.01             | Optimizer weight decay                      |\n| `hidden_dim`                   | 2048 (4Ã—512)     | FFN hidden dimension                        |\n| `clip`                         | 1.0              | Gradient clipping threshold                 |\n\n### Audio Processing\n| Parameter               | Value    | Description                                  |\n|-------------------------|----------|----------------------------------------------|\n| `log_mel_features`      | 80       | Mel spectrogram channels                    |\n| `kernel_size`           | 5        | Convolution kernel size                     |\n| `stride`                | (2,10)   | Convolution stride (time, freq)             |\n| `sr`, `SAMPLING_RATE`   | 16000    | Audio sample rate (Hz)                      |\n| `N_MELS`                | 80       | Number of Mel bands                         |\n| `WINDOW_DURATION`       | 0.050s   | Analysis window duration                    |\n| `STRIDE_DURATION`       | 0.0125s  | Window stride duration                      |\n| `max_t`                 | 512      | Maximum spectrogram time steps              |\n| `n_channels`            | 80       | Input spectrogram channels                  |\n### Dataset\n\n[Gigaspeech](https://huggingface.co/datasets/speechcolab/gigaspeech) (can be used)\n\n\n### Frameworks:\n**Pytorch**\n\n\n### Epochs/Steps\nSteps (train) = 150\n\nVal iterations = every 50 steps\n\n\n### Loss Curves\n\n![Train and Val loss curves](images/loss.jpg)\n\n\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/TTS",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/TTS?ref=master",
      "download_url": null,
      "created_date": "2025-03-26",
      "github_date": "2025-03-26"
    },
    {
      "name": "Transformer",
      "display_name": "Transformer",
      "description": "From scratch implementation of Transformer",
      "readme_content": "\n# Vanilla Transformers for Machine Translation in Pytorch\n\nI implemented the Vanilla Transformers using Pytorch on the German-English dataset.\n\n[Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n\n\n### Datasets\n\n**Multi30k de-en**: [Link](https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/)\n\n### Frameworks:\n**Pytorch**\n\n\n### Results (on T4 GPU Single)\n\n**Training epochs:** 3\n**Val epochs:** 5\n\n**Train loss:** 0.02  (mean)\n**Val loss:** 0.03 (mean)\n\n[NOTE]: The train and val loss seems to be off. Please submit a PR or open a discussion if you find the issue and would really appreciate your help!\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/Transformer",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/Transformer?ref=master",
      "download_url": null,
      "created_date": "2025-03-10",
      "github_date": "2025-03-10"
    },
    {
      "name": "VAE",
      "display_name": "Vae",
      "description": "From scratch implementation of VAE",
      "readme_content": "# Variational Autoencoder (VAE) from Scratch\n\nI implemented a Variational Autoencoder Architecture from Scratch using PyTorch on the **CelebA dataset** for high-resolution face generation and reconstruction.\n\n[Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114)\n\n## Results\n\n### Original vs Reconstructed Images\n\nThe following images show the comparison between original CelebA face images (top row) and their reconstructions by the VAE (bottom row):\n\n<!-- Main image reference -->\n<img src=\"data/image.png\" alt=\"VAE Results\" width=\"800\"/>\n\n<!-- Fallback references -->\n![VAE Results](./data/image.png) > **Note**: If images don't load, please check the `data/` folder in this repository:\n> - `data/image.png` - Reconstruction comparison results\n> - `data/losses.jpg` - Training loss curves  \n> - `data/arithmetic.jpg` - Latent space visualizations\n> - `data/samples.jpg` - Generated sample faces from latent space\n\n*VAE: Original (top) vs Reconstructed (bottom) - Shows the model's ability to reconstruct high-resolution face images from the latent space representation.*\n\n### Training Progress\n\n<!-- Main image reference -->\n<img src=\"data/losses.jpg\" alt=\"Training Losses\" width=\"600\"/>\n\n<!-- Fallback reference -->\n![Training Losses](./data/losses.jpg)\n\n*Training and validation losses over epochs showing convergence of reconstruction and KL divergence losses.*\n\n### Latent Space Arithmetic\n\n<!-- Main image reference -->\n<img src=\"data/arithmetic.jpg\" alt=\"Latent Arithmetic\" width=\"700\"/>\n\n<!-- Fallback reference -->\n![Latent Arithmetic](./data/arithmetic.jpg)\n\n*Latent space interpolation and arithmetic operations demonstrating the smooth and meaningful latent representations learned by the VAE.*\n\n### Generated Samples\n\n<!-- Main image reference -->\n<img src=\"data/samples.jpg\" alt=\"Generated Samples\" width=\"800\"/>\n\n<!-- Fallback reference -->\n![Generated Samples](./data/samples.jpg)\n\n*Random samples generated from the latent space showing the diversity and quality of faces that the VAE can produce.*\n\n## Model Hyperparameters\n\n| Parameter      | Value | Description                                                                 \n|----------------|-------|-----------------------------------------------------------------------------|\n| `input_dim`    | 3     | Input channels (RGB color images).                                          |\n| `hidden_dim`   | 128   | Hidden dimension for convolutional layers.                                  |\n| `output_dim`   | 32    | Latent space dimension (bottleneck).                                       |\n| `batch_size`   | 32    | The number of samples processed before the model is updated.                |\n| `learning_rate`| 0.0005| Learning rate for Adam optimizer.                                          |\n| `epochs`       | 200   | Number of training epochs.                                                  |\n| `leaky_relu`   | 0.01  | Negative slope for LeakyReLU activation.                                   |\n| `image_size`   | 128x128| Input image resolution for CelebA faces.                                  |\n\n### Dataset\n\n**CelebA**: Large-scale CelebFaces Attributes Dataset\n- 202,599 face images of celebrities\n- High-resolution RGB images (128x128 for this implementation)\n- Rich variety of facial expressions, poses, and lighting conditions\n- Dataset split: 80% training, 20% validation\n\n**Data Preprocessing**:\n- Resize to 128x128 pixels\n- Convert to RGB tensors\n- Normalize to [0, 1] range\n- No additional data augmentation to preserve face structure\n\n### Frameworks:\n**Pytorch**\n\n### Architecture\n\n**Encoder**: \n- 4 Convolutional layers with LeakyReLU activation\n- Progressive channel increase: 3 â†’ 128 â†’ 256 â†’ 256 â†’ 256\n- Stride 2 for downsampling to reduce spatial dimensions\n- Flatten and linear layers for mean and log variance (reparameterization trick)\n- Output: 32-dimensional latent space\n\n**Decoder**:\n- Linear layer to expand 32D latent representation to 262,144 dimensions\n- Reshape to 256 Ã— 32 Ã— 32 feature maps\n- 4 Transposed Convolutional layers with LeakyReLU activation\n- Progressive channel decrease: 256 â†’ 256 â†’ 256 â†’ 128 â†’ 3\n- Stride 2 for upsampling to reconstruct 128Ã—128 images\n- Sigmoid activation for final RGB output [0, 1]\n\n### Training Details\n\n**Optimizer**: Adam with learning rate 0.0005  \n**Loss Function**: Reconstruction Loss (MSE) + KL Divergence  \n**Training/Validation Split**: 80/20  \n**Device**: CUDA (with automatic CPU fallback)\n**Progress Tracking**: tqdm progress bars with real-time loss monitoring\n**Logging**: Weights & Biases (wandb) for experiment tracking\n\n### VAE-Specific Components\n\n**Reparameterization Trick**: Enables backpropagation through stochastic sampling  \n**KL Divergence**: Regularizes latent space to follow standard normal distribution  \n**Latent Space**: 32-dimensional for rich face feature representation\n**Loss Weighting**: Balanced reconstruction and KL terms for stable training\n\n\n\n**Final Training Metrics**:\n- Reconstruction Loss: MSE between original and reconstructed images\n- KL Loss: Ensures latent variables follow standard normal distribution\n- Total Loss: Weighted combination optimizing both reconstruction quality and latent space structure\n\n## Files in Repository\n\n### Notebooks\n- `celeba-variational-autoencoders.ipynb` - Main implementation with CelebA dataset\n- `variational-autoencoders.ipynb` - Original MNIST implementation\n- `model.ipynb` - Model architecture experiments\n- `inference_vae.py` - Inference script for generating new faces\n\n### Data\n- `data/image.png` - Sample reconstruction results visualization\n- `data/losses.jpg` - Training loss curves and convergence plots  \n- `data/arithmetic.jpg` - Latent space interpolation and arithmetic examples\n- `data/samples.jpg` - Generated sample faces from latent space\n\n> **Image Loading Issues?** \n> If images don't display in your markdown viewer:\n> 1. Navigate to the `data/` folder directly to view images\n> 2. Try using absolute paths: `./data/image.png`\n> 3. Some markdown viewers require the repository to be cloned locally\n> 4. GitHub should display the images correctly in the web interface\n\n### Model Checkpoints\n- `vae_checkpoint_epoch_240.pth` - Trained model weights after 240 epochs\n\n## Usage\n\n### Training the Model\n\n1. **Setup Environment**:\n```bash\npip install torch torchvision tqdm wandb torchinfo matplotlib\n```\n\n2. **Prepare CelebA Dataset**:\n   - Download CelebA dataset\n   - Place images in `/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/`\n   - Or modify the `image_dir` path in the notebook\n\n3. **Run Training**:\n   - Open `celeba-variational-autoencoders.ipynb`\n   - Execute cells sequentially\n   - Monitor progress with tqdm progress bars\n   - Track metrics on Weights & Biases\n\n### Inference\n\nUse the trained model for:\n- **Image Reconstruction**: Encode and decode existing faces\n- **Face Generation**: Sample from latent space to generate new faces\n- **Latent Interpolation**: Smooth transitions between faces\n<!-- - **Attribute Manipulation**: Modify specific facial features -->\n\n```python\n# Load trained model\ncheckpoint = torch.load('vae_checkpoint_epoch_240.pth')\nmodel.load_state_dict(checkpoint['model_state_dict'])\n\n# Generate new faces\nwith torch.no_grad():\n    z = torch.randn(16, 32).to(device)  # Sample from latent space\n    generated_faces = model.decoder(z)\n```\n\n\n\n### Frameworks:\n**PyTorch**\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/VAE",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/VAE?ref=master",
      "download_url": null,
      "created_date": "2025-06-17",
      "github_date": "2025-06-17"
    },
    {
      "name": "ViT",
      "display_name": "Vit",
      "description": "From scratch implementation of ViT",
      "readme_content": "\n# Vision Transformer (ViT-b-16) from Scratch\n\nImplmented a ViT Architecture from Scratch using Pytorch on a subset of Food-101 dataset.\n\n[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/1511.06434)\n\n\n## Dataset Information\n\n**Dataset (Train):** Subset of Food101 (3 classes-255 images total)\n**Dataset (Test):** Subset of Food101 (3 classes-75 images total)\n\n\n### Frameworks\n\n**Pytorch**\n\n\n## Results\n\n**Training loss**: 1.20 \\\n**Test loss**: 1.52\n## Authors\n\n- [@YuvrajSingh](https://www.github.com/YuvrajSingh-mist)\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/ViT",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/ViT?ref=master",
      "download_url": null,
      "created_date": "2024-06-20",
      "github_date": "2024-06-20"
    },
    {
      "name": "WGANs",
      "display_name": "Wgans",
      "description": "From scratch implementation of WGANs",
      "readme_content": "\n# WGAN and WGAN-GP architecture in Pytorch\n\nI implemented the WGAN and WGAN-GP using Pytorch on the flickr8000 dataset.\n\n[Wasserstein GAN](https://arxiv.org/abs/1701.07875)\n[Improved training with WGAN](https://arxiv.org/abs/1704.00028)\n\n### Datasets\n\n**MNIST**: [Link](https://www.kaggle.com/datasets/hojjatk/mnist-dataset)\n\n### Frameworks:\n**Pytorch**\n\n\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/WGANs",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/WGANs?ref=master",
      "download_url": null,
      "created_date": "2025-02-09",
      "github_date": "2025-02-09"
    },
    {
      "name": "Whisper",
      "display_name": "Whisper",
      "description": "From scratch implementation of Whisper",
      "readme_content": "\n\n# Whisper model in Pytorch from scratch implementation\n\nTrained a small whisper model coded and trained from scratch in Pytorch \n\n\n\n[Robust Speech Recognition via Large-Scale Weak Supervision](https://cdn.openai.com/papers/whisper.pdf)\n\n## ModelArgs Hyperparameters\n\n| Parameter               | Value                  | Description                                                                 |\n|-------------------------|------------------------|-----------------------------------------------------------------------------|\n| `batch_size`            | 64                     | The number of samples processed before the model is updated.                |\n| `max_lr`                | 2e-4                   | Maximum learning rate.                                                      |\n| `dropout`               | 0.1                    | Dropout rate for regularization.                                            |\n| `epochs`                | 10                     | Number of training epochs.                                                  |\n| `block_size`            | 64                     | Sequence length (number of tokens or time steps).                           |\n| `tgt_vocab_size`        | 50262     | Size of the target vocabulary.                                              |\n| `embeddings_dims`       | 384                    | Dimensionality of token embeddings.                                         |\n| `attn_dropout`          | 0.1                    | Dropout rate for attention layers.                                          |\n| `no_of_heads`           | 6                      | Number of attention heads in multi-head attention.                          |\n| `no_of_decoder_layers`  | 6                      | Number of decoder layers in the model.                                      |\n| `weight_decay_optim`    | 0.01                   | Weight decay for the optimizer.                                             |\n| `log_mel_features`      | 80                     | Number of Mel spectrogram features.                                         |\n| `kernel_size`           | 3                      | Kernel size for convolutional layers.                                       |\n| `stride`                | 2             | Stride for convolutional layers.                                            |\n| `sr`                    | 16000                  | Sampling rate of the audio.                                                 |\n| `device`                | `'cuda:0'`             | Device to run the model on (e.g., GPU).                                     |\n| `SAMPLING_RATE`         | 16000                  | Sampling rate of the audio.                                                 |\n| `N_MELS`                | 80                     | Number of Mel bins in the spectrogram.                                      |\n| `WINDOW_DURATION`       | 0.025                  | Duration of the analysis window in seconds (25 ms).                         |\n| `STRIDE_DURATION`       | 0.010                  | Stride between consecutive windows in seconds (10 ms).                      |\n| `max_t`                 | 500                    | Maximum time steps in the spectrogram.                                      |\n| `n_channels`            | 80                     | Number of channels in the input spectrogram.                                |\n| `hidden_dim`            | 4 * `embeddings_dims`  | Number of neurons in the feed-forward network (FFN).                        |\n\"\"\"\n\n### Dataset\n\n[Gigaspeech](https://huggingface.co/datasets/speechcolab/gigaspeech)\n\nUsed the 'xs' snapshot.\n\n### Frameworks:\n**Pytorch**\n\n\n### Epochs/Steps\nEpochs (train) = 10\n\nVal iterations = every epoch\n\n\n### Loss Curves\n\n![Train and Val loss curves](img/loss.jpg)\n\n\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/Whisper",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/Whisper?ref=master",
      "download_url": null,
      "created_date": "2025-04-25",
      "github_date": "2025-04-25"
    },
    {
      "name": "lstm",
      "display_name": "Lstm",
      "description": "From scratch implementation of lstm",
      "readme_content": "\n# LSTM in Pytorch from scratch implementation\n\nTrained 128K LSTM model coded from scratch in Pytorch \n\n## ModelArgs Hyperparameters\n\n| Parameter    | Value    | Description                                                                 \n|--------------|----------|-----------------------------------------------------------------------------|\n| `batch_size` | 32       | The number of samples processed before the model is updated.                |\n| `max_lr`     | 1e-4     | Maximum learning rate.                                                      |\n| `dropout`    | 0.1      | Dropout.                                                                    |\n| `epochs`     | 50       | Epochs                                                                      |           \n| `block_size` | 64       | Sequence length                                                             |\n| `No of neurons`     | 128       | Epochs                                                               |   \n\n\n### Frameworks:\n**Pytorch**\n\n\n### Epochs/Steps\nEpochs (train) = 50\n\nVal iterations = every epoch\n\n\n### Losses\n\nTrain loss - 0.49 \n\nVal loss - 0.48\n\n### Loss Curves\n\n![Train and Val loss curves](img/loss_curves.jpg)\n\n\n\n",
      "github_url": "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/lstm",
      "api_url": "https://api.github.com/repos/YuvrajSingh-mist/Paper-Replications/contents/lstm?ref=master",
      "download_url": null,
      "created_date": "2025-04-25",
      "github_date": "2025-04-25"
    }
  ],
  "last_updated": "2025-08-09T09:08:02.221663"
}