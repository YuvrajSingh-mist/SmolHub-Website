---
title: "Vit"
excerpt: "Implementation of ViT from the Paper Replications repository"
collection: paper_replications
layout: paper-replication
category: "Language Models"
framework: "PyTorch"
dataset: "Custom"
github_url: "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/ViT"
date: 2025-08-07
---

## Overview
Implementation of ViT from the Paper Replications repository

## Key Features
- Transformer Architecture

## Technical Details
- **Framework**: PyTorch
- **Dataset**: Custom
- **Category**: Language Models

## Implementation Details

# Vision Transformer (ViT-b-16) from Scratch

Implmented a ViT Architecture from Scratch using Pytorch on a subset of Food-101 dataset.

[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/1511.06434)

## Dataset Information

**Dataset (Train):** Subset of Food101 (3 classes-255 images total)
**Dataset (Test):** Subset of Food101 (3 classes-75 images total)

### Frameworks

**Pytorch**

## Results

**Training loss**: 1.20 \
**Test loss**: 1.52
## Authors

- [@YuvrajSingh](https://www.github.com/YuvrajSingh-mist)

## Source Code
üìÅ **GitHub Repository**: [ViT](https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/ViT)

View the complete implementation, training scripts, and documentation on GitHub.
