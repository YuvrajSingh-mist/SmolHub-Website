---
title: "Differential Transformer"
excerpt: "From scratch implementation of Differential Transformer"
collection: models
layout: model-implementation
category: "Language Models"
framework: "PyTorch"
dataset: "TinyShakespeare"
github_url: "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/Differential Transformer"
date: 2025-02-09
---

## Overview
From scratch implementation of Differential Transformer

## Key Features
- Transformer Architecture

## Technical Details
- **Framework**: PyTorch
- **Dataset**: TinyShakespeare
- **Category**: Language Models

## Implementation Details

# Differential Transformers in Pytorch

I implemented the Differential Transformers using Pytorch on Tinyshakespeare dataset.

[Differential Transformers](https://arxiv.org/pdf/2410.05258)

### Datasets

**Tineshakespeare**: in the /data folder

### Frameworks:
**Pytorch**

### Results (on A100 GPU Single)

**Training steps:** 2000
**Training steps:** per 100 training steps

**Train loss:**  5.95
**Val loss:** 5.98

## Source Code
üìÅ **GitHub Repository**: [Differential Transformer](https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/Differential Transformer)

View the complete implementation, training scripts, and documentation on GitHub.
