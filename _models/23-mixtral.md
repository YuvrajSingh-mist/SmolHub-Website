---
title: "Mixtral"
excerpt: "Implementation of Mixtral from scratch"
collection: models
layout: model-implementation
category: "Training Optimization"
framework: "PyTorch"
dataset: "Custom"
github_url: "https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/Mixtral"
date: 2025-08-08
---

## Overview
Implementation of Mixtral from scratch

## Technical Details
- **Framework**: PyTorch
- **Dataset**: Custom
- **Category**: Training Optimization

## Implementation Details

# Mixtral in Pytorch

I implemented the Mixtral architecture from scratch using Pytorch on Tinyshakespeare dataset.

[Mixtral of Experts](https://arxiv.org/pdf/2401.04088)


### Datasets

**Tineshakespeare**: in the /data folder

### Frameworks:
**Pytorch**


### Results (on T4 GPU Single)

**Training steps:** 1000
**IValidation steps:** per 50 training steps

**Train loss:** 2.0422 
**Val loss:** 2.0898

## Source Code
üìÅ **GitHub Repository**: [Mixtral](https://github.com/YuvrajSingh-mist/Paper-Replications/tree/master/Mixtral)

View the complete implementation, training scripts, and documentation on GitHub.
